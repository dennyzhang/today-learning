* Today's Learning                                                :BLOG:Life:
:PROPERTIES:
:type:   Life
:END:
Personal diary for what I have learned day by day.

Check more in #today-learnings channel of our DevOps slack group.

https://www.dennyzhang.com/slack
---------------------------------------------------------------------

#+BEGIN_HTML
<a href="https://github.com/dennyzhang/today-learning"><img align="right" width="200" height="183" src="https://www.dennyzhang.com/wp-content/uploads/denny/watermark/github.png" /></a>
<div id="the whole thing" style="overflow: hidden;">
<div style="float: left; padding: 5px"> <a href="https://www.linkedin.com/in/dennyzhang001"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/linkedin.png" alt="linkedin" /></a></div>
<div style="float: left; padding: 5px"><a href="https://github.com/dennyzhang"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/github.png" alt="github" /></a></div>
<div style="float: left; padding: 5px"><a href="https://www.dennyzhang.com/slack" target="_blank" rel="nofollow"><img src="https://www.dennyzhang.com/wp-content/uploads/sns/slack.png" alt="slack"/></a></div>
</div>

<br/><br/>
<a href="http://makeapullrequest.com" target="_blank" rel="nofollow"><img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg" alt="PRs Welcome"/></a>
#+END_HTML

*No sensitive information related to my current employer*

** Feb, 2018
[Denny Today] Feb 7th, Houston TX
#+BEGIN_EXAMPLE
- Define Jenkins job to create public VM by terraform
  https://github.com/DennyZhang/terraform_jenkins_digitalocean
- Manually remove resources(VM/volumes/closed ES indices) for scaling down
#+END_EXAMPLE

[Denny Today] Feb 6th, Houston TX
#+BEGIN_EXAMPLE
- Publish ES script for deleting closed ES indices:
  https://github.com/DennyZhang/elasticsearch-cli-tool/tree/master/delete_closed_es_indices
- Interview multiple candidates
#+END_EXAMPLE

[Denny Today] Feb 5th, Houston TX
#+BEGIN_EXAMPLE
- ES maintenance: self-protection for index deletion. only allow delete closed index.
- Terraform wrapper: Create Jenkins job to create desired machine flavor.
- Code problem(twopointer): https://code.dennyzhang.com/minimum-window-substring
#+END_EXAMPLE

[Denny Today] Feb 2nd, Houston TX
#+BEGIN_EXAMPLE
- Update apache docker to enable SSL for vhosts
- Update kitchen testcase to enforce SSL validation test
- Solved one interesting Trie Tree problem

https://code.dennyzhang.com/implement-magic-dictionary
#+END_EXAMPLE

[Denny Today] Feb 1st, Houston TX
#+BEGIN_EXAMPLE
- Lots of ES reindexing.
- Working on ES performance issues.
- Solving some dynamic programming problems:
  https://code.dennyzhang.com/tag/dynamicprogramming
#+END_EXAMPLE

** Jan, 2018
[Denny Today] Jan 31st, Houston TX
#+BEGIN_EXAMPLE
- Update chef code to avoid DB/APP autostart when cloud provider reboot hypervisors.
- Routine support and coding
- Learned reversior sampling algorithms
  https://code.dennyzhang.com/tag/reservoirsampling
#+END_EXAMPLE

[Denny Today] Jan 30th, Houston TX
#+BEGIN_EXAMPLE
- Preparing for region reboot from cloud provider, due to Meltdown and Spectre issue.
- Practice solving Backtracking problems by code
#+END_EXAMPLE

[Denny Today] Jan 29th, Houston TX
#+BEGIN_EXAMPLE
- Lots of work with ES reindexing.
  We have around 300 index to be migrated, due to data structure changes.
  The reindexing has failed many times because search queue is full. Also index has been closed due to some unhandled exceptions.
#+END_EXAMPLE

Dump the main logic to below GitHub repo: https://github.com/DennyZhang/elasticsearch-cli-tool

[Denny Today] Jan 25th, Houston TX
#+BEGIN_EXAMPLE
- Defined a Jenkins pipeline job to proactively run all Jenkins check jobs for prod env
- Bumped into an ES alias inconsistency issue.
curl "http://${es_ip}:${es_port}/_alias/${alias_name}" : indicates alias has been updated correctly
curl "http://$es_ip:$es_port/${alias_name}/_count": indicates alias still point to old index
#+END_EXAMPLE

[Denny Today], Jan 22nd, Houston TX
#+BEGIN_EXAMPLE
- Cracked some dynamic programming problems. Pretty challenging.
   https://code.dennyzhang.com/tag/dynamicprogramming
#+END_EXAMPLE


[Denny Today] Jan 18th, Houston, TX
#+BEGIN_EXAMPLE
- Per new DigitialOcean machine flavor, presented prod env migration proposal.
  We will save *43.72%* per month!!

Live Demo: https://code.dennyzhang.com/
#+END_EXAMPLE

[Denny Today] Jan 17th, Houston TX
#+BEGIN_EXAMPLE
- Exported my local Python syntax cheatsheet to GitHub
   https://github.com/DennyZhang/cheatsheet-python-A4
#+END_EXAMPLE


[Denny Today] Jan 16th. Houston TX
#+BEGIN_EXAMPLE
- Solving one interesting design problem(code test)
  https://leetcode.com/problems/insert-delete-getrandom-o1-duplicates-allowed/description/
#+END_EXAMPLE

[Denny Today], Jan 12st, Houston TX
#+BEGIN_EXAMPLE
- ES cluster run into red twice in the past 2 days.
  Should be some application requests trigger this. But no solid evidence yet.
#+END_EXAMPLE

[Denny Today] Jan 9th, Houston TX
#+BEGIN_EXAMPLE
- Exercise my *emacs* elisp skills: enforce 2-way sync in between a wordpress blog and a GitHub repo.
  `Any emacs veterans here`?
#+END_EXAMPLE

[Denny Yesterday] Jan 2nd, Houston TX
#+BEGIN_EXAMPLE

Learning 3 k8s service types. And trying to understand how it works behind the scene.
Not very clear yet
#+END_EXAMPLE

** Nov, 2017
[Denny Today] Nov 29th, Houston TX
#+BEGIN_EXAMPLE
- Started a GitHub repo for my k8s deep dive
https://github.com/DennyZhang/challenges-kubernetes
#+END_EXAMPLE
** More Resources
 License: Code is licensed under [[https://www.dennyzhang.com/wp-content/mit_license.txt][MIT License]].
 #+BEGIN_HTML
 <a href="https://www.dennyzhang.com"><img align="right" width="201" height="268" src="https://raw.githubusercontent.com/USDevOps/mywechat-slack-group/master/images/denny_201706.png"></a>
 <a href="https://www.dennyzhang.com"><img align="right" src="https://raw.githubusercontent.com/USDevOps/mywechat-slack-group/master/images/dns_small.png"></a>

 <a href="https://www.linkedin.com/in/dennyzhang001"><img align="bottom" src="https://www.dennyzhang.com/wp-content/uploads/sns/linkedin.png" alt="linkedin" /></a>
 <a href="https://github.com/DennyZhang"><img align="bottom"src="https://www.dennyzhang.com/wp-content/uploads/sns/github.png" alt="github" /></a>
 <a href="https://www.dennyzhang.com/slack" target="_blank" rel="nofollow"><img align="bottom" src="https://www.dennyzhang.com/wp-content/uploads/sns/slack.png" alt="slack"/></a>
 #+END_HTML

Blog URL: https://www.dennyzhang.com/today-learning
* org-mode configuration                                           :noexport:
#+STARTUP: overview customtime noalign logdone showall
#+DESCRIPTION: 
#+KEYWORDS: 
#+AUTHOR: Denny Zhang
#+EMAIL:  denny@dennyzhang.com
#+TAGS: noexport(n)
#+PRIORITIES: A D C
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_EXCLUDE_TAGS: exclude noexport
#+SEQ_TODO: TODO HALF ASSIGN | DONE BYPASS DELEGATE CANCELED DEFERRED
#+LINK_UP:   
#+LINK_HOME: 
* 2017                                                             :noexport:
** 2017-01
*** 2017-01-01: explorees16 can't ping or telnet in explore env: Linode maintainence
   CLOSED: [2017-01-01 Sun 08:55]
*** 2017-01-02: In prod env: enable nagios monitoring for prod-app-4 and app-5, which are not detectable from haproxy.
   CLOSED: [2017-01-02 Mon 15:49]
*** 2017-01-02: data clean up: docker hub(images), digitalocean(snapshots)
   CLOSED: [2017-01-02 Mon 16:35]
  Enable dropbox for daily backup
  Build totvs/mdm:latest and push the change for package_url update
  Cleanup chef_json parameters in prod env and explore env: remove TODO
  Remove DigitalOcean useless Snapshots
  Remove jenkins image: totvslabs/mdm_jenkins
*** 2017-01-03: Update nagios client only in explore and prod env
   CLOSED: [2017-01-03 Tue 11:21]
*** 2017-01-03: [[https://trello.com/c/xxRBlCbB][#1733]] Chef enforce pre-check before update: input parameter checks, infra check
  CLOSED: [2016-12-24 Sat 11:43]
typo in chef_json(extra : for node ip)

> recipes/precheck.rb && vim  recipes/precheck.rb

> ../common-basic/libraries/pre_check_helper.rb && vim  ../common-basic/libraries/pre_check_helper.rb


> recipes/precheck.rb && vim  recipes/precheck.rb

git@bitbucket.org:lrpdevops/mdmdevops-totvslabs.git

export branch_name=1.55-deploy

cd /var/lib/jenkins/code/DockerDeployAllInOne/1.55-deploy/mdmdevops-totvslabs/cookbooks/all-in-one
export INSTANCE_NAME=all-in-one-DockerDeployAllInOne-91
export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true
export CLEAN_START=false
export ENABLE_MORE_TEST=false
export PACKAGE_URL='http://172.17.0.1:18000'
export APP_BRANCH_NAME=1.55
export FRAMEWORK_BRANCH_NAME=1.55
*** 2017-01-03: [[https://trello.com/c/BCS6CpDN][#1539]] Get slack notification for es slow query or slow index actions
   CLOSED: [2017-01-03 Tue 11:41]
*** 2017-01-03: Review all Linode VMs and decommision 3 VMs: https://github.com/TOTVS/mdmdevops/wiki/MDM-Server-List#machine-list-in-linode
  CLOSED: [2017-01-03 Tue 09:17]
#+BEGIN_EXAMPLE
denny zhang [9:14 AM]
Now we can get slack report for Linode cost just like DigitalOcean.

[9:15]
@kungwang, I'm not familiar with some Linode VMs.

Could you please help to confirm whether we're still effectively using them?

- If yes, let's update the information to below wiki.
https://github.com/TOTVS/mdmdevops/wiki/MDM-Server-List#machine-list-in-linode

- Otherwise, we decommission them to save some cost.

2351522          linode2351522             45.33.107.32         80.0
2463874          linode2463874             96.126.96.103        80.0
2481979          linode2481979             173.255.243.140      40.0
#+END_EXAMPLE

45.33.107.32: stop <2017-01-03 12:47 UTC +8>
2463874          linode2463874             96.126.96.103: stop <2017-01-03 13:25 UTC +8>
*** 2017-01-03: support for Diogo: ios and android distribution process
  CLOSED: [2017-01-03 Tue 17:22]
ssh -p 4022 root@repo.fluigdata.com

ssh -p 4022 root@104.236.159.226

/var/www/repo/upload

ChangeMe1234

scp -P 4022 /etc/hosts upload@104.236.159.226:/var/www/repo/upload/hosts
http://repo.fluigdata.com:18000/upload/hosts
*** 2017-01-03: disk cleanup for repo.fluigdata.com
   CLOSED: [2017-01-03 Tue 17:46]
*** 2017-01-03: Branch out 1.57 and run CI tests
   CLOSED: [2017-01-03 Tue 22:58]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-01-04: Diogo: enable ssl for repo.fluigdata.com: [[https://trello.com/c/1dePaiQ3][#1579]] Define the process to distribute iOS and Android App
  CLOSED: [2017-01-04 Wed 12:58]

Diogo Carmo [11:11 AM]
hey @denny.zhang was able to login successfully! but I’m not sure if this repo is going to work, ‘cause to be able to distribute the iOS app, we need to use SSL

[11:11]
does https://repo.fluigdata.com:18000 works?

denny zhang [11:12 AM]
Got it, @diogo.carmo. Let me find a time to enable ssl today.

ChangeMe1234

#+BEGIN_EXAMPLE
ssh upload@104.236.159.226 date
scp /etc/hosts upload@repo.fluigdata.com:/var/www/upload/hosts
curl -I https://repo.fluigdata.com/hosts
#+END_EXAMPLE

vim /etc/apache2/ports.conf

enable port 443

vim /etc/apache2/sites-enabled/upload.conf

<VirtualHost *:443>
    ServerAdmin webmaster@dummy-host.example.com
    DocumentRoot /var/www/upload
    SSLEngine on
    SSLCertificateFile "/root/ssl_key/__fluigdata_com.crt"
    SSLCertificateKeyFile "/root/ssl_key/fluigdata.key"
    SSLCACertificateFile "/root/ssl_key/__fluigdata_com.ca-bundle"
    <Directory '/var/www/upload'>
        Options Includes FollowSymLinks MultiViews
        AllowOverride None
        Require all granted
    </Directory>
    ErrorLog /var/log/apache2/upload-error.log
    CustomLog /var/log/apache2/upload.log common
</VirtualHost>
*** 2017-01-04: monitor https://repo.fluigdata.com
   CLOSED: [2017-01-04 Wed 14:39]
*** 2017-01-04: autoscaling error: incompabile changes: DIGITALOCEAN_ACCESS_TOKEN -> DIGTIALOCEAN_TOKEN_V2
   CLOSED: [2017-01-04 Wed 11:37]
*** 2017-01-04: test autosacling in docker and digitalocean with basic test scenario
   CLOSED: [2017-01-04 Wed 14:29]
*** 2017-01-04: [#A] max count doesn't work for cb autoscaling: if provision fails, disable autoscale
  CLOSED: [2017-01-04 Wed 16:42]
#+BEGIN_EXAMPLE
2017-01-04 22:05:01 -----> Execute command on default-kitchen-autoscale-cluster.
2017-01-04 22:05:02        In /root/devops/mdmdevops/cookbooks/elasticsearch-mdm, run kitchen destroy
2017-01-04 22:05:03 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:04        [36m-----> Destroying <couchbase-20170104-204906-ubuntu-1404>...[0m
2017-01-04 22:05:04        [36m       Finished destroying <couchbase-20170104-204906-ubuntu-1404> (0m0.00s).[0m
2017-01-04 22:05:04 -----> Kitchen is finished. (0m0.66s)
2017-01-04 22:05:04 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:05        [36m-----> Destroying <couchbase-20170104-210206-ubuntu-1404>...[0m
2017-01-04 22:05:05        [36m       Finished destroying <couchbase-20170104-210206-ubuntu-1404> (0m0.00s).[0m
2017-01-04 22:05:05 -----> Kitchen is finished. (0m0.66s)
2017-01-04 22:05:05 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:06        [36m-----> Destroying <couchbase-20170104-211206-ubuntu-1404>...[0m
2017-01-04 22:05:06        [36m       Finished destroying <couchbase-20170104-211206-ubuntu-1404> (0m0.00s).[0m
2017-01-04 22:05:06 -----> Kitchen is finished. (0m0.65s)
2017-01-04 22:05:06 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:07        [36m-----> Destroying <couchbase-20170104-212206-ubuntu-1404>...[0m
2017-01-04 22:05:07        [36m       Finished destroying <couchbase-20170104-212206-ubuntu-1404> (0m0.00s).[0m
2017-01-04 22:05:07 -----> Kitchen is finished. (0m0.94s)
2017-01-04 22:05:08 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:09        [36m-----> Destroying <couchbase-20170104-213206-ubuntu-1404>...[0m
2017-01-04 22:05:09        [36m       Finished destroying <couchbase-20170104-213206-ubuntu-1404> (0m0.00s).[0m
2017-01-04 22:05:09 -----> Kitchen is finished. (0m0.94s)
2017-01-04 22:05:09 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:10        [36m-----> Destroying <couchbase-20170104-214206-ubuntu-1404>...[0m
2017-01-04 22:05:10        [36m       Finished destroying <couchbase-20170104-214206-ubuntu-1404> (0m0.00s).[0m
2017-01-04 22:05:10 -----> Kitchen is finished. (0m0.64s)
2017-01-04 22:05:10 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:11        [36m-----> Destroying <couchbase-20170104-215206-ubuntu-1404>...[0m
2017-01-04 22:05:11        [36m       Finished destroying <couchbase-20170104-215206-ubuntu-1404> (0m0.00s).[0m
2017-01-04 22:05:11 -----> Kitchen is finished. (0m0.74s)
2017-01-04 22:05:12 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:12        [36m-----> Destroying <couchbase-20170104-220206-ubuntu-1404>...[0m
2017-01-04 22:05:12        [36m       Finished destroying <couchbase-20170104-220206-ubuntu-1404> (0m0.00s).[0m
2017-01-04 22:05:12 -----> Kitchen is finished. (0m0.65s)
2017-01-04 22:05:13 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:13        [36m-----> Destroying <elasticsearch-20170104-204511-ubuntu-1404>...[0m
2017-01-04 22:05:17        [36m       Digital Ocean instance <36310523> destroyed.[0m
2017-01-04 22:05:17        [36m       Finished destroying <elasticsearch-20170104-204511-ubuntu-1404> (0m3.60s).[0m
2017-01-04 22:05:17 -----> Kitchen is finished. (0m4.28s)
2017-01-04 22:05:17 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:18        [36m-----> Destroying <elasticsearch-20170104-210512-ubuntu-1404>...[0m
2017-01-04 22:05:21        [36m       Digital Ocean instance <36311503> destroyed.[0m
2017-01-04 22:05:21        [36m       Finished destroying <elasticsearch-20170104-210512-ubuntu-1404> (0m2.66s).[0m
2017-01-04 22:05:21 -----> Kitchen is finished. (0m3.29s)
2017-01-04 22:05:21        In /root/devops/mdmdevops/cookbooks/couchbase-mdm, run kitchen destroy
2017-01-04 22:05:21 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:22        [36m-----> Destroying <couchbase-20170104-204906-ubuntu-1404>...[0m
2017-01-04 22:05:24        [36m       Digital Ocean instance <36310728> destroyed.[0m
2017-01-04 22:05:24        [36m       Finished destroying <couchbase-20170104-204906-ubuntu-1404> (0m2.32s).[0m
2017-01-04 22:05:24 -----> Kitchen is finished. (0m3.06s)
2017-01-04 22:05:25 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:25        [36m-----> Destroying <couchbase-20170104-210206-ubuntu-1404>...[0m
2017-01-04 22:05:30        [36m       Digital Ocean instance <36311332> destroyed.[0m
2017-01-04 22:05:30        [36m       Finished destroying <couchbase-20170104-210206-ubuntu-1404> (0m4.24s).[0m
2017-01-04 22:05:30 -----> Kitchen is finished. (0m4.91s)
2017-01-04 22:05:30 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:31        [36m-----> Destroying <couchbase-20170104-211206-ubuntu-1404>...[0m
2017-01-04 22:05:34        [36m       Digital Ocean instance <36311843> destroyed.[0m
2017-01-04 22:05:34        [36m       Finished destroying <couchbase-20170104-211206-ubuntu-1404> (0m2.77s).[0m
2017-01-04 22:05:34 -----> Kitchen is finished. (0m3.43s)
2017-01-04 22:05:34 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:35        [36m-----> Destroying <couchbase-20170104-212206-ubuntu-1404>...[0m
2017-01-04 22:05:37        [36m       Digital Ocean instance <36312320> destroyed.[0m
2017-01-04 22:05:37        [36m       Finished destroying <couchbase-20170104-212206-ubuntu-1404> (0m2.10s).[0m
2017-01-04 22:05:37 -----> Kitchen is finished. (0m2.74s)
2017-01-04 22:05:37 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:38        [36m-----> Destroying <couchbase-20170104-213206-ubuntu-1404>...[0m
2017-01-04 22:05:40        [36m       Digital Ocean instance <36312823> destroyed.[0m
2017-01-04 22:05:40        [36m       Finished destroying <couchbase-20170104-213206-ubuntu-1404> (0m2.03s).[0m
2017-01-04 22:05:40 -----> Kitchen is finished. (0m2.65s)
2017-01-04 22:05:40 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:41        [36m-----> Destroying <couchbase-20170104-214206-ubuntu-1404>...[0m
2017-01-04 22:05:45        [36m       Digital Ocean instance <36313407> destroyed.[0m
2017-01-04 22:05:45        [36m       Finished destroying <couchbase-20170104-214206-ubuntu-1404> (0m4.01s).[0m
2017-01-04 22:05:45 -----> Kitchen is finished. (0m4.66s)
2017-01-04 22:05:45 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:46        [36m-----> Destroying <couchbase-20170104-215206-ubuntu-1404>...[0m
2017-01-04 22:05:48        [36m       Digital Ocean instance <36314046> destroyed.[0m
2017-01-04 22:05:48        [36m       Finished destroying <couchbase-20170104-215206-ubuntu-1404> (0m1.91s).[0m
2017-01-04 22:05:48 -----> Kitchen is finished. (0m2.55s)
2017-01-04 22:05:48 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:49        [36m-----> Destroying <couchbase-20170104-220206-ubuntu-1404>...[0m
2017-01-04 22:05:51        [36m       Digital Ocean instance <36314708> destroyed.[0m
2017-01-04 22:05:51        [36m       Finished destroying <couchbase-20170104-220206-ubuntu-1404> (0m1.71s).[0m
2017-01-04 22:05:51 -----> Kitchen is finished. (0m2.33s)
2017-01-04 22:05:51 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:52        [36m-----> Destroying <elasticsearch-20170104-204511-ubuntu-1404>...[0m
2017-01-04 22:05:52        [36m       Finished destroying <elasticsearch-20170104-204511-ubuntu-1404> (0m0.00s).[0m
2017-01-04 22:05:52 -----> Kitchen is finished. (0m0.71s)
2017-01-04 22:05:52 -----> Starting Kitchen (v1.4.1)
2017-01-04 22:05:53        [36m-----> Destroying <elasticsearch-20170104-210512-ubuntu-1404>...[0m
2017-01-04 22:05:53        [36m       Finished destroying <elasticsearch-20170104-210512-ubuntu-1404> (0m0.00s).[0m
2017-01-04 22:05:53 -----> Kitchen is finished. (0m0.68s)
#+END_EXAMPLE
*** 2017-01-04: metric cloud VMs cost: sort by hostname; get sum: of weekly cost
  CLOSED: [2017-01-04 Wed 23:28]
> /tmp/cloud_cost_slack_report2.py && vim /tmp/cloud_cost_slack_report2.py

devopsbotBOT [6:58 PM]
added and commented on a Plain Text snippet: Digitalocean Cost For All Droplets
ID        Name                IP               Price
4992032   mdm-workstation     104.236.159.226  $40.0
5524580   mdm-nexus           104.236.180.184  $10.0
13859826  william-ai-03       107.170.30.178   $160.0
16305944  prod-jenkins        45.55.6.34       $40.0
16306314  prod-cb-1           159.203.198.129  $320.0
16306902  prod-cb-3           104.236.179.76   $320.0
16307726  prod-es-1           159.203.216.25   $320.0
16307757  prod-es-2           107.170.212.76   $320.0
16307820  prod-es-3           192.241.211.99   $320.0
16307889  prod-es-4           159.203.219.53   $320.0
16307937  prod-es-5           159.203.211.150  $320.0
16307977  prod-es-6           159.203.192.146  $320.0
16308253  prod-app-1          159.203.234.164  $160.0
*** 2017-01-05: [[https://trello.com/c/ZId29dVO][#1547]] Generate weekly slack report for cloud cost: digitalocean and linode
   CLOSED: [2017-01-05 Thu 10:21]
*** 2017-01-05: update autoscaling progress to slack, and send critical slack alerts if failed: http://injenkins.fluigdata.com:48080/job/AutoscaleDockerESClusterTest/3/console
  CLOSED: [2017-01-05 Thu 14:22]
Whenever we add a new node, send out slack notification. if fail, disable autoscaling feature, and send out alerts
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-01-05: Update all detail information of our nagios system to wiki
  CLOSED: [2017-01-05 Thu 16:01]
https://github.com/TOTVS/mdmdevops/wiki/Nagios-Monitoring-System

Kung Wang [10:53 AM]
Denny, can you document on what things are checked on nagios and what REST API it is called on wiki?

[10:54]
and we enriched it once we have more metrices
*** 2017-01-06: [[https://trello.com/c/aEXCI82F][#1778]] mdmbackup is not shown checking es nodes via mdm application
  CLOSED: [2017-01-06 Fri 07:41]
elasticsearchDataConfig:
    clusterSettings:
        cluster.name: kitchen-mdm-cluster
        cluster.routing.allocation.disk.watermark.low: 90%
        cluster.routing.allocation.disk.watermark.high: 50gb
        discovery.zen.ping.multicast.enabled: false
        discovery.zen.ping.unicast.enabled: true
        # Sample: [ "XXX:9300", "XXX:9300" ]
        discovery.zen.ping.unicast.hosts: [ "kitchen-mdm-cluster-node1:9300","kitchen-mdm-cluster-node2:9300" ]
    nodeSettings:
        # if node.master and node.data are true, then this node will become server as well as hosting data
        # then we need to move path.* settings to node settings in order to make path.* taking effect
        # node.name should change to value of hostname when deployed
        node.name: kitchen-mdm-cluster-node3node_backup
        node.master: false
        node.data: false
        http.enabled: false
        # network.host should change to app's external IP when deployed in production
        # also remember to add network.host in elasticsearch.yml to localhost

kitchen-mdm-cluster-node1 172.17.0.3
kitchen-mdm-cluster-node2 172.17.0.4
kitchen-mdm-cluster-node3 172.17.0.5
kitchen-mdm-cluster-node4 172.17.0.6

# node1: couchbase, haproxy, nagios, elasticsearch, elasticsearch_audit
# node2: couchbase, haproxy, elasticsearch, elasticsearch_audit
# node3: mdm, mdmbackup
# node4: mdm(not recognized by loadbalancer)

#+BEGIN_EXAMPLE
root@kitchen-mdm-cluster-node3:/opt/mdm/config# curl '172.17.0.3:9200/_cat/nodes'
172.17.0.4 172.17.0.4 42 62 0.86 d m node2-kitchen-cluster-mdm-4nodes
172.17.0.5 172.17.0.5 21 62 0.86 - - kitchen-mdm-cluster-node3node_backup
172.17.0.6 172.17.0.6 31 62 0.86 - - kitchen-mdm-cluster-node4node
172.17.0.5 172.17.0.5 56 62 0.86 - - kitchen-mdm-cluster-node3node
172.17.0.3 172.17.0.3 47 62 0.86 d * node1-kitchen-cluster-mdm-4nodes

root@kitchen-mdm-cluster-node3:/opt/mdm/config# curl '172.17.0.5:9200/_cat/nodes'
172.17.0.6 172.17.0.6 31 62 0.67 - - kitchen-mdm-cluster-node4node_audit
172.17.0.3 172.17.0.3 19 62 0.67 d * node1-kitchen-cluster-mdm-4nodes
172.17.0.4 172.17.0.4 11 62 0.67 d m node2-kitchen-cluster-mdm-4nodes
172.17.0.5 172.17.0.5 59 62 0.67 - - kitchen-mdm-cluster-node3node_audit
#+END_EXAMPLE
*** 2017-01-07: add prod-es-16 with an extra es node to prod env
   CLOSED: [2017-01-07 Sat 08:13]
*** 2017-01-07: enforce hourly job to track jsump: http://prodjenkins.fluigdata.com:18080/view/Tmp/job/TmpDumpJstackLog/build?delay=0sec
   CLOSED: [2017-01-07 Sat 08:13]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-01-09: add prod-es-17 to prod env: http://prodjenkins.fluigdata.com:18080/job/DeploySystem/145/console
  CLOSED: [2017-01-09 Mon 08:21]
107.170.218.112
*** 2017-01-10: add prod-es-18 to prod env: http://prodjenkins.fluigdata.com:18080/job/DeploySystem/146/console
  CLOSED: [2017-01-10 Tue 13:32]
107.170.223.238
*** 2017-01-10: [[https://github.com/TOTVS/mdmdevops/wiki/Advanced-Questions-About-Deployment#queston-how-to-configure-maxsimultaneousthreads-and-maxtotalthreads-at-node-level][Update wiki]]: Queston: How to configure maxSimultaneousThreads and maxTotalThreads at node level
   CLOSED: [2017-01-10 Tue 16:07]
*** 2017-01-10: [[https://trello.com/c/R4lnAYrK][#1592]] Support configuring threads count in node level, instead of system level: http://injenkins.fluigdata.com:48080/view/Basic/job/DockerDeployFeatureCookbooks/149/console
  CLOSED: [2017-01-09 Mon 18:13]
export branch_name=1.57-deploy
export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true

export TEST_KITCHEN_YAML=".kitchen.yml"
# export DOCKER_PORT_FORWARD_PREFIX=31
export PACKAGE_URL='http://172.17.0.1:18000'
export APP_BRANCH_NAME=1.57
export FRAMEWORK_BRANCH_NAME=1.57
export CLUSTER_ID=kitchen-mdm-feature

cd /var/lib/jenkins/code/DockerDeployFeatureCookbooks/1.57-deploy/mdmdevops-totvslabs/cookbooks/mdm-cluster
export INSTANCE_NAME=mdm-cluster-DockerDeployFeatureCookbooks-149
export KITCHEN_YAML=.kitchen.yml

# TODO: support setting node-wise, instead of system-wise
default['app_mdm']['max_simultaneous_threads'] = '4'
default['app_mdm']['max_total_threads'] = '32'
**** misc
http://prodjenkins.fluigdata.com:18080/job/MonitorServerFileChanges/1829/console

#+BEGIN_EXAMPLE
Bruno Volpato [12:53 PM]
@denny.zhang we need to be able to specify different thread counts for the UI servers and worker servers

[12:54]
can you please add the 2 counts (max simultaneous and max total threads) in different parameters for each type of server?

denny zhang [12:54 PM]
Not yet.

I have a ticket for this. Could I deliver that by next Tues?

Bruno Volpato [12:55 PM]
@kungwang we should be fine for Tues right? we can change manually and keep skipping the yml update

Kung Wang [12:55 PM]
yes, np. that’s the expectation we can manage

denny zhang [12:55 PM]
I will cherry-pick to master, once it’s done

Thus we will only need to manual change once.

Kung Wang [12:57 PM]
Let’s change two UI app servers to no processing thread now manually, while waiting for deployment change next Tuesday

[12:57]
Bruno, Denny ok for this?

denny zhang [12:57 PM]
Fine for me.

[12:59]
If I finish it this weekend, I will let you guys know.

Thus we can save some effort.

Bruno Volpato [1:00 PM]
changed manually the number of threads

[1:00]
let me deploy the LB to have app1 and app2 as UI and the others as workers

denny zhang [1:01 PM]
Oh, man. You’re super fast.
#+END_EXAMPLE
**** 2017-01-10: verify in longruncluster
  CLOSED: [2017-01-10 Tue 16:06]
  "mdm_cluster":
        # If certain hosts are not given, app_mdm::maxSimultaneousThreads will be used by default
        {"max_simultaneous_threads_dict":
                {"kitchen-cluster-node2":"8",
                "kitchen-cluster-node3":"16"
                },
         "max_total_threads_dict":
                {"kitchen-cluster-node2":"64",
                "kitchen-cluster-node3":"64"
                }
         }
}
*** 2017-01-10: Code build failure: fix rm bug http://injenkins.fluigdata.com:48080/view/Basic/job/DockerDeploySandboxActiveSprint/157/console
  CLOSED: [2017-01-10 Tue 16:08]
touch /var/www/repo/$app_branch_name
touch $working_dir/$branch_name/$git_repo

find /var/www/repo -maxdepth 1 -name "[1-9]+\.[1-9]+" -type d -mtime +$old_build_retention_days -exec rm -rf '{}' +
find $working_dir -mindepth 2 -maxdepth 2 -type d -mtime +$old_build_retention_days -exec rm -rf {} +
*** 2017-01-10: Add prod-es-19 (in SFO2) to prod env
  CLOSED: [2017-01-10 Tue 17:36]
138.68.44.102
*** 2017-01-10: Add prod-es-21 (in SFO2) to prod env
  CLOSED: [2017-01-10 Tue 17:57]
138.197.193.202
*** 2017-01-10: Add prod-es-20 (in SFO2) to prod env
  CLOSED: [2017-01-10 Tue 17:57]
138.68.46.207
*** 2017-01-10: [#A] prod env maintaince: migrate ES cluster from SFO1 to SFO2
   CLOSED: [2017-01-10 Tue 19:00]
*** 2017-01-10: Add prod-es-22 (in SFO2) to prod env
  CLOSED: [2017-01-10 Tue 20:33]
138.197.198.250
*** 2017-01-10: Add prod-es-23 (in SFO2) to prod env
  CLOSED: [2017-01-10 Tue 20:33]
138.197.202.167
*** 2017-01-10: create jenkins job to create disk capacity of all es nodes: http://prodjenkins.fluigdata.com:18080/job/CheckESNodesDisk/1/console
   CLOSED: [2017-01-10 Tue 21:00]
*** 2017-01-11: [[https://github.com/TOTVS/mdmdevops/wiki/Critical-Maintenance-History-Of-Prod-Env][Add Wiki]]: Critical Maintenance History Of Prod Env
   CLOSED: [2017-01-11 Wed 10:33]
*** 2017-01-11: [[https://github.com/TOTVS/mdmdevops/wiki/Critical-Maintenance-History-Of-Explore-Env][Add wiki]]: Critical Maintenance History Of Explore Env
   CLOSED: [2017-01-11 Wed 10:39]
*** 2017-01-11: [[https://github.com/TOTVS/mdmdevops/wiki/Add-a-DigitalOcean-Volume-for-ES-node][Add wiki]]: Add a DigitalOcean Volume for ES node
   CLOSED: [2017-01-11 Wed 10:45]
*** 2017-01-11: Add prod-es-25 (in SFO2) to prod env
  CLOSED: [2017-01-11 Wed 11:55]
138.197.217.168
*** 2017-01-11: Add prod-es-24 (in SFO2) to prod env
  CLOSED: [2017-01-11 Wed 11:55]
138.197.217.103
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-01-11: remove prod-es-18 (107.170.223.238) <2017-01-11 16:00 UTC +8>
  CLOSED: [2017-01-11 Wed 22:46]
https://github.com/TOTVS/mdmdevops/wiki/MDM-Prod-Env-In-DigitalOcean

https://cloud.digitalocean.com/droplets?i=6872f6

- http://repo.fluigdata.com:18080/job/TCPScanReport/configure
- update 3 jobs
http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/configure
http://prodjenkins.fluigdata.com:18080/job/DeploySystemRehearsal/configure
http://prodjenkins.fluigdata.com:18080/job/DeploySystem/configure

- update wiki
- remove nagios
**** ufw update
107.170.223.238
*** 2017-01-11: remove prod-es-17 (107.170.218.112) <2017-01-11 18:08 UTC +8>
  CLOSED: [2017-01-11 Wed 22:46]
https://cloud.digitalocean.com/droplets/36746587/graphs?i=6872f6
*** 2017-01-11: remove prod-es-15 (107.170.253.222) <2017-01-11 19:08 UTC +8>
  CLOSED: [2017-01-11 Wed 22:46]
https://cloud.digitalocean.com/droplets?i=6872f6
*** 2017-01-11: remove prod-es-14 (107.170.216.152) <2017-01-11 19:32 UTC +8>
   CLOSED: [2017-01-11 Wed 22:46]
*** 2017-01-11: remove prod-es-13 (192.241.206.113) <2017-01-11 22:06 UTC +8>
   CLOSED: [2017-01-11 Wed 22:46]
*** 2017-01-12: remove prod-es-12 (192.241.228.149) <2017-01-12 08:46 UTC +8>
  CLOSED: [2017-01-12 Thu 10:17]
https://cloud.digitalocean.com/droplets/34327393/graphs?i=6872f6

http://repo.fluigdata.com:18080/job/TCPScanReport/configure
http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/configure
http://prodjenkins.fluigdata.com:18080/job/DeploySystemRehearsal/configure
http://prodjenkins.fluigdata.com:18080/job/DeploySystem/configure
*** 2017-01-12: remove prod-es-11 (107.170.252.123) <2017-01-12 09:08 UTC +8>
  CLOSED: [2017-01-12 Thu 10:17]
https://cloud.digitalocean.com/droplets?sort=name&sort_direction=asc&i=6872f6
*** 2017-01-12: remove prod-es-9 (198.199.95.111) <2017-01-12 09:43 UTC +8>
  CLOSED: [2017-01-12 Thu 10:17]
https://cloud.digitalocean.com/droplets?i=6872f6&page=2&sort=name&sort_direction=asc
*** 2017-01-12: update all jenkins jobs for code build for data retention
  CLOSED: [2017-01-12 Thu 10:22]
https://github.com/TOTVS/mdmdevops/commit/129c9352d7f77ee3af262d443a190caa76eb2270
diff --git a/cookbooks/jenkins-mdm/templates/default/BuildMDMRepo.xml.erb b/cookbooks/jenkins-mdm/templates/default/BuildMDMRepo.xml.erb
index 178e8bf..cdd963f 100644
--- a/cookbooks/jenkins-mdm/templates/default/BuildMDMRepo.xml.erb
+++ b/cookbooks/jenkins-mdm/templates/default/BuildMDMRepo.xml.erb
@@ -273,7 +273,8 @@ fi
 [ -n &quot;$old_build_retention_days&quot; ] || old_build_retention_days=7
 echo &quot;================= Remove old backkup older than $old_build_retention_days =================&quot;
 touch /var/www/repo/$app_branch_name
-touch $working_dir/$branch_name/$git_repo
+touch $working_dir/$framework_branch_name/$framework_github_repo
+touch $working_dir/$app_branch_name/$github_repo

 find /var/www/repo -maxdepth 1 -name &quot;[1-9]+\.[1-9]+&quot; -type d -mtime +$old_build_retention_days -exec rm -rf &apos;{}&apos; +
 find $working_dir -mindepth 2 -maxdepth 2 -type d -mtime +$old_build_retention_days -exec rm -rf {} +
*** 2017-01-12: add prod-es-17 in SFO2
  CLOSED: [2017-01-12 Thu 12:20]
138.197.216.23
*** 2017-01-12: add prod-es-18 in SFO2
  CLOSED: [2017-01-12 Thu 12:20]
138.197.217.98
*** 2017-01-12: remove prod-es-8 (192.241.203.166) <2017-01-12 11:17 UTC +8>
  CLOSED: [2017-01-12 Thu 13:43]
https://cloud.digitalocean.com/droplets/33431687/graphs?i=6872f6
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-01-19: [[https://trello.com/c/y7sP8mJH][#1833]] Support deployment without updating jar files: http://injenkins.fluigdata.com:48080/job/LongRunAllInOne/63/console
  CLOSED: [2017-01-12 Thu 14:23]
ls -lth /opt/mdm/

app_mdm::skip_update_jar

update doc:
*** 2017-01-12: remove prod-es-7 (107.170.237.239) <2017-01-12 14:27 UTC +8>
   CLOSED: [2017-01-12 Thu 16:59]
*** 2017-01-12: remove prod-es-10 (104.236.187.173) <2017-01-12 13:41 UTC +8>
   CLOSED: [2017-01-12 Thu 16:59]
*** 2017-01-12: remove prod-es-10 (104.236.187.173) <2017-01-12 13:41 UTC +8>
   CLOSED: [2017-01-12 Thu 16:59]
*** 2017-01-12: remove prod-es-6 (159.203.192.146) <2017-01-12 16:37 UTC +8>
  CLOSED: [2017-01-12 Thu 23:38]
master-index-8a6a5b30ade911e698710401f8d88c01  4     r      STARTED          64291  66.7mb 159.203.192.146 prod-es-6
staging-index-860245c0841e11e6a8260401f8d88101 0     r      STARTED              3   109kb 159.203.192.146 prod-es-6
staging-index-b74f5a703d5e11e68ac00401f8d88501 2     p      STARTED          17811  62.5mb 159.203.192.146 prod-es-6
master-index-d3ae95b0d12811e69edf0401f8d88501  1     r      STARTED              1   6.7kb 159.203.192.146 prod-es-6
master-index-41d17930c8e211e6aae30401f8d88101  4     p      STARTED              1   8.2kb 159.203.192.146 prod-es-6
staging-index-f1e6b200c8e611e6aae30401f8d88101 1     p      STARTED              0    159b 159.203.192.146 prod-es-6
staging-index-da1c1280ac9b11e68e250401f8d88501 4     r      STARTED        1035342 696.9mb 159.203.192.146 prod-es-6
master-index-da1c1280ac9b11e68e250401f8d88501  0     p      STARTED              1   6.6kb 159.203.192.146 prod-es-6
staging-index-41d17930c8e211e6aae30401f8d88101 0     p      STARTED              0    159b 159.203.192.146 prod-es-6
staging-index-abae8b30ac9b11e692000401f8d88101 1     p      STARTED         655138     7gb 159.203.192.146 prod-es-6
staging-fd1125e03cb711e6878f0401f8d88c01       3     p      STARTED              0    159b 159.203.192.146 prod-es-6
master-index-5e527960c34211e6b3ff0401f8d88c01  3     p      STARTED        5164013   3.8gb 159.203.192.146 prod-es-6
master-index-025e2270c5cd11e6b9610401f8d88501  4     p      STARTED              1   8.2kb 159.203.192.146 prod-es-6
staging-index-cf5e90403d5e11e68ac00401f8d88501 2     r      STARTED          33646  96.4mb 159.203.192.146 prod-es-6
master-index-abae8b30ac9b11e692000401f8d88101  3     p      STARTED      165991101 175.5gb 159.203.192.146 prod-es-6
staging-index-5e527960c34211e6b3ff0401f8d88c01 3     r      STARTED         310291 777.4mb 159.203.192.146 prod-es-6
master-index-cf5e90403d5e11e68ac00401f8d88501  3     r      STARTED         579746 558.5mb 159.203.192.146 prod-es-6
scroll                                         2     r      STARTED              0    159b 159.203.192.146 prod-es-6
staging-index-025e2270c5cd11e6b9610401f8d88501 3     p      STARTED              0    159b 159.203.192.146 prod-es-6
master-index-3366bf206f2a11e694f80401f8d88c01  0     p      STARTED         429979   372mb 159.203.192.146 prod-es-6
master-index-f1e6b200c8e611e6aae30401f8d88101  3     p      STARTED              0    159b 159.203.192.146 prod-es-6
staging-index-e4010da4110ba377d100f050cb4440db 2     p      STARTED           6684  22.5mb 159.203.192.146 prod-es-6
staging-index-d3ae95b0d12811e69edf0401f8d88501 4     p      STARTED              1   8.1kb 159.203.192.146 prod-es-6
master-index-8cd6e43115e9416eb23609486fa053e3  1     p      STARTED        4301305   3.6gb 159.203.192.146 prod-es-6
master-index-b74f5a703d5e11e68ac00401f8d88501  2     p      STARTED         450881 283.5mb 159.203.192.146 prod-es-6
*** 2017-01-12: remove prod-es-5 (159.203.211.150) <2017-01-12 17:22 UTC +8>
  CLOSED: [2017-01-12 Thu 23:38]
staging-index-8a6a5b30ade911e698710401f8d88c01 3     r      STARTED             10   1.4mb 159.203.211.150 prod-es-5
master-index-8a6a5b30ade911e698710401f8d88c01  4     r      STARTED          64291  66.7mb 159.203.211.150 prod-es-5
staging-index-860245c0841e11e6a8260401f8d88101 0     p      STARTED              3   109kb 159.203.211.150 prod-es-5
staging-index-b74f5a703d5e11e68ac00401f8d88501 4     p      STARTED          17533  61.2mb 159.203.211.150 prod-es-5
staging-index-8cd6e43115e9416eb23609486fa053e3 0     r      INITIALIZING                   159.203.211.150 prod-es-5
master-index-d3ae95b0d12811e69edf0401f8d88501  2     p      STARTED              0    159b 159.203.211.150 prod-es-5
master-index-41d17930c8e211e6aae30401f8d88101  3     r      STARTED              1  10.4kb 159.203.211.150 prod-es-5
staging-index-f1e6b200c8e611e6aae30401f8d88101 0     r      STARTED              0    159b 159.203.211.150 prod-es-5
staging-index-da1c1280ac9b11e68e250401f8d88501 3     r      STARTED        1036659 701.2mb 159.203.211.150 prod-es-5
master-index-da1c1280ac9b11e68e250401f8d88501  3     r      STARTED              0    159b 159.203.211.150 prod-es-5
staging-index-41d17930c8e211e6aae30401f8d88101 2     r      STARTED              0    159b 159.203.211.150 prod-es-5
staging-index-abae8b30ac9b11e692000401f8d88101 3     r      STARTED         650121   6.6gb 159.203.211.150 prod-es-5
staging-fd1125e03cb711e6878f0401f8d88c01       2     p      STARTED              0    159b 159.203.211.150 prod-es-5
master-index-5e527960c34211e6b3ff0401f8d88c01  2     r      STARTED        5151177   3.7gb 159.203.211.150 prod-es-5
master-index-025e2270c5cd11e6b9610401f8d88501  3     p      STARTED              0    159b 159.203.211.150 prod-es-5
staging-index-cf5e90403d5e11e68ac00401f8d88501 4     p      STARTED          33457 127.4mb 159.203.211.150 prod-es-5
master-index-abae8b30ac9b11e692000401f8d88101  4     r      STARTED      165799337   147gb 159.203.211.150 prod-es-5
staging-index-5e527960c34211e6b3ff0401f8d88c01 4     r      STARTED         316782 735.4mb 159.203.211.150 prod-es-5
master-index-cf5e90403d5e11e68ac00401f8d88501  4     r      STARTED         582318 460.6mb 159.203.211.150 prod-es-5
scroll                                         4     p      STARTED              0    159b 159.203.211.150 prod-es-5
staging-index-025e2270c5cd11e6b9610401f8d88501 2     r      STARTED              0    159b 159.203.211.150 prod-es-5
master-index-e4010da4110ba377d100f050cb4440db  2     p      STARTED          31858    47mb 159.203.211.150 prod-es-5
master-index-f1e6b200c8e611e6aae30401f8d88101  1     r      STARTED              0    159b 159.203.211.150 prod-es-5
staging-index-e4010da4110ba377d100f050cb4440db 0     r      STARTED           6711  26.2mb 159.203.211.150 prod-es-5
master-index-8cd6e43115e9416eb23609486fa053e3  2     r      STARTED        4298940   3.6gb 159.203.211.150 prod-es-5
master-index-b74f5a703d5e11e68ac00401f8d88501  4     r      STARTED         451303 338.1mb 159.203.211.150 prod-es-5
*** 2017-01-12: remove prod-es-4 (159.203.219.53)
  CLOSED: [2017-01-12 Thu 23:38]
staging-index-8a6a5b30ade911e698710401f8d88c01 2     p      STARTED             48 623.8kb 159.203.219.53  prod-es-4
master-index-8a6a5b30ade911e698710401f8d88c01  0     p      STARTED          65583  71.2mb 159.203.219.53  prod-es-4
staging-index-799e458055c611e6bb000401f8d88101 0     r      STARTED          51810 136.2mb 159.203.219.53  prod-es-4
staging-index-b74f5a703d5e11e68ac00401f8d88501 4     r      STARTED          17533  61.2mb 159.203.219.53  prod-es-4
staging-index-8cd6e43115e9416eb23609486fa053e3 4     p      STARTED       17245659  16.5gb 159.203.219.53  prod-es-4
master-index-d3ae95b0d12811e69edf0401f8d88501  3     r      STARTED              0    159b 159.203.219.53  prod-es-4
master-index-41d17930c8e211e6aae30401f8d88101  2     p      STARTED              0    159b 159.203.219.53  prod-es-4
staging-index-f1e6b200c8e611e6aae30401f8d88101 0     p      STARTED              0    159b 159.203.219.53  prod-es-4
staging-index-da1c1280ac9b11e68e250401f8d88501 3     p      STARTED        1036659 701.2mb 159.203.219.53  prod-es-4
master-index-3fa847206b9911e6b61d0401f8d88101  0     r      STARTED              4  33.4kb 159.203.219.53  prod-es-4
master-index-da1c1280ac9b11e68e250401f8d88501  1     p      STARTED              0    159b 159.203.219.53  prod-es-4
staging-index-41d17930c8e211e6aae30401f8d88101 4     r      STARTED              0    159b 159.203.219.53  prod-es-4
staging-index-abae8b30ac9b11e692000401f8d88101 0     p      STARTED         649504     5gb 159.203.219.53  prod-es-4
master-index-5e527960c34211e6b3ff0401f8d88c01  1     p      STARTED        5163269   4.9gb 159.203.219.53  prod-es-4
master-index-025e2270c5cd11e6b9610401f8d88501  0     p      STARTED              1   8.2kb 159.203.219.53  prod-es-4
staging-index-cf5e90403d5e11e68ac00401f8d88501 3     p      STARTED          33987   107mb 159.203.219.53  prod-es-4
master-index-abae8b30ac9b11e692000401f8d88101  4     p      STARTED      165807163 146.9gb 159.203.219.53  prod-es-4
staging-index-5e527960c34211e6b3ff0401f8d88c01 3     p      STARTED         310285 759.6mb 159.203.219.53  prod-es-4
master-index-cf5e90403d5e11e68ac00401f8d88501  4     r      STARTED         582318 460.6mb 159.203.219.53  prod-es-4
scroll                                         0     r      STARTED              0    159b 159.203.219.53  prod-es-4
staging-index-025e2270c5cd11e6b9610401f8d88501 4     p      STARTED              0    159b 159.203.219.53  prod-es-4
master-index-e4010da4110ba377d100f050cb4440db  4     r      STARTED          31694    44mb 159.203.219.53  prod-es-4
master-index-f1e6b200c8e611e6aae30401f8d88101  2     r      STARTED              0    159b 159.203.219.53  prod-es-4
staging-index-e4010da4110ba377d100f050cb4440db 1     r      STARTED           6758  25.6mb 159.203.219.53  prod-es-4
staging-index-d3ae95b0d12811e69edf0401f8d88501 3     r      STARTED              0    159b 159.203.219.53  prod-es-4
master-index-8cd6e43115e9416eb23609486fa053e3  0     p      STARTED        4292062   3.6gb 159.203.219.53  prod-es-4
master-index-b74f5a703d5e11e68ac00401f8d88501  3     r      STARTED         449084   311mb 159.203.219.53  prod-es-4
*** 2017-01-13: remove prod-es-3 (192.241.211.99) <2017-01-12 22:10 UTC +8>
  CLOSED: [2017-01-13 Fri 10:10]
https://github.com/TOTVS/mdmdevops/wiki/MDM-Prod-Env-In-DigitalOcean/_edit
http://repo.fluigdata.com:18080/job/TCPScanReport/configure
http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/configure
http://prodjenkins.fluigdata.com:18080/job/DeploySystemRehearsal/configure
http://prodjenkins.fluigdata.com:18080/job/DeploySystem/configure
*** 2017-01-13: remove prod-es-2 (107.170.212.76) <2017-01-12 23:45 UTC +8>
   CLOSED: [2017-01-13 Fri 10:10]
*** 2017-01-13: remove prod-es-1 (159.203.216.25) <2017-01-13 08:50 UTC +8>
   CLOSED: [2017-01-13 Fri 10:46]
*** 2017-01-13: inject roboson's key: http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/209/console
  CLOSED: [2017-01-13 Fri 11:16]
cp /root/.ssh/authorized_keys /root/.ssh/authorized_keys.bak
echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7TomREpvJgmH0GokMx62fh+GOMQ8kQ7Rax0u2lMC3FoXvQw/GLj46Z8yAaNwy2l9/JSkPtOeXdY+VYJNNKFJ2jznU+M/f3VtqBYNdRh4gScOcNUayEH2x8m7PP9UaJ7vg/iyksvkQLQQ4D1rKLMZBjMM4sCvbRVqvT7cbCAn6+DqOM32+1DG17kQVFjql2ns9GRVIsaGdJz8y/KtIGmygzGT/mJjiv1A/++ddnR/lP2JhV0nDR28L/9fR+TzYa7DKUNKwKF5l9DzwYAmFKuJCJRGwnFbw28Pg5lwop2v+tF2aikED7a85bXZX9zMVFxkYe8mcvN5v+Hp3nVF32fRd robson.poffo@totvs.com.br" >> /root/.ssh/authorized_keys
*** 2017-01-15: rename endpoint for prod-es-16
  CLOSED: [2017-01-15 Sun 08:52]
curl -XPUT "http://$es_ip:9200/_cluster/settings" -d '
{
  "persistent": {
    "cluster.routing.allocation.enable": "none"
  }
}'

curl -XPOST "http://$es_ip:9200/_flush/synced"

curl $es_ip:9200/_cluster/health?pretty
curl $es_ip:9200/_cat/nodes?v

curl -XPUT "http://$es_ip:9200/_cluster/settings" -d '
{
  "persistent": {
    "cluster.routing.allocation.enable": "all"
  }
}'

#+BEGIN_EXAMPLE
# login to ES-16 node
# Step 1: Disable shard allocation
curl -XPUT "http://`ifconfig | grep inet | awk '{print $2}' | grep -v 127 | cut -d":" -f2`:9200/_cluster/settings" -d '
{
  "persistent": {
    "cluster.routing.allocation.enable": "none"
  }
}'
# Step 2: Perform a synced flush
# Shard recovery will be much faster if you stop indexing and issue a synced-flush request:
curl -POST "http://`ifconfig | grep inet | awk '{print $2}' | grep -v 127 | cut -d":" -f2`:9200/_flush/synced"
# shutdown ES server
/etc/init.d/elasticsearch stop
# Step 4: rename folder and modify elasticsearch.yml file
a. umount /mnt/prod-es-16-volume
b. mv /mnt/prod-es-16-volume /mnt/es-extra-volume
c. change /etc/fstab to use /mnt/es-extra-volume
d. change /etc/elasticsearch/elasticsearch.yml to use /mnt/es-extra-volume
e. mount /mnt/es-extra-volume
# Step 5: Start the ES server
/etc/init.d/elasticsearch start
# Step 5: Wait for yellow
curl -XGET "http://`ifconfig | grep inet | awk '{print $2}' | grep -v 127 | cut -d":" -f2`:9200/_cat/health"
curl -XGET "http://`ifconfig | grep inet | awk '{print $2}' | grep -v 127 | cut -d":" -f2`:9200/_cat/nodes" | grep -v client
# Step 6: Reenable allocation
curl -XPUT "http://`ifconfig | grep inet | awk '{print $2}' | grep -v 127 | cut -d":" -f2`:9200/_cluster/settings" -d '
{
  "persistent": {
    "cluster.routing.allocation.enable": "all"
  }
}'
# Step 7: monitor progress
curl -XGET "http://`ifconfig | grep inet | awk '{print $2}' | grep -v 127 | cut -d":" -f2`:9200/_cat/health"
curl -XGET "http://`ifconfig | grep inet | awk '{print $2}' | grep -v 127 | cut -d":" -f2`:9200/_cat/recovery"
#+END_EXAMPLE
*** 2017-01-15: [#A] Make sure /etc/elasticsearch/elasticsearch.yml properly updated
  CLOSED: [2017-01-15 Sun 14:07]
http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/221/console
http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/222/console

cp /etc/elasticsearch/elasticsearch.yml /tmp/elasticsearch.yml
# discovery.zen.ping.unicast.hosts: ["138.68.3.169:9300", "138.197.216.23:9300", "138.197.217.98:9300", "138.68.44.102:9300", "138.68.46.207:9300", "138.197.193.202:9300", "138.197.198.250:9300", "138.197.202.167:9300", "138.197.217.103:9300", "138.197.217.168:9300"]
sed -i 's/discovery.zen.ping.unicast.hosts.*/discovery.zen.ping.unicast.hosts: ["138.68.3.169:9300", "138.197.216.23:9300", "138.197.217.98:9300", "138.68.44.102:9300", "138.68.46.207:9300", "138.197.193.202:9300", "138.197.198.250:9300", "138.197.202.167:9300", "138.197.217.103:9300", "138.197.217.168:9300"]/g' /tmp/elasticsearch.yml
grep discovery.zen.ping.unicast.hosts /tmp/elasticsearch.yml
diff /etc/elasticsearch/elasticsearch.yml /tmp/elasticsearch.yml
*** 2017-01-15: change dns
  CLOSED: [2017-01-15 Sun 14:12]
DNS entry: www.Totvslabs.com
Currently: totvslabs.com
Change to CNAME: stsur.x.incapdns.net

DNS entry: Totvslabs.com
Currently: 104.236.159.226
Change to A: 107.154.105.24 Add A: 107.154.106.24

*** 2017-02-01: [#A] after deployment, cleanup couchbase mounted volume
  CLOSED: [2017-02-01 Wed 09:58]
https://github.com/TOTVS/mdmdevops/commit/3d888535e81e43f76de54cb1c1cca8dee1b407f7
#+BEGIN_EXAMPLE
cookbooks/all-in-one/.kitchen.hooks/post-destroy.sh
@@ -0,0 +1,8 @@
 +#!/bin/bash
 +if kitchen list | grep ' Docker ' 1>/dev/null 2>&1; then
 +    if kitchen exec .* -c "sudo test -f /opt/couchbase/" 2>/dev/null; then
 +        command="kitchen exec .* -c 'sudo rm -rf /opt/couchbase/*'"
 +        echo "run: $command"
 +        eval "$command" || true
 +    fi
 +fi
#+END_EXAMPLE
*** couchbase memcached fail to start
https://forums.couchbase.com/t/memcached-server-wont-start-after-install-on-rhel6/1094

docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test totvslabs/mdm:latest /bin/bash
docker exec -it my-test bash

docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test 0f10c08645f6 /bin/bash
docker exec -it my-test bash

lsof -i tcp:11210 | grep LISTEN

cd /opt/couchbase/var/lib/couchbase/logs

grep version /opt/couchbase/etc/runtime.ini

root@kitchen-mdm-feature-node1:/opt/couchbase/backup# grep version /opt/couchbase/etc/runtime.ini
version = 4.1.0-5005

telnet localhost 11210
root@kitchen-mdm-feature-node1:/opt/couchbase/var/lib/couchbase# ps -ef | grep memcached
couchba+ 32400 32241 37 05:49 ?        00:01:51 /opt/couchbase/bin/memcached -C /opt/couchbase/var/lib/couchbase/config/memcached.json


root@all-in-one-DockerDeployAllInOne-235:/# /opt/couchbase/bin/memcached -C /opt/couchbase/var/lib/couchbase/config/>
2017-02-01T05:55:12.654819Z WARNING NUMA: Set memory allocation policy to 'interleave'.
2017-02-01T05:55:12.654937Z WARNING Restarting file logging
2017-02-01T05:55:12.666194Z WARNING Breakpad caught crash in memcached. Writing crash dump to /opt/couchbase/var/lib/couchbase/crash/41c27375-08e5-e920-3de54ba2-78bbaec0.dmp before terminating.
2017-02-01T05:55:12.666306Z WARNING Stack backtrace of crashed thread:
2017-02-01T05:55:12.667260Z WARNING     /opt/couchbase/bin/memcached() [0x40bf44]
2017-02-01T05:55:12.667317Z WARNING     /opt/couchbase/bin/memcached(_ZN15google_breakpad16ExceptionHandler12GenerateDumpEPNS0_12CrashContextE+0x3d4) [0x434824]
2017-02-01T05:55:12.667338Z WARNING     /opt/couchbase/bin/memcached() [0x434a25]
2017-02-01T05:55:12.667355Z WARNING     /opt/couchbase/bin/memcached(_ZN15google_breakpad16ExceptionHandler13SignalHandlerEiP9siginfo_tPv+0x97) [0x434b67]
2017-02-01T05:55:12.667379Z WARNING     /lib/x86_64-linux-gnu/libpthread.so.0() [0x7f2f9520a330]
2017-02-01T05:55:12.667399Z WARNING     [0x7f2f9710f6b0]
Segmentation fault

root@all-in-one-DockerDeployAllInOne-235:/opt/couchbase/var/lib/couchbase/logs# tail ./memcached.log.9.txt
2017-02-01T05:57:15.258740Z WARNING Restarting file logging
2017-02-01T05:57:15.258619Z WARNING NUMA: Set memory allocation policy to 'interleave'.
2017-02-01T05:57:15.271256Z WARNING Breakpad caught crash in memcached. Writing crash dump to /opt/couchbase/var/lib/couchbase/crash/0c4dbc28-f4aa-33b3-2df1503e-6a32037d.dmp before terminating.
2017-02-01T05:57:15.271326Z WARNING Stack backtrace of crashed thread:
2017-02-01T05:57:15.271505Z WARNING     /opt/couchbase/bin/memcached() [0x40bf44]
2017-02-01T05:57:15.271526Z WARNING     /opt/couchbase/bin/memcached(_ZN15google_breakpad16ExceptionHandler12GenerateDumpEPNS0_12CrashContextE+0x3d4) [0x434824]
2017-02-01T05:57:15.271533Z WARNING     /opt/couchbase/bin/memcached() [0x434a25]
2017-02-01T05:57:15.271539Z WARNING     /opt/couchbase/bin/memcached(_ZN15google_breakpad16ExceptionHandler13SignalHandlerEiP9siginfo_tPv+0x97) [0x434b67]
2017-02-01T05:57:15.271549Z WARNING     /lib/x86_64-linux-gnu/libpthread.so.0() [0x7f5fd7b3b330]
2017-02-01T05:57:15.271554Z WARNING     [0x7f5fd9a406b0]** 2017-01-15: [[https://trello.com/c/y7sP8mJH][#1833]] Support deployment without updating jar files
  CLOSED: [2017-01-16 Mon 10:23]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-01-16: keep config in sync
   CLOSED: [2017-01-16 Mon 11:01]

*** 2017-01-17: [[https://trello.com/c/7crGWzRL][#1852]] For prod env, migrate all existing servers to SFO2
   CLOSED: [2017-01-17 Tue 11:15]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-01-17: migrate app cluster to SFO2: http://prodjenkins.fluigdata.com:18080/job/DeploySystem/168/console
  CLOSED: [2017-01-17 Tue 11:04]
# old
159.203.234.164:2702
159.203.202.27:2702
159.203.198.98:2702
162.243.155.164:2702
45.55.1.180:2702
45.55.9.157:2702

# new
138.197.193.53:2702
138.197.199.11:2702
138.197.199.169:2702
138.197.214.85:2702
138.197.222.35:2702
138.197.217.166:2702

8 Core/ 16 GB Memory / 160 GB Disk / SFO1
**** 2017-01-17: Add prod-app-01 in SFO2
   CLOSED: [2017-01-16 Mon 21:45]
138.197.193.53
**** 2017-01-17: Add prod-app-02 in SFO2
   CLOSED: [2017-01-16 Mon 21:45]
138.197.199.11
**** 2017-01-17: Add prod-app-03 in SFO2
   CLOSED: [2017-01-16 Mon 21:45]
138.197.199.169
**** 2017-01-17: Add prod-app-04 in SFO2
   CLOSED: [2017-01-16 Mon 21:45]
138.197.214.85
**** 2017-01-17: Add prod-app-05 in SFO2
   CLOSED: [2017-01-16 Mon 21:45]
138.197.222.35
**** 2017-01-17: Add prod-app-06 in SFO2
   CLOSED: [2017-01-16 Mon 21:45]
138.197.217.166
**** #  --8<-------------------------- separator ------------------------>8--
**** stop and remove prod-app-1 in SFO1
159.203.234.164:2702
**** stop and remove prod-app-2 in SFO1
159.203.202.27:2702
**** stop and remove prod-app-3 in SFO1
159.203.198.98:2702
**** stop and remove prod-app-4 in SFO1
162.243.155.164:2702
**** stop and remove prod-app-5 in SFO1
45.55.1.180:2702
**** stop and remove prod-app-6 in SFO1
45.55.9.157:2702
*** 2017-01-17: migrate couchbase cluster to SFO2
  CLOSED: [2017-01-16 Mon 22:00]
https://github.com/TOTVS/mdmdevops/wiki/MDM-Prod-Env-In-DigitalOcean/_edit

http://repo.fluigdata.com:18080/job/TCPScanReport/configure
http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/configure
http://prodjenkins.fluigdata.com:18080/job/DeploySystemRehearsal/configure
http://prodjenkins.fluigdata.com:18080/job/DeploySystem/configure
**** 2017-01-17: Add prod-cb-01 in SFO2 to prod env
   CLOSED: [2017-01-16 Mon 18:12]
prod-cb-01 138.197.194.193
**** 2017-01-17: Add prod-cb-02 in SFO2 to prod env <2017-01-16 14:07 UTC +8>
   CLOSED: [2017-01-16 Mon 18:12]
prod-cb-02 138.197.193.196
**** 2017-01-17: remove prod-cb-2 in SFO1 <2017-01-16 15:23 UTC +8>
    CLOSED: [2017-01-16 Mon 18:12]
**** 2017-01-17: Add prod-cb-03 in SFO2 to prod env <2017-01-16 15:55 UTC +8>
   CLOSED: [2017-01-16 Mon 18:12]
prod-cb-03 138.197.201.198
**** 2017-01-17: Add prod-cb-04 in SFO2 to prod env <2017-01-16 16:24 UTC +8>
   CLOSED: [2017-01-16 Mon 18:12]
prod-cb-04 138.68.3.236
**** 2017-01-17: remove prod-cb-3 in SFO1 <2017-01-16 17:13 UTC +8>
   CLOSED: [2017-01-16 Mon 18:12]
104.236.179.76
https://cloud.digitalocean.com/droplets/16306902/power?i=6872f6
**** 2017-01-17: failover and remove prod-cb-4 in SFO1 <2017-01-16 17:40 UTC +8>
   CLOSED: [2017-01-16 Mon 18:12]
159.203.247.196
**** 2017-01-17: Add prod-cb-05 in SFO2 to prod env <2017-01-16 18:12 UTC +8>
   CLOSED: [2017-01-16 Mon 18:22]
138.68.17.99
**** 2017-01-17: Add prod-cb-06 in SFO2 to prod env <2017-01-16 18:47 UTC +8>
   CLOSED: [2017-01-16 Mon 18:57]
138.197.193.160
**** 2017-01-17: Add prod-cb-07 in SFO2 to prod env <2017-01-16 19:19 UTC +8>
   CLOSED: [2017-01-16 Mon 19:32]
138.197.217.56
**** 2017-01-17: failover and remove prod-cb-5 in SFO1 <2017-01-16 19:46 UTC +8>
   CLOSED: [2017-01-16 Mon 20:33]
104.236.148.126
**** 2017-01-17: failover and remove prod-cb-6 in SFO1 <2017-01-16 20:07 UTC +8>
   CLOSED: [2017-01-16 Mon 20:33]
104.236.129.147
**** 2017-01-17: failover and remove prod-cb-7 in SFO1 <2017-01-16 20:33 UTC +8>
   CLOSED: [2017-01-16 Mon 20:33]
192.241.225.40
**** #  --8<-------------------------- separator ------------------------>8--
**** TODO failover and remove prod-cb-1 in SFO1
159.203.198.129

*** 2017-01-17: migrate loadbalancer cluster to SFO2: http://prodjenkins.fluigdata.com:18080/job/DeploySystem/168/console
  CLOSED: [2017-01-17 Tue 14:01]
ping fluigdata.com
ping app.fluigdata.com
ping totvslabs.fluigdata.com

# old
159.203.198.171:2702
159.203.202.250:2702

# new
138.197.210.243:2702
138.197.221.100:2702
**** add prod-lb-01, prod-lb-02 in SFO2
138.197.210.243
138.197.221.100

*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-01-18: [[https://trello.com/c/ieODPyNd][#1785]] Monitor mdm thread count and run jstack dump if necessary: http://injenkins.fluigdata.com:48080/view/Basic/job/DockerDeployAllInOne/167/console
  CLOSED: [2017-01-20 Fri 15:56]
> /tmp/check_proc_threadcount.sh && vim /tmp/check_proc_threadcount.sh

bash /tmp/check_proc_threadcount.sh -w 1024 -c 2048 --pid 20875

Copy check_java_threadcount folder to devops code

http://stackoverflow.com/questions/3677563/how-expensive-is-it-to-execute-jstack-on-a-running-jvm
#+BEGIN_EXAMPLE
Kung Wang [4:35 PM]
so, once every five minutes?

denny zhang [4:36 PM]
Nice. Let me update the code and enforce this early next week.

Kung Wang [4:38 PM]
so, every 5 mins will just collect count, but once we find count is high(needs definition from Bruno), then we dump the jstack to file? what do you guys think?

denny zhang [4:39 PM]
Good to me.

#+END_EXAMPLE
*** 2017-01-18: nagios monitor mdm log for OutOfMemoryException pattern
  CLOSED: [2017-01-16 Mon 14:32]
]133;D;0]1337;RemoteHost=root@prod-app-1]1337;CurrentDir=/opt/mdm/logs]133;Aroot@prod-app-1:/opt/mdm/logs# ]133;Bgrep -C 3 CouchbaseOutOfMemoryException   mdm-initscript.log
grep -C 3 CouchbaseOutOfMemoryException   mdm-initscript.log
]133;C;! at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
! at java.lang.Thread.run(Thread.java:745)

[16 Jan 2017;14:30:26.303]-[ERROR][TenantResource - logResponseForException:71 - dw-14489 - GET /api/v1/tenants/5e527960c34211e6b3ff0401f8d88c01/possibleEntityTemplates - T_5c2051] - Exception occurred in operation: 'TenantResource - getPossibleEntityTemplates' bubbled to the service layer - com.couchbase.client.java.error.CouchbaseOutOfMemoryException ! com.couchbase.client.java.error.CouchbaseOutOfMemoryException: null
! at com.couchbase.client.java.CouchbaseAsyncBucket$2.call(CouchbaseAsyncBucket.java:251)
! at com.couchbase.client.java.CouchbaseAsyncBucket$2.call(CouchbaseAsyncBucket.java:233)
! at rx.internal.operators.OperatorFilter$1.onNext(OperatorFilter.java:53)
*** 2017-01-19: [[https://github.com/TOTVS/mdmdevops/wiki/Add-an-Azure-Volume-for-ES-node][Add wiki]]: Add an Azure Volume for ES node
   CLOSED: [2017-01-19 Thu 16:48]
*** 2017-01-20: [[https://github.com/TOTVS/mdmdevops/wiki/Add-an-Azure-Volume-for-CB-node][Add wiki]]: Add an Azure Volume for CB node
   CLOSED: [2017-01-20 Fri 11:09]
*** 2017-01-20: create ms sql server vm in azure
  CLOSED: [2017-01-20 Fri 15:48]
william-aimssql

server ip: 191.232.247.91
port: 1433
username: totvs_msssql
password: ChangeMe@TOTVSLabs1

#+BEGIN_EXAMPLE
Basics
Subscription
FluigData
Resource group
mysql-resourcegroup
Location
Brazil South
Settings
Computer name
william-aimssql
Disk type
SSD
User name
totvslabs_mysql
Size
Standard DS3 v2
Storage account
(new) mysqlresourcegroup840
Virtual network
(new) mysql-resourcegroup-vnet
Subnet
(new) default (10.0.0.0/24)
Public IP address
(new) william-aimssql-ip
Network security group (firewall)
(new) william-aimssql-nsg
Availability set
None
Guest OS diagnostics
Disabled
Boot diagnostics
Enabled
Diagnostics storage account
(new) mysqlresourcegroup183
SQL Server settings
SQL connectivity level
Private
SQL port
1433
SQL Authentication
Disabled
R Services (Advanced analytics)
Disabled
Storage optimization type
General
Storage size
1 (TB)
Storage performance
5000 IOPS, 192 Throughput (MBps)
Automated patching
Enabled
Auto patching schedule
Sunday at 2:00
Automated backup
Disabled
Azure Key Vault integration
Disabled
#+END_EXAMPLE

Kung Wang [12:01 PM] 
Denny, we also need an machine with MS SQL server in it. Do you see any service in Azure that can provision a machine with MS SQL default installed?

[12:01]  
in this case, we don’t need to provide license for MS SQL.

denny zhang [12:01 PM] 
It has

Kung Wang [12:02 PM] 
nice, can you create one for me? with 16G RAM and around 500G disk

[12:02]  
it will be used by our AI team.

denny zhang [12:03 PM] 
Would type#2 works?
https://github.com/TOTVS/mdmdevops/wiki/Bematech-Env-In-Azure#machine-list-for-bematech-prod-env-in-azure

Kung Wang [12:03 PM] 
yes

denny zhang [12:04 PM] 
uploaded this image: Pasted image at 2017-01-20, 12:03 PM
Add Comment

denny zhang [12:04 PM] 
And I name it as william-ai-mssql. Sounds good?

*** 2017-01-20: Create Bematech cluster In Azure
   CLOSED: [2017-01-20 Fri 15:57]
*** 2017-01-20: janitor work: remove firewall rules in existing nodes: http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/260/console
   CLOSED: [2017-01-20 Fri 19:42]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-01-30: [#A] Soteria: 2h: [[https://bitbucket.org/nubesecure/devops/issues/3/decouple-code-build-and-image-build][#3]] Decouple code build and image build process
  CLOSED: [2017-01-30 Mon 22:55]
PR: 

https://bitbucket.org/nubesecure/brozton/pull-requests/1/copy-generated-artifact-to-docker-images/diff

#+BEGIN_EXAMPLE
dennyzhang [10:19 AM] 
About "Decouple code build and image build process"

dennyzhang [10:31 AM] 
I’m thinking process like this.
```1. Create docker autobuild repo in docker hub for each docker image. So we will see multiple docker autobuild repos in docker hub against dofacdenny repo in bitbucket.
2. In penroz and brozton, bitbucket pipeline will trigger gradle build, whenever we have a new check-in for given branch. (This is what we're doing)
3. At the end of the pipeline, we trigger a hook. The hook will do a new build, and upload those jarfiles and config templates into dofacdenny repo. 

*If we can use files generated by bitbucket build and git push them directly, that would be even easier and more reliable. Thus we neither need extra machine nor extra code build.*
4. docker autobuild repo will automatically detect new git pushs, and run docker build and docker push automatically.
```

We will attack below questions later:
```1. Get timely alerts, once docker image build fail
2. How to verify docker images functionally work, after docker hub autobuild```
#+END_EXAMPLE
*** 2017-02-02: 1h: Soteria: Update Dockerfile format: add Images Name, bitbucket url
  CLOSED: [2017-02-02 Thu 22:39]
https://bitbucket.org/nubesecure/dofacdenny/commits/c4a5f46c691ee98c8c3565b4f7740fc356d19bc8
https://bitbucket.org/nubesecure/dofacdenny/commits/3b9a32fb7e19c45a21582efe04e9c082b81bf3a0
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-01-23: [[https://trello.com/c/hIBuelBy][#1898]] RunCommandOnServers job: show hostname first, before running ssh commands
   CLOSED: [2017-01-23 Mon 17:04]
*** 2017-01-24: [[https://github.com/TOTVS/mdmdevops/wiki/Build-Deploy-Test-Against-Any-Branch][Add wiki]]: Build Deploy Test Against Any Branch
   CLOSED: [2017-01-24 Tue 11:10]
*** 2017-01-24: [[https://trello.com/c/95bhN73J][#1706]] sshd hardening for vulnerability scan: http://injenkins.fluigdata.com:48080/view/Basic/job/DockerDeployFeatureCookbooks/189/console
  CLOSED: [2017-01-23 Mon 15:08]
http://linux.uits.uconn.edu/2014/06/25/ssh-weak-ciphers-and-mac-algorithms/

ssh -vvv -p 2702 root@45.33.87.74 date 2>&1 | grep cipher

arcfour

*** 2017-01-25: Prepare servers for bematech migration
  CLOSED: [2017-01-25 Wed 12:02]
2 cb nodes: bematech-cb-6, bematech-cb-7
2 es nodes: bematech-es-9, bematech-es-10
1 app nodes: bematech-app-6
**** 2017-01-25: add cb nodes: http://bematechjenkins.fluigdata.com:18080/job/DeploySystem/2/console
  CLOSED: [2017-01-25 Wed 10:12]
2.46TB -> 3.46 TB
**** 2017-01-25: Add bematech-es-9, bematech-es-10: http://bematechjenkins.fluigdata.com:18080/job/DeploySystem/3/console
    CLOSED: [2017-01-25 Wed 10:20]
**** 2017-01-25: add bematech-app-6 and enable monitoring: http://bematechjenkins.fluigdata.com:18080/job/DeploySystem/4/console
    CLOSED: [2017-01-25 Wed 11:49]
*** 2017-01-25: Update chef code: apache2 starts in all nodes, because we have installed a nagios plugin
  CLOSED: [2017-01-25 Wed 13:41]
- disable apache2 from autostart
- chef stop apache2
*** 2017-01-26: [[https://trello.com/c/e3W8AVdN][#1707]] Provide java debugging facility scripts in python
  CLOSED: [2017-01-27 Fri 15:30]
export JAVA_ANALYZE_ACTION="analyze_gc_logfile"
export JAVA_ANALYZE_LOGFILE="/tmp/gc.log"
export JAVA_ANALYZE_APIKEY="29792f0d-5d5f-43ad-9358-ee0e8d8ab439"

python /tmp/java_analyze.py

> /tmp/java_analyze.py && vim /tmp/java_analyze.py

export JAVA_ANALYZE_ACTION="analyze_jstack_logfile"
export JAVA_ANALYZE_LOGFILE="/tmp/jstack.log"
export JAVA_ANALYZE_APIKEY="29792f0d-5d5f-43ad-9358-ee0e8d8ab439"

python /tmp/java_analyze.py
http://api.gceasy.io/my-gc-report.jsp?p=YXJjaGl2ZWQvMjAxNy8wMS8yNi8tLWFwaS0yOTc5MmYwZC01ZDVmLTQzYWQtOTM1OC1lZTBlOGQ4YWI0Mzk5YTkxMzMwNC0yNjVhLTQzZDYtODY1ZS02NjY3NjdhODUwNzkudHh0LS0=&channel=API
http://fastthread.io/ft-thread-report.jsp?dumpId=1#summaryMenu
**** analyze_gc_log
function mdmanalyzegclog() {
  server=${1-prodapp1}
  port=${2-2702}

  echo "Collecting gc - /tmp/dump-$server.log"
  scp -P $port root@$server:/opt/mdm/logs/mdm-gc.log /tmp/gc-$server.log

  #key 29792f0d-5d5f-43ad-9358-ee0e8d8ab439
  curl -s -X POST http://api.gceasy.io/analyzeGC?apiKey=29792f0d-5d5f-43ad-9358-ee0e8d8ab439 --header "Content-Type: application/json" --data-binary @"/tmp/gc-$server.log" > /tmp/gc-$server-analysis.json

  analysisUrl=$(cat /tmp/gc-$server-analysis.json | jq -r '.graphURL')
  hasProblems=$(cat /tmp/gc-$server-analysis.json | jq -r 'has("problem")')

  if [ "$hasProblems" = "true" ]; then
    terminal-notifier -title "TOTVS MDM" -message "Possible problems found in GC!"
    cat /tmp/gc-$server-analysis.json | jq -r '[ .graphURL, .problem ]'
  fi
}
**** threaddump
function mdmanalyzethreaddump() {
  server=${1-prodapp1}
  port=${2-2702}

  dump_collect="ps aux | grep -ie 'mdm/app' | grep -v 'grep' | awk '{print \$2}' | xargs jstack -l"

  echo "Collecting sample $i - /tmp/dump-$server.log"
  ssh -p $port root@$server "$dump_collect" > /tmp/dump-$server.log

  #key 29792f0d-5d5f-43ad-9358-ee0e8d8ab439
  curl -s -X POST http://api.fastthread.io/fastthread-api?apiKey=29792f0d-5d5f-43ad-9358-ee0e8d8ab439 --header "Content-Type: application/json" --data-binary @"/tmp/dump-$server.log" > /tmp/dump-$server-analysis.json

  analysisUrl=$(cat /tmp/dump-$server-analysis.json | jq -r '.graphURL')
  hasProblems=$(cat /tmp/dump-$server-analysis.json | jq -r 'has("problem")')

  if [ "$hasProblems" = "true" ]; then
    terminal-notifier -title "TOTVS MDM" -message "Possible problems found in thread dump!"
    cat /tmp/dump-$server-analysis.json | jq -r '[ .graphURL, .problem ]'
  fi
}
**** TODO bruno's bash script
Bruno Volpato [5:15 PM]
I created this small sh to verify if there's any problem on the current stack, hopefully we'll be able to integrate with our checks

[5:15]
 ```function mdmanalyzethreaddump() {
  server=${1-prodapp1}
  port=${2-2702}

  dump_collect="ps aux | grep -ie 'mdm/app' | grep -v 'grep' | awk '{print \$2}' | xargs jstack -l"

  echo "Collecting sample $i - /tmp/dump-$server.log"
  ssh -p $port root@$server "$dump_collect" > /tmp/dump-$server.log

  #key 29792f0d-5d5f-43ad-9358-ee0e8d8ab439
  curl -s -X POST http://api.fastthread.io/fastthread-api?apiKey=29792f0d-5d5f-43ad-9358-ee0e8d8ab439 --header "Content-Type: application/json" --data-binary @"/tmp/dump-$server.log" > /tmp/dump-$server-analysis.json

  analysisUrl=$(cat /tmp/dump-$server-analysis.json | jq -r '.graphURL')
  hasProblems=$(cat /tmp/dump-$server-analysis.json | jq -r 'has("problem")')

  if [ "$hasProblems" = "true" ]; then
    terminal-notifier -title "TOTVS MDM" -message "Possible problems found in thread dump!"
    cat /tmp/dump-$server-analysis.json | jq -r '[ .graphURL, .problem ]'
  fi
}
```

denny zhang [5:17 PM]
That’s nice. Let me “steal” it and keep polishing it as a co-author.
[5:20]
 ```function mdmanalyzegclog() {
  server=${1-prodapp1}
  port=${2-2702}

  echo "Collecting gc - /tmp/dump-$server.log"
  scp -P $port root@$server:/opt/mdm/logs/mdm-gc.log /tmp/gc-$server.log

  #key 29792f0d-5d5f-43ad-9358-ee0e8d8ab439
  curl -s -X POST http://api.gceasy.io/analyzeGC?apiKey=29792f0d-5d5f-43ad-9358-ee0e8d8ab439 --header "Content-Type: application/json" --data-binary @"/tmp/gc-$server.log" > /tmp/gc-$server-analysis.json

  analysisUrl=$(cat /tmp/gc-$server-analysis.json | jq -r '.graphURL')
  hasProblems=$(cat /tmp/gc-$server-analysis.json | jq -r 'has("problem")')

  if [ "$hasProblems" = "true" ]; then
    terminal-notifier -title "TOTVS MDM" -message "Possible problems found in GC!"
    cat /tmp/gc-$server-analysis.json | jq -r '[ .graphURL, .problem ]'
  fi


}
```

[5:21]
`isProblem: true` will likely indicate that we have memory leaks or long GC pauses
**** TODO file ticket: Bruno trick
#+BEGIN_EXAMPLE
#devops

|
 8View member list (5/8 online)
|
0
View Pinned Items
|

Start a call
 Channel Settings
Show Channel Details

Show Mentions & Reactions
Show Starred Items
More Items
*bold* _italics_ ~strike~ `code` ```preformatted``` >quote
ALL UNREADS

CHANNELS (54)
back-end
competitor-watch
cooltechanddesign
customers
devops
explore_env_alert
fd-ai
fd-alerts
fd-dev
fd-explore
fd-metric
fd-platform
fd-product
fd-production
feriado
front-end
general
hr
hr-remote
internal-jenkins-ci
labs-culture
labs_ca
ligados
mdm-logs
performance
qa
slack_nagios_test
userexperience

DIRECT MESSAGES (60)
slackbot
 denny zhang (you)
 Bruno Volpato
 Chinwei Wong
 katherine
katherine, Diogo, Thiago, Matt, Wilson
 Kung Wang
Kung, Chinwei
 Mitu Singh
 Robson Poffo
 Wilson Souza
Quick Switcher⌘K
And more...

----- November 29th -----

Nagios BOT [2:39 PM]
104.236.179.76/check_memory is WARNING:
MEMORY WARNING : Mem used: 89.22%, Swap used: 0%


Nagios BOT [4:11 PM]
159.203.192.146/check_disk_rootfs is WARNING:
DISK WARNING used :  / 14.52% free


Nagios BOT [4:27 PM]
45.55.1.132/check_memory is WARNING:
MEMORY WARNING : Mem used: 88.89%, Swap used: 0%

[4:31]
159.203.192.146/check_disk_rootfs is OK


Nagios BOT [9:41 PM]
159.203.192.146/check_disk_rootfs is WARNING:
DISK WARNING used :  / 14.85% free


Nagios BOT [9:51 PM]
159.203.192.146/check_disk_rootfs is OK


Nagios BOT [10:22 PM]
45.55.1.132/check_memory is OK


Nagios BOT [10:43 PM]
159.203.198.129/check_memory is OK


----- Yesterday November 30th, 2016 -----

Nagios BOT [12:21 AM]
159.203.198.129/check_memory is WARNING:
MEMORY WARNING : Mem used: 80.19%, Swap used: 0%

[12:24]
45.55.1.132/check_memory is WARNING:
MEMORY WARNING : Mem used: 80.12%, Swap used: 0%


Nagios BOT [12:44 AM]
45.55.1.132/check_memory is OK


Nagios BOT [1:22 AM]
45.55.1.132/check_memory is WARNING:
MEMORY WARNING : Mem used: 80.15%, Swap used: 0%


Nagios BOT [9:05 AM]
107.170.212.76/check_disk_rootfs is WARNING:
DISK WARNING used :  / 14.66% free


Nagios BOT [9:20 AM]
107.170.212.76/check_disk_rootfs is OK


Nagios BOT [3:14 PM]
159.203.192.146/check_disk_rootfs is WARNING:
DISK WARNING used :  / 14.67% free


Nagios BOT [3:34 PM]
159.203.192.146/check_disk_rootfs is OK


Nagios BOT [3:56 PM]
159.203.198.129/check_memory is CRITICAL:
MEMORY CRITICAL : Mem used: 90.09%, Swap used: 0%

[3:57]
159.203.192.146/check_disk_rootfs is WARNING:
DISK WARNING used :  / 14.43% free

[3:59]
104.236.179.76/check_memory is CRITICAL:
MEMORY CRITICAL : Mem used: 90.01%, Swap used: 0%

[4:01]
159.203.198.129/check_memory is WARNING:
MEMORY WARNING : Mem used: 89.30%, Swap used: 0%

[4:02]
159.203.247.196/check_memory is WARNING:
MEMORY WARNING : Mem used: 80.68%, Swap used: 0%

[4:04]
104.236.179.76/check_memory is WARNING:
MEMORY WARNING : Mem used: 89.41%, Swap used: 0%

[4:06]
159.203.198.129/check_memory is CRITICAL:
MEMORY CRITICAL : Mem used: 91.43%, Swap used: 0%

[4:07]
45.55.1.132/check_memory is CRITICAL:
MEMORY CRITICAL : Mem used: 90.15%, Swap used: 0%

[4:09]
104.236.179.76/check_memory is CRITICAL:
MEMORY CRITICAL : Mem used: 91.04%, Swap used: 0%

[4:12]
45.55.1.132/check_memory is WARNING:
MEMORY WARNING : Mem used: 89.64%, Swap used: 0%

[4:17]
45.55.1.132/check_memory is CRITICAL:
MEMORY CRITICAL : Mem used: 92.64%, Swap used: 0%

[4:18]
107.170.212.76/check_disk_rootfs is WARNING:
DISK WARNING used :  / 14.08% free


Nagios BOT [4:39 PM]
104.236.148.126/check_memory is WARNING:
MEMORY WARNING : Mem used: 80.89%, Swap used: 0%


Nagios BOT [4:53 PM]
107.170.212.76/check_disk_rootfs is OK


Nagios BOT [5:06 PM]
107.170.212.76/check_disk_rootfs is WARNING:
DISK WARNING used :  / 14.64% free


Nagios BOT [5:16 PM]
159.203.198.129/check_memory is OK


Nagios BOT [5:32 PM]
45.55.1.132/check_memory is OK


Nagios BOT [7:34 PM]
104.236.179.76/check_memory is OK


----- Today December 1st, 2016 -----

Nagios BOT [1:04 AM]
104.236.148.126/check_memory is OK


Nagios BOT [2:42 AM]
107.170.212.76/check_disk_rootfs is WARNING:
DISK WARNING used :  / 14.73% free


Nagios BOT [5:07 AM]
159.203.247.196/check_memory is CRITICAL:
MEMORY CRITICAL : Mem used: 90.13%, Swap used: 0%

[5:09]
159.203.192.146/check_disk_rootfs is WARNING:
DISK WARNING used :  / 14.51% free

[5:12]
159.203.247.196/check_memory is WARNING:
MEMORY WARNING : Mem used: 86.81%, Swap used: 0%


Nagios BOT [5:42 AM]
159.203.247.196/check_memory is CRITICAL:
MEMORY CRITICAL : Mem used: 90.15%, Swap used: 0%

Kung Wang [5:57 AM]
Denny, when you are up, we need your help to add another ES machine into production cluster


Nagios BOT [7:22 AM]
159.203.247.196/check_memory is WARNING:
MEMORY WARNING : Mem used: 81.75%, Swap used: 0%

denny zhang [7:53 AM]
@kungwang Let me check.


jenkins BOT [8:28 AM]
----------------
DeploySystem - #106 Started by user DennyZhang (Open)


denny zhang [8:28 AM]
Adding prod-es-8(192.241.203.166)


jenkins BOT [8:35 AM]
----------------
DeploySystem - #106 Success after 7 min 30 sec (Open)


denny zhang [8:44 AM]
New es node has been added. Rebalance is on the way.


Nagios BOT [9:59 AM]
159.203.192.146/check_disk_rootfs is OK


Nagios BOT [1:22 PM]
107.170.212.76/check_disk_rootfs is OK

Bruno Volpato [1:26 PM]
what's the ssh port that we are using now?

[1:27]
@kungwang @denny.zhang

denny zhang [1:33 PM]
2702

[1:33]
For prod/explore env, configure sshd to listen on port 2702, instead of 22
https://trello.com/c/oOys2bmN

Bruno Volpato [1:35 PM]
thanks


Nagios BOT [1:48 PM]
159.203.234.164/check_mdm_mem is WARNING:
Memory: WARNING VIRT: 16500 MB - RES: 10793 MB used!

Bruno Volpato [1:54 PM]
@denny.zhang taking a look on the disk for ES, node 2 has a crash heap dump of 18gb in the disk

[1:54]
 ```root@prod-es-2:~# ls -lah /usr/share/elasticsearch
total 18G
drwxr-xr-x   7 elasticsearch elasticsearch 4.0K Sep  8 14:49 .
drwxr-xr-x 129 root          root          4.0K Nov 28 20:51 ..
-rw-r--r--   1 root          root           150 May 17  2016 NOTICE.txt
-rw-r--r--   1 root          root          8.5K May 17  2016 README.textile
drwxr-xr-x   2 root          root          4.0K Jun 21 02:23 bin
-rw-------   1 elasticsearch elasticsearch  18G Sep  8 14:51 java_pid11310.hprof```

[1:55]
heap dumps are good for post-analysis, but maybe we can set a rule to get rid of them

[1:56]
if they are there for more than 2 days, it's very unlikely that we'll analyze it -- to be sincere, we don't even have machine to open a 18g dump

denny zhang [1:57 PM]
Bruno, thanks for the info.

[1:57]
Will improve this.

Just curious how you find this? It was generated Sep 8th.

Bruno Volpato [2:01 PM]
analyzing `du`, trying to figure if we had something other than the data

[2:01]
disks are filling really fast

denny zhang [2:02 PM]
Make senses.

Let me remove java_pid11310.hprof now?

Bruno Volpato [2:06 PM]
yes, sure

[2:06]
prodes6 also had this

denny zhang [2:07 PM]
Removed in es02. Didn't find it in es06. Guess you have finished the cleanup. Right?

Bruno Volpato [2:07 PM]
yes


Nagios BOT [2:30 PM]
159.203.234.164/check_mdm_healthcheck is CRITICAL:
CHECK_NRPE: Socket timeout after 10 seconds.

denny zhang [2:32 PM]
root@prod-app-1:/etc/nagios/nrpe.d# /usr/lib/nagios/plugins/check_http -H localhost -u http://127.0.0.1:8081/admin/healthcheck -p 8081

CRITICAL - Socket timeout after 10 seconds

Bruno Volpato [2:32 PM]
@robson.poffo ran anything special on production?

[2:35]
memory blew up

denny zhang [2:35 PM]
Yes, looks like so

Bruno Volpato [2:35 PM]
22 seconds full gc pauses and can not free any ram.

[2:35]
http://gceasy.io/my-gc-report.jsp?p=c2hhcmVkLzIwMTYvMTIvMS8tLXByb2RhcHAxLWdjLmxvZy0tNi0zMy01Ng==

[2:37]
problem started around 30 min ago

denny zhang [2:40 PM]
uploaded an image: Pasted image at 2016-12-01, 2:39 PM
  Add Comment

denny zhang [2:40 PM]
We get mdm memory warning about 54 minutes ago.

So what we should do now?

Bruno Volpato [2:40 PM]
analyzing (edited)

denny zhang [2:40 PM]
Wait or restart mdm service?

Bruno Volpato [2:40 PM]
wait

denny zhang [2:40 PM]
Sure

[2:41]
Right back after 10 minutes.

Bruno Volpato [2:46 PM]
identified a thread that's causing this

[2:46]
@robson.poffo did you try to drop the staging type receitaws?

Bruno Volpato [2:52 PM]
omg this request was fired 3 days ago (edited)

[2:53]
it spent three days deleting the documents and crashed the server probably when mounting the response (edited)

[2:54]
 ```[28 Nov 2016;05:14:00.324]-[INFO ][ElasticsearchSyncDaoImpl - deleteByQuery:1232 - dw-1222 - DELETE /api/v1/staging/entities/applications/333584e0b3a511e698870401f8d88101/types/receitaws - T_135a3d] - deleteByQuery. aliasName: staging-8cd6e43115e9416eb23609486fa053e3, query: {"bool":{"must":[{"bool":{"should":[{"type":{"value":"333584e0b3a511e698870401f8d88101_receitaws"}},{"term":{"mdmStagingType":"receitaws"}}]}},{"term":{"mdmApplicationId.raw":"333584e0b3a511e698870401f8d88101"}},{"term":{"mdmTenantId.raw":"8cd6e43115e9416eb23609486fa053e3"}}]}}

[01 Dec 2016;05:18:10.925]-[INFO ][ElasticsearchSyncDaoImpl - deleteByQuery:1263 - dw-1222 - DELETE /api/v1/staging/entities/applications/333584e0b3a511e698870401f8d88101/types/receitaws - T_135a3d] - Closing the bulk processor```
(edited)

[2:54]
the same request (T_135a3d) is running for 3 days

denny zhang [2:57 PM]
Interesting. Any way we can detect this automatically?

Bruno Volpato [2:59 PM]
it's hard to tell - LB gives up of the request after a couple seconds

[2:59]
only by log analysis I would say

[2:59]
thread dump analysis also wouldn't help too much, because we reuse the threads

[2:59]
every T_xxxxxx is unique for each REST call

[3:01]
btw, yes - can verify that this point is buggy and this endpoint doesn't scale.

[3:02]
it raises a query to Elasticsearch to delete all documents from a type. then the response is the IDS that were deleted, and we start sending the delete requests to Couchbase

[3:04]
the problem is that we don't split in multiple requests. we run in one batch, for this type it had 32 million docs!

denny zhang [3:07 PM]
Thanks, Bruno!

Bruno Volpato [3:07 PM]
will restart the server

[3:08]
at least we found the root cause.

denny zhang [3:10 PM]
Yeah, I will try to read log more. See whether we can get some patterns to detect suspicious issues earlier

Bruno Volpato [3:12 PM]
in this case, what happened me to identify what was stuck was the thread dumps, @denny.zhang


Nagios BOT [3:13 PM]
159.203.234.164/check_mdm_mem is OK

Bruno Volpato [3:13 PM]
`kill -3` in the application pid, and it prints all the thread dumps to stdout (mdm-initscript.log)

[3:13]
after running 3 times with some interval, I could tell that this thread was stuck in a specific process

[3:14]
but this requires some app structure knowledge too, sadly, but I'm sharing just to you have an idea

denny zhang [3:14 PM]
Cool, thanks for the tip. Let me exercise it locally

Bruno Volpato [3:14 PM]
the trace will give you that:

[3:14]
 ```dw-1222 - DELETE /api/v1/staging/entities/applications/333584e0b3a511e698870401f8d88101/types/receitaws - priority:5 - threadId:0x00007f3ee008d800 - nativeId:0x3cab - state:TIMED_WAITING
stackTrace:
java.lang.Thread.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait for <0x000000056424eb08> (a java.util.concurrent.CountDownLatch$Sync)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1037)
at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)
at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)
at com.couchbase.client.java.util.Blocking.blockForSingle(Blocking.java:72)
at com.couchbase.client.java.CouchbaseBucket.get(CouchbaseBucket.java:138)
at com.couchbase.client.java.CouchbaseBucket.get(CouchbaseBucket.java:133)
at com.totvslabs.framework.couchbase.dao.impl.CouchbaseSyncDaoImpl.cbGet(CouchbaseSyncDaoImpl.java:456)
at com.totvslabs.framework.couchbase.dao.impl.CouchbaseSyncDaoImpl.get(CouchbaseSyncDaoImpl.java:477)
at com.totvslabs.framework.dao.impl.sync.GenericSyncDaoImpl.deleteByQuery(GenericSyncDaoImpl.java:1836)
at com.totvslabs.mdm.app.core.services.impl.StagingServiceImpl.deleteStagingForApplicationEntity(StagingServiceImpl.java:1539)
at com.totvslabs.mdm.app.core.services.impl.StagingServiceImpl.dropEntity(StagingServiceImpl.java:730)
at com.totvslabs.mdm.app.rest.resources.staging.StagingResource.dropStagingType_aroundBody26(StagingResource.java:441)
at com.totvslabs.mdm.app.rest.resources.staging.StagingResource.dropStagingType_aroundBody27$advice(StagingResource.java:78)
at com.totvslabs.mdm.app.rest.resources.staging.StagingResource.dropStagingType(StagingResource.java:1)
```

[3:14]
if you see that run after 2 or 3 times with some interval, and this threadId is still doing the same thing, usually it's not a good thing


Nagios BOT [3:15 PM]
159.203.234.164/check_mdm_healthcheck is OK

denny zhang [3:15 PM]
It make senses. Thanks for the sharing, Bruno! Also the issue support

Bruno Volpato [3:21 PM]
hotfix applied on master. will run some tests before deploying

[3:23]
it was trying to allocate all the deleted documents in memory, this operation was requiring around 30.6gb RAM, bringing all the documents from CB, while it really just needs to bring the IDs. the same operation would use only 500mb footprint now. (edited)

[3:24]
there's still room for improvement, but doesn't need to be handled as a hotfix

Robson Poffo [3:28 PM]
@bruno I was not working with production

Bruno Volpato [3:28 PM]
yes nvm, figured what was happening.

Robson Poffo [3:28 PM]
@bruno @denny.zhang I have some queries that I was running in production, now I am changing the ssh port to run again

Bruno Volpato [3:28 PM]
queries on ssh?

Robson Poffo [3:29 PM]
no, queries on EL

[3:30]
but first closing the ssh to run the queries

#+END_EXAMPLE
**** TODO Bruno java tips
#+BEGIN_EXAMPLE
Bruno Volpato [2:27 PM]
yes.. it really depends on the issue, but if we have frequent stacks, it may help

[2:28]
for this week's issue, a big number of thread was being created over time (we can see that the error started to happen after the process being up for 90 hours), and 4 hours would be more than enough

[2:28]
but if it's some insane peak in a short period of time, it wouldn't help.

[2:29]
if we can detect that memory is blowing, it might be easier

[2:29]
for this we can reduce the -Xms (initial memory), and when it starts to use near to -Xmx, we can do a stack

[2:29]
we could run a heap dump too, but usually it costs a lot of disk, this is why I didn't suggest to run it

[2:29]
but if we know that we are in some kind of trouble, it may be useful

[2:30]
`jcmd {pid} GC.heap_dump /tmp/dump-app.bin`

[2:30]
but the file size can be close to `-Xmx` size, so we need to be aware of this


Nagios BOT [2:59 PM]
prod-es-3/check_elasticsearch_mem is CRITICAL:
Memory: CRITICAL RES: 28677 MB - VIRT: 114210 MB used!

denny zhang [4:25 PM]
@bruno

Good to know that. Thanks for the tips.

Changed the job to hourly. Let’s see what we can explore from what we have captured.

Kung Wang [4:26 PM]
Bruno, is there a way we can get thread count from the system? because I believe this is will determine the best time to get jstack

[4:27]
so far I think this will need help from BE, to provide something like this:

http://stackoverflow.com/questions/9963331/how-to-know-how-many-threads-have-been-created-and-running

expose as API so Denny can consume, if this count goes high, then we get jstack. What do you think Bruno?
stackoverflow.com
How to know how many threads have been created and running?
This is my simple program in Java:public class Counter extends Thread { public static void main(String args[]) { Thread t1 = new Thread(); Thread t2 = new Thread(); ...

[4:28]
for jcmd, it’s another issue, this is when we know it’s definitely a big problem, then we run this, otherwise, 12G on each capture is a lot, it will fill up disk super quick

denny zhang [4:29 PM]
Yeah, currently nagios is monitoring file handler count of mdm application.

If we can also monitor the thread count, it would be even better.
#+END_EXAMPLE
*** 2017-01-27: chef install scripts: http://injenkins.fluigdata.com:48080/job/DockerDeployAllInOne/215/console
   CLOSED: [2017-01-27 Fri 15:28]
*** 2017-01-29: [[https://trello.com/c/e3W8AVdN][#1931]] Provide java debugging facility scripts in python
   CLOSED: [2017-01-29 Sun 13:08]
*** 2017-01-29: [[https://trello.com/c/5O9DFlhR][#1899]] DeploySystem improvement: Only if current mdm is up and running, we continue to deploy other mdm nodes
   CLOSED: [2017-01-29 Sun 13:09]

*** #  --8<-------------------------- separator ------------------------>8--
** 2017-02
*** 2017-02-03: [[https://github.com/TOTVS/mdmdevops/wiki/Feb,-2017:-Migrate-data-from-DigitalOcean-to-Azure][Add wiki]]: Migration procedure: Migrate data from DigitalOcean to Azure
   CLOSED: [2017-02-03 Fri 15:31]
*** 2017-02-01: In prod env: enabled elasticsearch snapshot/restore feature and restart cluster gracefully
   CLOSED: [2017-02-02 Thu 12:00]
*** 2017-02-01: [[https://trello.com/c/DlouHsB7][#1964]] chef: enable data repo for ES snapshot/restore ES feature
   CLOSED: [2017-02-02 Thu 12:02]

*** 2017-01-30: Azure support: resize william mssql Azure VM from 1TB disk to 8TB disk
   CLOSED: [2017-01-30 Mon 15:14]
*** 2017-02-01: Azure support: william mssql restore
   CLOSED: [2017-02-02 Thu 12:05]
*** 2017-01-31: mdm.yml update: include app/FluigIdentity.pk8 to deployment process
   CLOSED: [2017-01-31 Tue 12:25]
*** 2017-02-03: mdm.yml update: Per william's rest about AI section
  CLOSED: [2017-02-03 Fri 13:37]
carolAIConfig:
   productNormalizationHost: "173.255.254.43"
   productNormalizationRPath: "/r/productNormalization"

*** 2017-02-03: Test and report 2 issues about mdm and mdmbackup start failure
  CLOSED: [2017-02-03 Fri 11:55]
#+BEGIN_EXAMPLE
denny zhang [10:08 AM] 
Filed 2 tickets, please take a look:

mdmbackup fail to start in latest 1.58 all-in-one deployment
https://trello.com/c/4cHWUehW

mdm fail to start in latest 1.58 all-in-one deployment
https://trello.com/c/eUABqU3e
#+END_EXAMPLE
*** 2017-02-03: [Support] Jenkins job: create mdm cluster env for bruno: http://injenkins.fluigdata.com:48080/view/All/job/DeployDigitalOceanMDMCluster/1/console
   CLOSED: [2017-02-03 Fri 09:30]
*** 2017-02-03: [[https://github.com/TOTVS/mdmdevops/wiki/One-Button-Deployment:-Setup-MDM-Cluster-Env][Add wiki]] One Button Deployment: Setup MDM Cluster Env
   CLOSED: [2017-02-03 Fri 15:26]
*** 2017-02-04: 3h: Docker build for all individual images
  CLOSED: [2017-02-04 Sat 07:52]
https://bitbucket.org/nubesecure/devops/issues/4/docker-build-for-all-individual-images

*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-02-06: [#A] NFS testing in bematech                      :IMPORTANT:
  CLOSED: [2017-02-06 Mon 15:09]
/data/elasticsearch/repo
*** 2017-02-07: reclaim a data volume for prod env, and setup NFS server for testing
   CLOSED: [2017-02-07 Tue 10:58]
*** 2017-02-07: #1795 (1) Add mdm-bematech-plugin to build and deployment process: http://injenkins.fluigdata.com:48080/view/Basic/job/DockerDeployAllInOne/270/console
  CLOSED: [2017-02-07 Tue 19:37]
https://trello.com/c/ntB12HOu
*** 2017-02-08: prod-es-25: replace 2TB volume with 3TB volume
   CLOSED: [2017-02-08 Wed 16:34]
*** 2017-02-08: elasticsearch recreate snapshot
   CLOSED: [2017-02-08 Wed 16:34]
*** 2017-02-08: 191.232.243.250(bematech-es-10) add 1 TB SSD volume to Azure, and setup NFS server
  CLOSED: [2017-02-07 Tue 13:51]
/usr/share/elasticsearch/repo: 1TB

- Format disk
#+BEGIN_EXAMPLE
mkfs.ext4 -F /dev/sdd

export mnt_point=/usr/share/elasticsearch/repo
sudo mkdir -p $mnt_point
chmod 777 /usr/share/elasticsearch/repo

mount -o discard,defaults /dev/sdd $mnt_point
echo "/dev/sdd $mnt_point ext4 defaults,nofail,discard 0 0" | sudo tee -a /etc/fstab

cat  /etc/fstab
#+END_EXAMPLE

- Setup nfs
apt-get install -y nfs-kernel-server

#+BEGIN_EXAMPLE
cat > /etc/exports <<EOF
/data/mnt/es-extra-volume/elasticsearch/repo *(rw,sync,fsid=0,crossmnt,no_subtree_check,no_root_squash)
/usr/share/elasticsearch/repo *(rw,sync,fsid=1,crossmnt,no_subtree_check,no_root_squash)
EOF
#+END_EXAMPLE

service nfs-kernel-server start
service nfs-kernel-server status

df -h
*** 2017-02-08: mount NFS disk In existing nodes: http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/283/console
  CLOSED: [2017-02-06 Mon 19:34]
apt-get install -y nfs-common

rm -rf /data/elasticsearch/repo
mkdir -p /data/elasticsearch/repo
chown elasticsearch:elasticsearch /data/elasticsearch/repo
mount -t nfs 191.232.243.250:/data/elasticsearch/repo /data/elasticsearch/repo
ls -lth /data/elasticsearch/repo
*** 2017-02-08: umount NFS disk for all es nodes: http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/285/console
  CLOSED: [2017-02-07 Tue 13:50]
umount /data/elasticsearch/repo || true
mount
*** 2017-02-08: create snapshot for big index in DO
   CLOSED: [2017-02-07 Tue 17:05]
*** 2017-02-08: elasticsearch data volume in prod env: 1TB
  CLOSED: [2017-02-06 Mon 15:16]
curl $es_ip:9200/_cat/shards?v | grep " p "
*** 2017-02-09: Start demo vm with first version: 2h
   CLOSED: [2017-02-09 Thu 10:40]
*** 2017-02-10: #14: Use SonarQube for static java code check: 2h
  CLOSED: [2017-02-10 Fri 18:32]
https://bitbucket.org/nubesecure/devops/issues/14/use-sonarqube-for-static-java-code-check
*** 2017-02-10: #15 Enable pipeline to enforce pylint static check for python code: 2h
  CLOSED: [2017-02-10 Fri 22:19]
https://bitbucket.org/nubesecure/devops/issues/15/enable-pipeline-to-enforce-pylint-static
*** 2017-02-05: #6 Enable pipeline to enforce rubocop static check for chef code: 1h
  CLOSED: [2017-02-05 Sun 22:58]
https://bitbucket.org/nubesecure/chef/issues/6/enable-pipeline-to-enforce-rubocop-static
docker pull ruby
docker run -t -d --privileged -h mytest --name my-test ruby /bin/bash
docker exec -it my-test bash

which rubocop 

gem install rubocop

https://bitbucket.org/nubesecure/chef/addon/pipelines/home#!/results/%7Bb38b727a-8993-40d5-a534-e1638f8994e6%7D
*** 2017-02-08: #9 Setup slack notification for #soteria-prod-alerts, #soteria-dev: 1h
  CLOSED: [2017-02-08 Wed 23:49]
https://bitbucket.org/nubesecure/devops/issues/9/setup-slack-notification-for-soteria-prod
*** 2017-02-06: add wiki: Critical Server List: 1h
  CLOSED: [2017-02-06 Mon 17:26]
https://bitbucket.org/nubesecure/dofacdenny/wiki/Critical%20Server%20List
*** 2017-02-11: Add wiki: System Deployment Process: 1h
  CLOSED: [2017-02-11 Sat 15:02]
https://bitbucket.org/nubesecure/devops/wiki/System%20Deployment%20Process
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-02-09: [#A] Update wiki: migration procedure
   CLOSED: [2017-02-09 Thu 10:39]
*** 2017-02-09: reconfigure the CB cluster: update jenkins for couchbase: http://bematechjenkins.fluigdata.com:18080/job/DeploySystem/12/console
  CLOSED: [2017-02-09 Thu 11:53]
Cluster RAM: DATA RAM Quota(25600)
mdm-master: 5120
mdm-session: 1024
mdm-staging: 18432

mt could not find index server

*** 2017-02-09: Bug fixing for DeployDigitalOceanMDMCluster: ES data node count
   CLOSED: [2017-02-09 Thu 18:03]
*** 2017-02-09: deprovision explore env: http://explorejenkins.fluigdata.com:18080/job/RunCommandOnServers/78/console
   CLOSED: [2017-02-09 Thu 18:58]
*** 2017-02-09: [#A] verify rsync copy speed up
  CLOSED: [2017-02-09 Thu 19:32]
/mnt/cb-backup-01/couchbase-cli/cbbackup http://138.197.201.198:8091 /mnt/cb-backup-01/backup/mdm-session -u Administrator -p password1234 -b mdm-session -m diff -t 4 --single-node >> /var/log/cb_backup.log

rsync -avz -e "ssh -o StrictHostKeyChecking=no -i /tmp/copy_id_rsa -p 2702 -o UserKnownHostsFile=/dev/null" --progress  root@138.68.4.184:/mnt/cb-backup-01/backup /opt/couchbase/backup2
*** 2017-02-09: Branch merge(1.58 to master) and Routine CI tests
   CLOSED: [2017-02-09 Thu 21:04]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-02-10: [#A] cbbackup bug: doesn't get full backup for cluster env, when node count is larger than replica count
   CLOSED: [2017-02-10 Fri 00:08]
**** DONE couchbase backup and restore test
     CLOSED: [2017-02-10 Fri 00:10]
***** couchbase bucket for mdm-session bucket
# with single-node
rm -rf /tmp/backup/couchbase/mdm-session
time /opt/couchbase/bin/cbbackup http://138.197.194.193:8091 /tmp/backup/couchbase/mdm-session -u Administrator -p password1234 -b mdm-session -m full -t 4 --single-node

# without single-node
rm -rf /tmp/backup/couchbase/mdm-session
time /opt/couchbase/bin/cbbackup http://138.197.194.193:8091 /tmp/backup/couchbase/mdm-session -u Administrator -p password1234 -b mdm-session -m full -t 4

#+BEGIN_EXAMPLE
root@prod-cb-backup:/tmp/backup/couchbase# /opt/couchbase/bin/cbbackup http://138.197.194.193:8091 /tmp/backup/couchbase/mdm-session -u Administrator -p password1234 -b mdm-session -m full -t 4
  [                    ] 1.8% (275000/estimated 15115304 msgs)
#+END_EXAMPLE
**** DONE full backup for mdm-session
  CLOSED: [2017-02-10 Fri 00:10]
mkdir -p /tmp/backup/couchbase/mdm-session

/opt/couchbase/bin/cbbackup http://138.197.194.193:8091 /tmp/backup/couchbase/mdm-session -u Administrator -p password1234 -b mdm-session -m full -t 4 --single-node >> /tmp/cb_backup.log

- From GUI: 9683829

- After restore: 1380366

- 2497606

- 2153107
#+BEGIN_EXAMPLE
root@prod-cb-backup:/mnt/cb-backup-01/backup/mdm-session/2017-02-10T001758Z# /opt/couchbase/bin/cbbackup http://138.197.194.193:8091 /tmp/backup/couchbase/mdm-session -u Administrator -p password1234 -b mdm-session -m full -t 4 --single-node >> /tmp/cb_backup.log
  [##                  ] 11.6% (249000/estimated 2153107 msgs)
#+END_EXAMPLE
**** DONE start 2 vm to try cbrestore
  CLOSED: [2017-02-10 Fri 00:10]
172.17.0.3

mdm-master: 996
***** 3 nodes cluster
# source couchbase: all-in-one
ssh -N -p 22 -i /tmp/test_id -f root@172.17.0.5 -L *:8091:localhost:8091 -n /bin/bash
http://45.33.87.74:8091
***** couchbase-mdm
# dst couchbase: couchbase-mdm
ssh -N -p 22 -i /tmp/test_id -f root@172.17.0.4 -L *:8092:localhost:8091 -n /bin/bash
http://45.33.87.74:8092
***** perform backup
rm -rf   /data/backup/couchbase
mkdir -p  /data/backup/couchbase

/opt/couchbase/bin/cbbackup http://127.0.0.1:8091 /data/backup/couchbase/mdm-master -u Administrator -p password1234 -b mdm-master -m full -t 4

ls -lth /data/backup/couchbase/mdm-master/*/*/*
***** copy backup set
# scp
rm -rf /tmp/couchbase
scp -r -i /tmp/test_id -P 22 root@172.17.0.5:/data/backup/couchbase /tmp/

export CB_PASSWD=password1234
cd /tmp/couchbase/mdm-master
time /opt/couchbase/bin/cbrestore . -u Administrator -p $CB_PASSWD \
     -x rehash=1 couchbase://localhost:8091 -b mdm-master -B mdm-master

rm -rf /data/backup/couchbase/*
***** command
/opt/couchbase/bin/cbbackup http://127.0.0.1:8091 /data/backup/couchbase/mdm-master -u Administrator -p password1234 -b mdm-master -m full -t 4 --single-node >> /var/log/cb_backup.log

#+BEGIN_EXAMPLE
2017-02-10 04:40:26,211 cb_backup.py:87 - INFO: Backup Couchbase bucket: mdm-master, method: diff
2017-02-10 04:40:26,212 cb_backup.py:88 - INFO: Run command: /opt/couchbase/bin/cbbackup http://127.0.0.1:8091 /data/backup/couchbase/mdm-master -u Administrator -p password1234 -b mdm-master -m diff -t 4 --single-node >> /var/log/cb_backup.log
  [                    ] 0.1% (1/estimated 996 msgs)
bucket: mdm-master, msgs transferred...
       :                total |       last |    per sec
 byte  :                  352 |        352 |      268.0
2017-02-10 04:40:27,635: mt could not find index server:0
done
2017-02-10 04:40:27,638 cb_backup.py:102 - INFO: Backup succeed for Couchbase.
#+END_EXAMPLE
**** DONE try to restore mdm-session
  CLOSED: [2017-02-10 Fri 00:10]
scp -r -P 2702 -o StrictHostKeyChecking=no -i /tmp/copy_id_rsa  -o UserKnownHostsFile=/dev/null \
root@138.68.4.184:/tmp/backup/couchbase /tmp/backup/
**** DONE cbrestore mdm-session
  CLOSED: [2017-02-10 Fri 00:10]
9684268
9683896

- In prod nagios GUI: 9681755
- Azure restore:      1380366
- cbrestore
#+BEGIN_EXAMPLE
restore . -u Administrator -p $CB_PASSWD \
>      -x rehash=1 couchbase://localhost:8091 -b mdm-session -B mdm-session

  [####                ] 17.8% (454000/estimated 2548572 msgs))
#+END_EXAMPLE

*** 2017-02-10: [#A] low disk of cb-03
  CLOSED: [2017-02-10 Fri 06:39]
prod-cb-03/check_disk_rootfs is CRITICAL:
DISK CRITICAL used :  / 9.80% free 

denny zhang [5:46 AM] 
Checking low disk of cb-03.

Now prod-cb-03 has 58% free disk. It’s fine now.

Confirmed the low disk is caused by multiple cbbackup process.

Previously we have canceled some couchbase backup. It turns out those process haven’t bee killed. Still running for days.

Certainly not good by design. I’ve manually killed them. Now it’s fine. Will deal with later.
```root@prod-cb-backup:/mnt/cb-backup-01# ps -ef | grep cb_backup
root     16649 16648  0 Feb09 ?        00:00:00 /usr/bin/python /opt/devops/bin/cb_backup.py --bucket_list=mdm-master,mdm-staging,mdm-session --cbserver=http://138.197.201.198:8091 --cbbackup_bin=/mnt/cb-backup-01/couchbase-cli/cbbackup --backup_dir=/mnt/cb-backup-01/backup --username Administrator --password password1234
root     16650 16649  0 Feb09 ?        00:00:00 /bin/sh -c /mnt/cb-backup-01/couchbase-cli/cbbackup http://138.197.201.198:8091 /mnt/cb-backup-01/backup/mdm-master -u Administrator -p password1234 -b mdm-master -m full -t 4 --single-node >> /var/log/cb_backup.log
root     16785 16784  0  2016 ?        00:00:00 /usr/bin/python /opt/devops/bin/cb_backup.py --username Administrator --password password1234 --bucket_list=mdm-master,mdm-staging --cbserver=http://104.236.179.76:8091 --cbbackup_bin=/mnt/cb-backup-01/couchbase-cli/cbbackup --backup_dir=/mnt/cb-backup-01/backup
root     16811 16810  0  2016 ?        00:00:00 /usr/bin/python /opt/devops/bin/cb_backup.py --bucket_list=mdm-master,mdm-staging --cbserver=http://104.236.179.76:8091 --cbbackup_bin=/mnt/cb-backup-01/couchbase-cli/cbbackup --backup_dir=/mnt/cb-backup-01/backup --username Administrator --password password1234
root     16812 16811  0  2016 ?        00:00:00 /bin/sh -c /mnt/cb-backup-01/couchbase-cli/cbbackup http://104.236.179.76:8091 /mnt/cb-backup-01/backup/mdm-master -u Administrator -p password1234 -b mdm-master -m full -t 4 --single-node >> /var/log/cb_backup.log
root     16831 16785  0  2016 ?        00:00:00 /bin/sh -c /mnt/cb-backup-01/couchbase-cli/cbbackup http://104.236.179.76:8091 /mnt/cb-backup-01/backup/mdm-staging -u Administrator -p password1234 -b mdm-staging -m full -t 4 --single-node >> /var/log/cb_backup.log
root     16900 16899  0  2016 ?        00:00:00 /usr/bin/python /opt/devops/bin/cb_backup.py --bucket_list=mdm-master,mdm-staging --cbserver=http://104.236.179.76:8091 --cbbackup_bin=/mnt/cb-backup-01/couchbase-cli/cbbackup --backup_dir=/mnt/cb-backup-01/backup --username Administrator --password password1234
root     16901 16900  0  2016 ?        00:00:00 /bin/sh -c /mnt/cb-backup-01/couchbase-cli/cbbackup http://104.236.179.76:8091 /mnt/cb-backup-01/backup/mdm-master -u Administrator -p password1234 -b mdm-master -m diff -t 4 --single-node >> /var/log/cb_backup.log
root     19527 19526  0 Feb09 ?        00:00:00 /usr/bin/python /opt/devops/bin/cb_backup.py --bucket_list=mdm-session,mdm-master,mdm-staging --cbserver=http://138.197.201.198:8091 --cbbackup_bin=/mnt/cb-backup-01/couchbase-cli/cbbackup --backup_dir=/mnt/cb-backup-01/backup --username Administrator --password password1234
root     21091 19527  0 Feb09 ?        00:00:00 /bin/sh -c /mnt/cb-backup-01/couchbase-cli/cbbackup http://138.197.201.198:8091 /mnt/cb-backup-01/backup/mdm-staging -u Administrator -p password1234 -b mdm-staging -m full -t 4 --single-node >> /var/log/cb_backup.log
root     26408 15668  0 07:15 pts/3    00:00:00 grep --color=auto cb_backup
root@prod-cb-backup:/mnt/cb-backup-01# kill 16649```
**** misc
root@prod-cb-03:/# df -h
Filesystem      Size  Used Avail Use% Mounted on
udev             16G  4.0K   16G   1% /dev
tmpfs           3.2G  400K  3.2G   1% /run
/dev/vda1       315G  282G   21G  94% /
none            4.0K     0  4.0K   0% /sys/fs/cgroup
none            5.0M     0  5.0M   0% /run/lock
none             16G     0   16G   0% /run/shm
none            100M     0  100M   0% /run/user
root@prod-cb-03:/# du -h -d 1 /opt/couchbase
24K     /opt/couchbase/share
166G    /opt/couchbase/var
112K    /opt/couchbase/etc
20K     /opt/couchbase/man
84M     /opt/couchbase/lib
19M     /opt/couchbase/samples
88K     /opt/couchbase/doc
107M    /opt/couchbase/bin
167G    /opt/couchbase

#+BEGIN_EXAMPLE
root@prod-cb-03:/opt/couchbase/var/lib/couchbase/data# du -h -d  1 /
48M     /boot
16K     /data
168G    /opt
1.2G    /usr
16K     /tmp
du: cannot access '/proc/4078/task/4097/fd/80': No such file or directory
du: cannot access '/proc/4078/task/4098/fd/738': No such file or directory
du: cannot access '/proc/4078/task/4099/fdinfo/80': No such file or directory
du: cannot access '/proc/4078/task/4102/fdinfo/80': No such file or directory
du: cannot access '/proc/4078/task/4103/fd/741': No such file or directory
du: cannot access '/proc/4078/task/4104/fdinfo/738': No such file or directory
du: cannot access '/proc/4078/task/14662/fd/714': No such file or directory
du: cannot access '/proc/4078/task/14663/fdinfo/714': No such file or directory
du: cannot access '/proc/4078/task/14666/fdinfo/80': No such file or directory
du: cannot access '/proc/4078/task/14666/fdinfo/714': No such file or directory
du: cannot access '/proc/4078/task/14666/fdinfo/738': No such file or directory
du: cannot access '/proc/4683/task/4683/fd/4': No such file or directory
du: cannot access '/proc/4683/task/4683/fdinfo/4': No such file or directory
du: cannot access '/proc/4683/fd/3': No such file or directory
du: cannot access '/proc/4683/fdinfo/3': No such file or directory
0       /proc
4.0K    /srv
16K     /lost+found
763M    /var
9.4M    /sbin
4.0K    /dev
8.0K    /media
4.0K    /lib64
4.0K    /home
6.8M    /etc
24M     /root
0       /sys
354M    /lib
4.0K    /mnt
400K    /run
4.0K    /coredump
9.6M    /bin
170G    /
#+END_EXAMPLE

#+BEGIN_EXAMPLE
root@prod-cb-backup:/mnt/cb-backup-01# ps -ef | grep cb_backup
root     16649 16648  0 Feb09 ?        00:00:00 /usr/bin/python /opt/devops/bin/cb_backup.py --bucket_list=mdm-master,mdm-staging,mdm-session --cbserver=http://138.197.201.198:8091 --cbbackup_bin=/mnt/cb-backup-01/couchbase-cli/cbbackup --backup_dir=/mnt/cb-backup-01/backup --username Administrator --password password1234
root     16650 16649  0 Feb09 ?        00:00:00 /bin/sh -c /mnt/cb-backup-01/couchbase-cli/cbbackup http://138.197.201.198:8091 /mnt/cb-backup-01/backup/mdm-master -u Administrator -p password1234 -b mdm-master -m full -t 4 --single-node >> /var/log/cb_backup.log
root     16785 16784  0  2016 ?        00:00:00 /usr/bin/python /opt/devops/bin/cb_backup.py --username Administrator --password password1234 --bucket_list=mdm-master,mdm-staging --cbserver=http://104.236.179.76:8091 --cbbackup_bin=/mnt/cb-backup-01/couchbase-cli/cbbackup --backup_dir=/mnt/cb-backup-01/backup
root     16811 16810  0  2016 ?        00:00:00 /usr/bin/python /opt/devops/bin/cb_backup.py --bucket_list=mdm-master,mdm-staging --cbserver=http://104.236.179.76:8091 --cbbackup_bin=/mnt/cb-backup-01/couchbase-cli/cbbackup --backup_dir=/mnt/cb-backup-01/backup --username Administrator --password password1234
root     16812 16811  0  2016 ?        00:00:00 /bin/sh -c /mnt/cb-backup-01/couchbase-cli/cbbackup http://104.236.179.76:8091 /mnt/cb-backup-01/backup/mdm-master -u Administrator -p password1234 -b mdm-master -m full -t 4 --single-node >> /var/log/cb_backup.log
root     16831 16785  0  2016 ?        00:00:00 /bin/sh -c /mnt/cb-backup-01/couchbase-cli/cbbackup http://104.236.179.76:8091 /mnt/cb-backup-01/backup/mdm-staging -u Administrator -p password1234 -b mdm-staging -m full -t 4 --single-node >> /var/log/cb_backup.log
root     16900 16899  0  2016 ?        00:00:00 /usr/bin/python /opt/devops/bin/cb_backup.py --bucket_list=mdm-master,mdm-staging --cbserver=http://104.236.179.76:8091 --cbbackup_bin=/mnt/cb-backup-01/couchbase-cli/cbbackup --backup_dir=/mnt/cb-backup-01/backup --username Administrator --password password1234
root     16901 16900  0  2016 ?        00:00:00 /bin/sh -c /mnt/cb-backup-01/couchbase-cli/cbbackup http://104.236.179.76:8091 /mnt/cb-backup-01/backup/mdm-master -u Administrator -p password1234 -b mdm-master -m diff -t 4 --single-node >> /var/log/cb_backup.log
root     19527 19526  0 Feb09 ?        00:00:00 /usr/bin/python /opt/devops/bin/cb_backup.py --bucket_list=mdm-session,mdm-master,mdm-staging --cbserver=http://138.197.201.198:8091 --cbbackup_bin=/mnt/cb-backup-01/couchbase-cli/cbbackup --backup_dir=/mnt/cb-backup-01/backup --username Administrator --password password1234
root     21091 19527  0 Feb09 ?        00:00:00 /bin/sh -c /mnt/cb-backup-01/couchbase-cli/cbbackup http://138.197.201.198:8091 /mnt/cb-backup-01/backup/mdm-staging -u Administrator -p password1234 -b mdm-staging -m full -t 4 --single-node >> /var/log/cb_backup.log
root     26408 15668  0 07:15 pts/3    00:00:00 grep --color=auto cb_backup
root@prod-cb-backup:/mnt/cb-backup-01# kill 16649
#+END_EXAMPLE
*** 2017-02-10: couchbase backup process takes much longer: 20 hours and 2 TB disk
   CLOSED: [2017-02-10 Fri 06:40]
*** 2017-02-10: restore mdm-session, try CB diff again: http://prodjenkins.fluigdata.com:18080/job/MyBackupSystem/8/console
   CLOSED: [2017-02-10 Fri 11:51]
*** 2017-02-11: incremental backup: http://prodjenkins.fluigdata.com:18080/job/MyBackupSystem/9/console
   CLOSED: [2017-02-11 Sat 11:42]

*** 2017-02-11: Run migration simulation for Bematech env. And resolve issues of DevOps side
   CLOSED: [2017-02-11 Sat 16:34]
*** 2017-02-12: [#A] Another incremental backup
  CLOSED: [2017-02-12 Sun 10:49]
#+BEGIN_EXAMPLE
denny zhang [4:56 PM] 
Just had a skype call with Kung.

I’m doing this today. It would take 7 hours.
1. Stop all mdm instances in Azure
2. [1 hours] Do a incremental backup(not full backup) for both ES and CB. It should finish within 1 hour. *And we shall have a more clean env with this point-in-time backup.*
3. [1 hours] Copy the diff
3. [4 hours] Restore ES and CB from this.
4. [0.5 hours] Bring up mdm in Azure
5. Ask people(mainly BE) help to verify the status
#+END_EXAMPLE
**** 2017-02-12: stop all mdm in azure: http://bematechjenkins.fluigdata.com:18080/job/RunCommandOnServers/42/console
    CLOSED: [2017-02-11 Sat 17:00]
**** 2017-02-12: backup cb: http://prodjenkins.fluigdata.com:18080/job/MyBackupSystem/10/console
    CLOSED: [2017-02-11 Sat 17:29]
*** #  --8<-------------------------- separator ------------------------>8--
**** HALF restore small ES index
***** 2017-02-12: [9:30 pm] check cbrestore status
     CLOSED: [2017-02-11 Sat 22:29]
**** HALF consolidate mdm big index
**** #  --8<-------------------------- separator ------------------------>8--
**** TODO restore big ES index
**** TODO set replica to 1
**** TODO update haproxy for DNS
totvs.fluigdata.com, bematech.fluigdata.com and bematechn.fluigdata.com

*** 2017-02-14: (2) Deployment support what plugins to enable, instead of installing all: http://injenkins.fluigdata.com:48080/view/Basic/job/DockerDeployBasicCookbooks/186/console
  CLOSED: [2017-02-14 Tue 13:59]
https://trello.com/c/7yg6edGk
*** 2017-02-15: Add bematech-cb-8 to Azure cluster
   CLOSED: [2017-02-15 Wed 09:15]
*** 2017-02-15: Add wiki: ChatOps via Slack integration
  CLOSED: [2017-02-15 Wed 12:33]
https://github.com/TOTVS/mdmdevops/wiki/ChatOps-via-Slack-integration
*** 2017-02-15: [#A] Cleanup after migration                      :IMPORTANT:
   CLOSED: [2017-03-09 Thu 20:59]
**** 2017-03-09: Handle bematech-cb-4 outage: add back, when cb rebalancing is done
   CLOSED: [2017-02-15 Wed 10:28]
#+BEGIN_EXAMPLE
Denny Zhang [9:21 AM]
Checking about bematech-cb-4


Nagios APP [9:22 AM]
bematech-cb-4/check_couchbase_replica is CRITICAL:
Error: Fail to get replica count for mdm-master bucket

Denny Zhang [9:24 AM]
```root@bematech-cb-4:/etc/nagios/nrpe.d# service couchbase-server status
 * couchbase-server is running
root@bematech-cb-4:/etc/nagios/nrpe.d# telnet localhost 8091
Trying 127.0.0.1...
telnet: Unable to connect to remote host: Connection refused
```

Denny Zhang [9:27 AM]
added and commented on this Plain Text snippet: cb_error.log
[{'ns_1@10.1.1.9',{'EXIT',<0.15902.794>,killed}}]
[ns_server:error,2017-02-14T05:47:17.078Z,ns_1@10.1.1.10:<0.15961.794>:janitor_agent:query_states:186]Failed to query vbucket states from some nodes:
[{'ns_1@10.1.1.9',{'EXIT',<0.15967.794>,killed}}]
[ns_server:error,2017-02-14T06:06:07.065Z,ns_1@10.1.1.10:<0.1130.795>:janitor_agent:query_states:186]Failed to query vbucket states from some nodes:
[{'ns_1@10.1.1.7',{'EXIT',<0.1250.795>,killed}}]
[ns_server:error,2017-02-14T06:06:12.070Z,ns_1@10.1.1.10:<0.1232.795>:janitor_agent:query_states:186]Failed to query vbucket states from some nodes:
[{'ns_1@10.1.1.7',{'EXIT',<0.1334.795>,killed}}]
[ns_server:error,2017-02-14T06:06:17.074Z,ns_1@10.1.1.10:<0.1362.795>:janitor_agent:query_states:186]Failed to query vbucket states from some nodes:
[{'ns_1@10.1.1.7',{'EXIT',<0.1356.795>,killed}}]
[ns_server:error,2017-02-15T07:06:20.313Z,ns_1@10.1.1.10:stats_archiver-@system-processes<0.27264.793>:stats_archiver:backup_logger:260]Failed to backup stats table 'stats_archiver-@system-processes-hour' with error enoent
[ns_server:error,2017-02-15T07:07:35.702Z,ns_1@10.1.1.10:<0.3440.839>:stats_archiver:backup_logger:260]Failed to backup stats table 'stats_archiver-@query-minute' with error enoent
[ns_server:error,2017-02-15T07:07:35.724Z,ns_1@10.1.1.10:stats_archiver-@query<0.2979.839>:stats_archiver:backup_logger:260]Failed to backup stats table 'stats_archiver-@query-hour' with error enoent
[ns_server:error,2017-02-15T07:07:35.741Z,ns_1@10.1.1.10:stats_archiver-@query<0.2979.839>:stats_archiver:backup_logger:260]Failed to backup stats table 'stats_archiver-@query-day' with error enoent
[ns_server:error,2017-02-15T07:07:35.756Z,ns_1@10.1.1.10:stats_archiver-@query<0.2979.839>:stats_archiver:backup_logger:260]Failed to backup stats table 'stats_archiver-@query-week' with error enoent
[ns_server:error,2017-02-15T07:07:35.768Z,ns_1@10.1.1.10:stats_archiver-@query<0.2979.839>:stats_archiver:backup_logger:260]Failed to backup stats table 'stats_archiver-@query-month' with error enoent
[ns_server:error,2017-02-15T07:07:35.780Z,ns_1@10.1.1.10:stats_archiver-@query<0.2979.839>:stats_archiver:backup_logger:260]Failed to backup stats table 'stats_archiver-@query-year' with error enoent
1 Comment Click to expand inline 22 lines
Checked /opt/couchbase/var/lib/couchbase/logs/error.log

Doesn’t look familiar to me.

Let me restart this cb service in this node, after data rebalancing is done.
#+END_EXAMPLE
**** 2017-03-09: shutdown NFS service in DO
    CLOSED: [2017-02-15 Wed 10:49]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-03-09: Bematech ES cluster run into low disk
   CLOSED: [2017-02-15 Wed 12:16]
#+BEGIN_EXAMPLE
Nagios APP [10:26 AM]
bematech-es-4/check_disk_rootfs is CRITICAL:
DISK CRITICAL used :  / 2.03% free

Denny Zhang [10:38 AM]
added and commented on this Plain Text snippet: es_disk_usage.txt
2017-02-15 16:33:05 =============== Run Command on bematech-es-1(10.1.1.12:2702)
2017-02-15 16:33:05 + df -h
2017-02-15 16:33:05 + grep /dev/
2017-02-15 16:33:05 /dev/sda1                                                29G  2.1G   26G   8% /
2017-02-15 16:33:05 /dev/sdc                                                504G  442G   37G  93% /data/mnt/es-extra-volume
2017-02-15 16:33:05 /dev/sdb1                                                55G   52M   53G   1% /mnt
2017-02-15 16:33:08
2017-02-15 16:33:08 =============== Run Command on bematech-es-2(10.1.1.13:2702)
2017-02-15 16:33:08 + df -h
2017-02-15 16:33:08 + grep /dev/
2017-02-15 16:33:08 /dev/sda1                                                29G  2.1G   26G   8% /
2017-02-15 16:33:08 /dev/sdc                                                504G  305G  174G  64% /data/mnt/es-extra-volume
2017-02-15 16:33:08 /dev/sdb1                                                55G   52M   53G   1% /mnt
2017-02-15 16:33:11
2017-02-15 16:33:11 =============== Run Command on bematech-es-3(10.1.1.14:2702)
2017-02-15 16:33:12 + df -h
2017-02-15 16:33:12 + grep /dev/
2017-02-15 16:33:12 /dev/sda1                                                29G  2.1G   26G   8% /
2017-02-15 16:33:12 /dev/sdc                                                504G  318G  161G  67% /data/mnt/es-extra-volume
2017-02-15 16:33:12 /dev/sdb1                                                55G   52M   53G   1% /mnt
2017-02-15 16:33:16
2017-02-15 16:33:16 =============== Run Command on bematech-es-4(10.1.1.15:2702)
2017-02-15 16:33:16 + df -h
2017-02-15 16:33:16 + grep /dev/
2017-02-15 16:33:16 /dev/sda1                                                29G  2.1G   26G   8% /
2017-02-15 16:33:16 /dev/sdc                                                504G  244G  236G  51% /data/mnt/es-extra-volume
2017-02-15 16:33:16 /dev/sdb1                                                55G   52M   53G   1% /mnt
2017-02-15 16:33:19
2017-02-15 16:33:19 =============== Run Command on bematech-es-5(10.1.1.16:2702)
2017-02-15 16:33:20 + df -h
2017-02-15 16:33:20 + grep /dev/
2017-02-15 16:33:20 /dev/sda1                                                29G  2.1G   26G   8% /
2017-02-15 16:33:20 /dev/sdc                                                504G  442G   37G  93% /data/mnt/es-extra-volume
2017-02-15 16:33:20 /dev/sdb1                                                55G   52M   53G   1% /mnt
2017-02-15 16:33:22
2017-02-15 16:33:22 =============== Run Command on bematech-es-6(10.1.1.17:2702)
2017-02-15 16:33:22 + df -h
2017-02-15 16:33:22 + grep /dev/
2017-02-15 16:33:22 /dev/sda1                                                29G  2.1G   26G   8% /
2017-02-15 16:33:22 /dev/sdc                                                504G  421G   58G  88% /data/mnt/es-extra-volume
2017-02-15 16:33:22 /dev/sdb1                                                55G   52M   53G   1% /mnt
2017-02-15 16:33:24
2017-02-15 16:33:24 =============== Run Command on bematech-es-7(10.1.1.18:2702)
2017-02-15 16:33:25 + df -h
2017-02-15 16:33:25 + grep /dev/
2017-02-15 16:33:25 /dev/sda1                                                29G  2.1G   26G   8% /
2017-02-15 16:33:25 /dev/sdc                                                504G  435G   45G  91% /data/mnt/es-extra-volume
2017-02-15 16:33:25 /dev/sdb1                                                55G   52M   53G   1% /mnt
2017-02-15 16:33:27
2017-02-15 16:33:27 =============== Run Command on bematech-es-8(10.1.1.19:2702)
2017-02-15 16:33:27 + df -h
2017-02-15 16:33:27 + grep /dev/
2017-02-15 16:33:27 /dev/sda1                                                29G  2.1G   26G   8% /
2017-02-15 16:33:27 /dev/sdc                                                504G  421G   58G  88% /data/mnt/es-extra-volume
2017-02-15 16:33:27 /dev/sdb1                                                55G   52M   53G   1% /mnt
2017-02-15 16:33:30
2017-02-15 16:33:30 =============== Run Command on bematech-es-9(10.1.1.27:2702)
2017-02-15 16:33:30 + df -h
2017-02-15 16:33:30 + grep /dev/
2017-02-15 16:33:30 /dev/sda1                                                29G  2.1G   26G   8% /
2017-02-15 16:33:30 /dev/sdb1                                                55G   52M   53G   1% /mnt
2017-02-15 16:33:30 /dev/sdc                                                504G  423G   56G  89% /data/mnt/es-extra-volume
1 Comment Click to expand inline 62 lines
ES replication is ongoing. I’m thinking whether Azure ES cluster would run into low disk capacity.

Here is what ES disk usage.

Denny Zhang [10:40 AM]
We have 2 options:

1. Add 1TB disk to ES nodes by default, instead of 500GB
2. Add more ES VM(s)

Certainly we should also need to remove tenants other than Bematech(n) and TOTVS.
#+END_EXAMPLE
**** 2017-03-09: shutdown NFS service in Azure
   CLOSED: [2017-02-15 Wed 12:06]
# in all ES nodes
umount /data/mnt/es-extra-volume/elasticsearch/repo
umount /usr/share/elasticsearch/repo

# in es server
mv /etc/exports /root/exports
service nfs-kernel-server stop
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-03-09: In Bematech-es-10, use 1TB disk
   CLOSED: [2017-02-15 Wed 12:06]
path.data: /usr/share/elasticsearch,/data/elasticsearch

ls -lth /usr/share/elasticsearch
ls -lth /data/elasticsearch

# mount the new volume
umount /usr/share/elasticsearch/repo
mnt_point="/data/elasticsearch2"
mkdir -p "$mnt_point"
chmod 777 "$mnt_point"
mount -o discard,defaults /dev/sdd "$mnt_point"

vim /etc/fstab

# disable shard re-allocation
curl -XPUT "http://$es_ip:9200/_cluster/settings" -d '
{
 "persistent": {
   "cluster.routing.allocation.enable": "none"
 }
}
'

service elasticsearch stop

service elasticsearch start
sleep 5
service elasticsearch status

# re-enable shard re-allocation
curl -XPUT "http://$es_ip:9200/_cluster/settings" -d '
{
 "persistent": {
   "cluster.routing.allocation.enable": "all"
 }
}
'

# check shards
watch "curl $es_ip:9200/_cat/shards?v  | grep -v STARTED"
**** 2017-03-09: In Bematech-es-10, remove snapshot
   CLOSED: [2017-02-15 Wed 12:06]
curl -XDELETE "$es_ip:9200/_snapshot/mdm_backup_small"
curl -XDELETE "$es_ip:9200/_snapshot/mdm_backup_big"

cd /usr/share/elasticsearch/repo
rm -rf /usr/share/elasticsearch/repo/mdm_backup_big

rm -rf /data/mnt/es-extra-volume/elasticsearch/repo/mdm_backup_small
**** 2017-03-09: bematech: enable ES replica to 2 for all es indices
    CLOSED: [2017-02-15 Wed 17:39]
**** 2017-03-09: verify ES replication status in Bematech
    CLOSED: [2017-02-15 Wed 17:39]
**** #  --8<-------------------------- separator ------------------------>8--
**** TODO remove useless tenants from DO, and scale down
Denny Zhang [10:11 AM]
@kungwang & @bruno

Since we have migrated huge tenants to Azure, do you think we should remove them out of DO?

Then I can scale down our DO env, which should *dramatically lower the bill*.

Let me know when would be a good timing for this scale down.
Also how to remove tenants.(Some doc would be nice. If you guys can do that directly, it would be great.)
**** TODO remove useless tenants from Azure
**** #  --8<-------------------------- separator ------------------------>8--
**** TODO Enable CBbackup in DO again
**** TODO Need CB backup in Bematech: Need 2TB disk
**** TODO cleanup in Bematech cb nodes: bematech-cb-02, bematech-cb-03
**** TODO Access Azure bills
Denny Zhang [11:02 AM]
Kung, is it possible that we can access Azure bills?

To avoid wasting $, it’s better we can have a detail breakdown of what we have been charged for. Even if others pay.
**** TODO fix the ES mount endpoint in Bematech
# copy folder??: /data/mnt/es-extra-volume
umount /data/mnt/es-extra-volume

export mnt_point=/data/elasticsearch
rm $mnt_point
sudo mkdir -p $mnt_point
sudo chmod 777 $mnt_point

mount -o discard,defaults /dev/sdc /data/elasticsearch
**** #  --8<-------------------------- separator ------------------------>8--
**** TODO In Bematech, remove some ES nodes to save cost

*** 2017-02-16: reset totvs.com email password
  CLOSED: [2017-02-16 Thu 12:06]
Change.This@that

Change.That@this
This.That@Change
That.Change@this

*** 2017-02-16: watch bematech CB rebalancing
   CLOSED: [2017-02-16 Thu 12:31]
*** 2017-02-16: Elasticsearch in prod-es-17 suddenly run into single node mode
  CLOSED: [2017-02-16 Thu 15:20]
https://trello.com/c/JvczPfqS
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-02-16: Add wiki: ChatOps via Slack integration: 1h
  CLOSED: [2017-02-16 Thu 22:17]
https://bitbucket.org/nubesecure/devops/wiki/ChatOps%20via%20Slack%20integration
*** 2017-02-16: Implement the first slack command: /dcloudexpense: 2h
   CLOSED: [2017-02-16 Thu 22:35]
*** 2017-02-16: #5: dofacdenny bitbucket repo is over 1GB, and keep growing: 4h
  CLOSED: [2017-02-16 Thu 14:01]
https://bitbucket.org/nubesecure/devops/issues/5/dofacdenny-bitbucket-repo-is-over-1gb-and#comment-34467126
How to verify?
git push to shrink-repo branch of dofacdenny, or run bitbucket pipeline in GUI

We shall see .git directory shrink into as smaller as ~132MB. And a clean git clone would take ~333MB

https://bitbucket.org/nubesecure/dofacdenny/addon/pipelines/home#!/results/%7Be4925d9d-74d2-475c-8871-9d5a85299d7b%7D

https://bitbucket.org/nubesecure/devops/issues/5/dofacdenny-bitbucket-repo-is-over-1gb-and

check repo size: https://bitbucket.org/nubesecure/dofacdenny/admin

git clone --mirror git@bitbucket.org:nubesecure/dofacdenny.git
bfg --strip-blobs-bigger-than 50M dofacdenny.git

cd dofacdenny.git
git reflog expire --expire=now --all && git gc --prune=now --aggressive

git push

*** 2017-02-17: (0.5) update slack notification, since slack channels have renamed
  CLOSED: [2017-02-17 Fri 15:56]
"fd-dev" to "carol-dev"
"fd-platform" to "carol-platform"
"fd-product" to "carol-product"
"fd-production" to "carol-production"

"fd-alerts" to "carol-alerts"
"fd-alerts-bematech" to "carol-alerts-bematech"
"fd-alerts-prod" to "carol-alerts-prod"
"fd-bematech" to "carol-bematech"
"fd-explore" to "carol-explore"
"fd-metric" to "carol-metric"
**** 2017-02-17: slack setting
    CLOSED: [2017-02-17 Fri 15:20]
**** 2017-02-17: uptimerobots
    CLOSED: [2017-02-17 Fri 15:20]

**** 2017-02-17: all jenkins
   CLOSED: [2017-02-17 Fri 15:20]
http://bematechjenkins.fluigdata.com:18080
http://prodjenkins.fluigdata.com:18080

http://injenkins.fluigdata.com:48080
http://repo.fluigdata.com:18080

su jenkins

cd ~/jobs

find -name config.xml | xargs grep "fd-"

find -name config.xml | xargs grep "fd-alerts"
find -name config.xml | xargs sed -i "s/fd-alerts/carol-alerts/g"

find -name config.xml | xargs grep "fd-bematech"
find -name config.xml | xargs sed -i "s/fd-bematech/carol-bematech/g"

find -name config.xml | xargs grep "fd-metric"
find -name config.xml | xargs sed -i "s/fd-metric/carol-metric/g"

find -name config.xml | xargs grep "fd-"

find -name config.xml | xargs sed -i "s/fd-dev/carol-dev/g"
find -name config.xml | xargs grep "fd-dev"

find -name config.xml | xargs grep "fd-product"

**** TODO update explore env

*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-02-20: In Bematech Azure, fail-over bematech-cb-4 with a new replacement
   CLOSED: [2017-02-20 Mon 17:52]
*** 2017-02-20: watch es cluster status, since bematech-es-4 is down
  CLOSED: [2017-02-20 Mon 19:55]
ssh -p 2702 root@191.234.189.51
*** 2017-02-21: branch out 1.60 and run all CI tests
   CLOSED: [2017-02-21 Tue 12:34]

*** 2017-02-21: fail-over the cb node and remove it: bematech-cb-4(10.1.1.10)
  CLOSED: [2017-02-21 Tue 16:59]
191.234.187.147
*** 2017-02-22: add one more CB and ES node to azure
   CLOSED: [2017-02-22 Wed 07:57]

*** 2017-02-22: Add wiki: Feedback About Azure Usage
  CLOSED: [2017-02-22 Wed 14:17]
https://github.com/TOTVS/mdmdevops/wiki/Feedback-About-Azure-Usage
*** 2017-02-22: [#A] Remove useless tenants: http://injenkins.fluigdata.com:48080/view/Basic/job/DockerDeploySandboxActiveSprint/260/console
  CLOSED: [2017-02-22 Wed 14:20]
https://admin.fluigdata.com:52443/fluigdata-admin/
admin@totvs.com Foobar1!

ufw allow 52443/tcp

curl -I https://admin.fluigdata.com/fluigdata-admin/

45.33.87.74
https://injenkins.fluigdata.com:52443

token

curl -k -XGET "https://localhost/api/v1/admin/tenantsAdmin/" -H "Authorization: $token"

totvslabs
**** [#A] in prod env
apt-get install -y jq

username="admin@totvs.com"
password="totvs@labs@921"

token=$(curl -k -s -XPOST "https://localhost/api/v1/oauth2/token" -d "grant_type=password&username=$username&password=$password&subdomain=admin&applicationId=0a0829172fc2433c9aa26460c31b78f0" | jq -r '.access_token')

echo $token

# List tenants (should have 17 tenants)
curl -k -XGET "https://localhost/api/v1/admin/tenantsAdmin/" -H "Authorization: $token"

# Remove tenant
tenant_name=totvs
curl -k -XDELETE "https://localhost/api/v1/admin/tenantsAdmin/$tenant_name" -H "Authorization: $token"

# List tenants (should have 16 tenants)
curl -k -XGET "https://localhost/api/v1/admin/tenantsAdmin/" -H "Authorization: $token"

# Remove tenant
tenant_name=bematechn
curl -k -XDELETE "https://localhost/api/v1/admin/tenantsAdmin/$tenant_name" -H "Authorization: $token"

# List tenants (should have 15 tenants)
curl -k -XGET "https://localhost/api/v1/admin/tenantsAdmin/" -H "Authorization: $token"

**** manually action
# in one app nodes

# Get tenants list
curl -k -XGET "https://localhost/api/v1/admin/tenantsAdmin/" -H "Authorization: $token"

# Remove tenants
tenant_name=totvslabs
curl -k -XDELETE "https://localhost/api/v1/admin/tenantsAdmin/$tenant_name" -H "Authorization: $token"

# Get tenants list
curl -k -XGET "https://localhost/api/v1/admin/tenantsAdmin/" -H "Authorization: $token"
**** TODO remove tenants from Bruno
ech on Azure runs normally? if yes, then we can start that

Denny Zhang [3:08 PM] 
Looks like so. See #fd-bematech

@kungwang & @bruno

Any idea how to remove tenants at application level?

If we want to do that today, we need to wait several hours. ES cluster is running data rebalancing, after the planned maintenance of DO this morning.

Bruno Volpato [3:10 PM] 
we have an API to do that


[3:11]  
but it's better if we deploy latest 1.58 to the env

[3:11]  
@wilson we have nothing blocking us from update 1.58 right?

Wilson Souza [3:13 PM] 
@robson.poffo opened a blocker today that is also happening on 1.58, but it works fine after a refresh so I think we can update and fix it on 1.59

[3:14]  
I also moved one ticket that has a similar situation to blockers

Denny Zhang [3:15 PM] 
Any doc about that API?

I’d like have a try offline first.

Bruno Volpato [3:25 PM] 
uploaded this image: Pasted image at 2017-02-16, 1:25 PM
Add Comment

Denny Zhang [3:27 PM] 
Nice. add how to list tenants?

[3:28]  
also how to create one dummy tenant.

Typical CRUD.

Bruno Volpato [3:28 PM] 
```token=$(curl -k -s -XPOST "https://localhost/api/v1/oauth2/token" -d 'grant_type=password&username=admin@totvs.com&password=Foobar1!&subdomain=admin&applicationId=0a0829172fc2433c9aa26460c31b78f0' | jq -r '.access_token')
curl -k -XDELETE "https://localhost/api/v1/admin/tenantsAdmin/bruno" -H "Authorization: $token"
```

[3:28]  
you can do it through UI

[3:28]  
https://admin.fluigdata.com/fluigdata-admin/

[3:28]  
admin@totvs.com Foobar1! is the default

Denny Zhang [3:29 PM] 
Nice. Will give it a try in local all-in-one env.

*** 2017-02-23: scale down DO env: CB(3) + ES(3) + AppUI(2) + AppWorker(1) + LB(2)
  CLOSED: [2017-02-23 Thu 15:54]
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
curl $es_ip:9200/_cat/shards?v | grep -v STARTED

service couchbase-server status
service couchbase-server stop

service elasticsearch status
service elasticsearch stop
**** 2017-02-23: Stop prod-es-25(138.197.217.168) now
    CLOSED: [2017-02-21 Tue 12:42]
**** 2017-02-23: Fail-over and stop prod-cb-07(138.197.217.56) now.
    CLOSED: [2017-02-21 Tue 12:42]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-02-23: Fail-over and stop prod-cb-01(138.197.194.193) now.
    CLOSED: [2017-02-21 Tue 14:41]
**** 2017-02-23: Stop prod-es-24(138.197.217.103) now
    CLOSED: [2017-02-21 Tue 14:41]
**** 2017-02-23: Fail-over and stop prod-cb-06(138.197.193.160) now.
    CLOSED: [2017-02-21 Tue 14:42]
**** 2017-02-23: Stop prod-es-23(138.197.202.167) now.
    CLOSED: [2017-02-21 Tue 14:42]

**** 2017-02-23: Stop prod-es-22(138.197.198.250) now.
    CLOSED: [2017-02-21 Tue 14:43]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-02-23: Fail-over and stop prod-cb-05(138.68.17.99) now.
    CLOSED: [2017-02-21 Tue 15:20]
**** 2017-02-23: Stop prod-es-21(138.197.193.202) now.
    CLOSED: [2017-02-21 Tue 15:20]
**** 2017-02-23: Stop prod-es-20(138.68.46.207) now.
    CLOSED: [2017-02-21 Tue 16:12]
**** 2017-02-23: update repo jenkins: tcpscan check
    CLOSED: [2017-02-21 Tue 17:45]
**** 2017-02-23: Stop prod-es-19(138.68.44.102) now.
    CLOSED: [2017-02-21 Tue 17:45]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-02-23: Stop prod-app-04(138.197.214.85) now.
    CLOSED: [2017-02-21 Tue 17:49]
**** 2017-02-23: Stop prod-app-05(138.197.222.35) now.
    CLOSED: [2017-02-21 Tue 17:49]
**** 2017-02-23: Stop prod-app-06(138.197.217.166) now.
    CLOSED: [2017-02-21 Tue 17:49]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-02-23: update jenkins jobs: to remove those nodes
    CLOSED: [2017-02-23 Thu 15:54]
**** 2017-02-23: remove ES volumes
    CLOSED: [2017-02-23 Thu 15:54]
**** TODO update ufw firewall: http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/309/console
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-02-23: verify nagios check: enforce all critical nodes are properly monitored and no useless monitoring
    CLOSED: [2017-02-23 Thu 15:54]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-02-23: Improve bematech jenkins: update server list, nagios parameters for chef_json, slack notification
   CLOSED: [2017-02-23 Thu 13:19]
*** 2017-02-23: #2094 (1) According to Bematech env, lower the ranking of OOM for ES process
  CLOSED: [2017-02-23 Thu 15:40]
https://trello.com/c/y6zmfNsL
/opt/devops/bin/disable_oom.sh /var/run/elasticsearch/elasticsearch.pid

http://bematechjenkins.fluigdata.com:18080/job/RunCommandOnServers/55/console
*** 2017-02-23: (1) Bug: In cluster deployment, first ES node will only have itself in the unicast list
  CLOSED: [2017-02-23 Thu 15:30]
https://trello.com/c/JNJYqYOG
Currently unicast list for elasticsearch.yml is caculated like below:

1. When deploy any ES node, get the ip of first ES node
- Query that ES instance to list all ES nodes
- Use the ip of these ES nodes as multi-cast ip list in elasticsearch.yml

It's better change the logic into below:

1. When deploy any ES node, get the ip of first ES node
- Query that ES instance to list all ES nodes
- Get the list of REST API returns, and combine it with ES nodes given by end user through common-basic::elasticsearch_hosts
- Use the combined list as the multi-cast ip list in elasticsearch.yml

*** 2017-02-23: cleanup iptables firewall for all DO nodes, since the scale down: http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/310/console
   CLOSED: [2017-02-23 Thu 22:58]
*** 2017-02-23: [#A] backup and restore DigitalOcean: Robson's VM: https://cloud.digitalocean.com/droplets/16861882/snapshots?i=4fab62
  CLOSED: [2017-02-23 Thu 22:58]
https://en.wikipedia.org/wiki/WSO2

#+BEGIN_EXAMPLE
Denny Zhang [7:24 PM] 
About smartcontact backup

Here is my plan. (Shall need some support from you).

Please let me know whether it works for you.

1. Create a snapshot of the VM
2. Shutdown the VM
3. Create a new VM from this snapshot. Let’s say VM2
4. [Robson] Please to verify VM2 it’s functionally working.
5. If it is, destroy VM and VM2.
6. We keep the snapshot for future usage.

Denny Zhang [12:32 AM] 
For the cost of snapshot, $0.05/GB/mo.

Just finished step1, and it has generated ~40GB snapshot. So it would be $2/mon.

Not much.
#+END_EXAMPLE
*** 2017-02-24: Add bematech-es-12 to bematech for OOM for ES nodes
   CLOSED: [2017-02-24 Fri 14:41]
*** 2017-02-24: #2089 Explore Environment updated everyday automatically
  CLOSED: [2017-02-24 Fri 16:26]
https://trello.com/c/7l2cpbYW
*** 2017-02-24: Add wiki: MDM Explore Env In Linode
  CLOSED: [2017-02-24 Fri 16:16]
https://github.com/TOTVS/mdmdevops/wiki/MDM-Explore-Env-In-Linode

*** 2017-02-25: Merge 1.59 To Master and run all CI tests
   CLOSED: [2017-02-25 Sat 07:19]
*** 2017-02-25: Add bematech-es-13 to ES cluster
   CLOSED: [2017-02-25 Sat 12:53]
*** 2017-02-26: [#A] change ES java Xmx and Xms from 16G to 12G: no need for es-10 and es-12
  CLOSED: [2017-02-26 Sun 10:25]
http://bematechjenkins.fluigdata.com:18080/job/RunCommandOnServers/64/console

service elasticsearch stop

service elasticsearch status

sleep 3

service elasticsearch start

es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

curl $es_ip:9200/_cat/nodes?v

ps -ef | grep ela

curl $es_ip:9200/_cat/shards?v | grep " p "
curl $es_ip:9200/_cat/shards?v | grep -v STARTED
curl $es_ip:9200/_cat/nodes?v
curl $es_ip:9200/_cluster/health?pretty
**** 2017-02-26: bematech-es-9
    CLOSED: [2017-02-26 Sun 10:25]
**** 2017-02-26: bematech-es-7
    CLOSED: [2017-02-26 Sun 09:33]
**** 2017-02-26: bematech-es-3
    CLOSED: [2017-02-25 Sat 23:00]
**** 2017-02-26: bematech-es-4
    CLOSED: [2017-02-25 Sat 22:57]
**** 2017-02-26: bematech-es-6
    CLOSED: [2017-02-25 Sat 18:34]
**** 2017-02-26: bematech-es-5
    CLOSED: [2017-02-25 Sat 17:41]
**** 2017-02-26: bematech-es-8
    CLOSED: [2017-02-25 Sat 11:00]
**** 2017-02-26: bematech-es-2
    CLOSED: [2017-02-25 Sat 11:00]
**** 2017-02-26: bematech-es-1
    CLOSED: [2017-02-25 Sat 08:49]

*** 2017-02-26: Add wiki: How To Update Demo Env In Linode: 2h
  CLOSED: [2017-02-26 Sun 11:27]
https://bitbucket.org/nubesecure/devops/wiki/How%20To%20Update%20Demo%20Env%20In%20Linode
*** 2017-02-26: pass working_env via arguments: 1h
   CLOSED: [2017-02-26 Sun 21:18]
*** 2017-02-26: #16 Automate the application deployment process in fresh VM
  CLOSED: [2017-02-26 Sun 22:42]
https://bitbucket.org/nubesecure/devops/issues/16/automate-the-application-deployment
*** 2017-02-26: #18 DeployDemoEnv: wrap up jenkins job to improve user experience: 2h+3h
  CLOSED: [2017-02-26 Sun 22:43]
https://bitbucket.org/nubesecure/devops/issues/18/deploydemoenv-wrap-up-jenkins-job-to
denny/sophia1

ssh -p 2702 root@45.33.55.168

docker stop myjenkins; docker rm myjenkins
mkdir -p /root/jenkins
chmod 777 /root/jenkins

docker run --name myjenkins -p 48084:8080 -p 50000:50000 -v /root/jenkins:/var/jenkins_home jenkins

http://www.shibgeek.com:48084
testadmin/soteria123
**** setup jenkins
testadmin
soteria123
TestAdmin
denny.zhang@soterianetworks.com
**** iptables setup
iptables -F; iptables -X
echo 'y' | ufw reset
echo 'y' | ufw enable
ufw default deny incoming
ufw default deny forward

# Allow traffic of safe ports
ufw allow 22,80,443/tcp

# Allow traffic from certain port
ufw allow 2702/tcp

# Allow traffic from given ethernet nic
ufw allow in on docker0

ufw allow from 45.33.87.74
**** manual setup: initialize jenkins credential
**** #  --8<-------------------------- separator ------------------------>8--
**** ssh key
**** TODO Jenkins job DeployDemoEnv
**** TODO Jenkins job DeployTestEnv
**** TODO Jenkins job ShrinkdofacdennyRepo
**** TODO Jenkins job build code
**** #  --8<-------------------------- separator ------------------------>8--
**** misc
docker stop soteria-jenkins; docker rm soteria-jenkins
docker run -t -d --privileged -h soteriajenkins --name soteria-jenkins java:8 /bin/bash
docker exec -it soteria-jenkins bash

mkdir -p /root/.ssh

cat > /root/.ssh/config <<EOF
host bitbucket.org
     Port 22
     StrictHostKeyChecking no
     User git
     IdentityFile /root/.ssh/docker_bitbucket_ssh_id
EOF

cat > /root/.ssh/docker_bitbucket_ssh_id <<EOF
-----BEGIN RSA PRIVATE KEY-----
MIIEogIBAAKCAQEAt11Lbq86V2ccgWZd9NCK0pKk08wAlmmB43CrfG3IpfD/myoX
fO+EumV3+wQaF75eUXAitcjWBbQde/O5q464Q+o2ynri2r+EbmP8x8AyUgueWyvq
c0SjSN0ITwGwZwy4OaMKF7LYV0HkBrDEhIPHpTCjdD6i9AA4qoiuYitbe/b7NbVj
UH3jSrIiYSeEhMQy/xgW9DIc9XBEz14cnzxlDNcmbWFawt1T/JiQl09GElv5mtVt
6s907QyaGU3j++xllUyAi7mKgmd7iqEilRqfALSf85Ovq8SwWpsFiPCIh1YpIqZf
zXWtYrgKo0xTi3IaElLmmWnEsQdQNNF6VOvzqQIDAQABAoIBABqFZO6qa47OvCG/
M6HANAQG784ueHtI4V+LIVKK8nWF6QKBUQph/lmTaYol6rw6bZvr0yll1gbXKViP
RPCaYO55xtj8cylegAITdjFSDDUvtwTK0ks/xzo3tgbvYJtXDKJRgZkx5fag8iFB
mfrssIilA09B7AHMsmWABXDvgnjoXnhrorCEeNvv4bCvyRlAG+Xb4pWuPdeXzCk+
eCDuSSw/I7pxWsqu9Hi4NPbYshvQijr6+s8aQulCC6uKcpxL9PkWGPZu1ZHUYV+Y
u6ioGj9PjENDo0kqCWmrjkBGP8hw/Z6W76QkVMjAUDtfij9tQMyXLbzhjbMrpI1+
FNwKxzECgYEA3EaKMt5IyW9eNcceRNE8RK1U4gYC8qtYewrWC6bZpwCSmxFvFfbA
rgekeC6FstL1+dMpJFcAIQOpi3r397lSx2MO7MFRGRkgb7BAYRrr0rImDFDQsUUk
YVCbQnwec80V7xuOo0Gly8lMammGZ+KFE4o3BSmROfQJ0povnV8snXUCgYEA1RpE
5aHlxWUEUUTJ+iFPIY17I9HU3c9UGgEsH8lNG162PqudbuZ6RxATyH50zRwp6D4l
t4Vb9AXFxRbkWqUbRYfnsYnCL/tw7cSNqWBe2KNPhtbqP0b3GAiYdkYI/Br/DpWs
zy3NGfzigu504fAvGw+vW3rwGnoxRUpndt6xcuUCgYA38EX6Qw7S8NBI8ecS7NvU
SPxMZeBrxe2zWX4f+WA+k9unO5ctSbuGtnQi/RWA4ygKKIDDNMDYoXh17mV4aHzI
G7TXlQRGTmY5VtYSkuaeXBVjfUZyLi/d8r1q9eA54jQWw7XGac5z4Qvh3ih+yzym
SKxOSE8UG5A/Bl7fJSNMXQKBgH4YZCooqSgrROm9tXHfYc5txC6Gy9lQA+i7Rtus
JocKfPYKYEat9C62ca5SPVTc1GNK2tCEBcwX2lVr3UVRqxhyeYORCymPM9xNULGn
h5fS0ozsAIHewPkkpOpfOJPMeZ2srSuOY218OJ/W76x9jbIKju2niglwwlTI2P36
ea+NAoGAUW5SAYexwYDNN0cPhoPxwBFJVPQ/klIdUjYYEwm5cj/jcq4fODtXpATM
IRmZntrAytpqsu3cZCtteAiV16TOpLmB1bQhAhx7t72v1oSQUC5Kbl344ncdmSQs
nEBeo1/ORocKvy6ffNs3TjaXFI3BZqrUQM3yvKQkyyiHmDKYx1o=
-----END RSA PRIVATE KEY-----
EOF

chmod 600 /root/.ssh/docker_bitbucket_ssh_id

cd /root/

source .env
source docker-compose-vm.env
docker-compose -f docker-compose-vm.yml up

#+BEGIN_EXAMPLE
root@soteria-demo:~/penroz# docker-compose -f ./docker-compose-vm.yml up
ERROR: In file '././docker-compose-vm.yml' service 'version' doesn't have any configuration options. All top level keys in your docker-compose.yml must map to a dictionary of configuration options.
#+END_EXAMPLE
**** install java8
apt-get install openjdk-8-jre-headless
**** install docker compose
https://docs.docker.com/compose/install/
**** git ssh key
**** install latest npm
apt-get install -y build-essential

cd /tmp/
wget http://repo.fluigdata.com:18000/node-v4.5.0.tar.gz
tar -xf node-v4.5.0.tar.gz
cd node-v4.5.0
# The step of make might take 20 minutes to finish
./configure && make
make install

source /etc/profile

# node should be v4.5.0
node --version

# npm should be 2.15.9
npm --version
**** #  --8<-------------------------- separator ------------------------>8--
**** cleanup
/root/.ssh/docker_bitbucket_ssh_id
**** start the vm
cd /opt/soteria/devops/deployment_conf/demo_env

export IMG=penroz/dofacdenny

#DO NOT CHANGE THESE YET!!
export LOG_ROOT=/var/log/soteria
export IDP_HOME=/opt/shibboleth-idp

docker-compose -f docker-compose-demo.yml up
**** general procedure
ssh key passphrase: soteriaDevOps123

docker login

export working_dir=/opt/soteria

mkdir -p $working_dir
cd $working_dir
git clone git@bitbucket.org:nubesecure/devops.git

cd $working_dir/devops

git checkout demo-env

cd $working_dir/devops/deployment_conf/demo_env

docker-compose -f docker-compose-demo.yml up

cd ../
source .env
cd ./demo_env

/usr/local/bin/docker-compose -f docker-compose-demo.yml up

**** TODO weird path issue
root@shibgeek-demo-2722:/opt/soteria/devops/deployment_conf/demo_env# docker-compose -f docker-compose-demo.yml up
-bash: /usr/bin/docker-compose: No such file or directory

*** 2017-02-27: Add bematech-cb-10 To Azure CB cluster
   CLOSED: [2017-02-27 Mon 09:40]
*** 2017-02-27: After tenant removal, CNAME setting in AWS Route53 are missing
  CLOSED: [2017-02-27 Mon 12:19]
curl -I https://senar.fluigdata.com/mdm-ui
curl -I https://product.fluigdata.com/mdm-ui
curl -I https://marelli.fluigdata.com/mdm-ui
curl -I https://inovalli.fluigdata.com/mdm-ui
#+BEGIN_EXAMPLE
Add Comment

Bruno Volpato [12:09 PM] 
Robson deleted some tenants from the Azure environment

[12:09]  
by default we send the deletion to AWS

new messages
[12:09]  
the problem is that we still wanted those subdomains, but in another server

[12:10]  
I suggested him to rename the tenants in their environments and it should work fine.

Robson Poffo [12:10 PM] 
I am changing manually the domains and changing back to original name to try to fix all DO tenants

Bruno Volpato [12:10 PM] 
but maybe we can fix the logic to not delete the DNS entry, or we can verify if points to that environment before deleting
Robson Poffo [12:13 PM] 
Ok.. it will be good if we can handle this situation. I don’t believe it will be frequently

Denny Zhang [12:15 PM] 
Yeah, let’s manually fix the DNS setting for this time.

BTW, I’ve created curl monitoring for below links. Hope it helps for issues like this in the future.

```https://senar.fluigdata.com/mdm-ui
https://product.fluigdata.com/mdm-ui
https://marelli.fluigdata.com/mdm-ui
https://inovalli.fluigdata.com/mdm-ui```
#+END_EXAMPLE
*** 2017-02-27: [#A] CB cluster change "Data RAM Quota” from 25G to 26G. :IMPORTANT:
  CLOSED: [2017-02-27 Mon 13:13]
http://bematechjenkins.fluigdata.com:18080/job/RunCommandOnServers/71/console
#+BEGIN_EXAMPLE
Denny Zhang [Today at 9:50 AM] 
in #carol-bematech
I’ve run “free -ml” for all CB nodes here.
http://bematechjenkins.fluigdata.com:18080/job/RunCommandOnServers/71/console

It looks like each node have used most of RAM. The good thing is around 4GB RAM was cached on average.

Right now, we are setting “Data RAM Quota” as 25G.

So I’m thinking if we reserve more RAM to CB, we might need less CB nodes as the data keeps coming.

Agree, @bruno & @kungwang ?

5 replies
Denny Zhang [1 hour ago] 
Let me change “Data RAM Quota” from 25G to 26G now. Agree, @bruno & @kungwang ? (edited)

Bruno Volpato [1 hour ago] 
Yes, if the machines handle it, it's fine. The nodes are focused to CB anyways

Denny Zhang [1 hour ago] 
Same point here.

Kung Wang [6 minutes ago] 
@denny.zhang & @bruno , same to Bruno said, let's increase to 26G and see

Denny Zhang [3 minutes ago] 
Cool. Yes, we have performed the change this morning. I will keep watching couchbase cluster from time to time.

Will try to replace existing CB nodes(DS4_V2) with DS12_V2 after everything looks stable. Probably tomorrow or the day after tomorrow.

When CB is fine, we will see whether to perform the same change for ES cluster.

This would save us $124/month for the replacement of one single node.
#+END_EXAMPLE
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-02-28: #7: Fix chef code defects found by rubocop: 2h
  CLOSED: [2017-02-28 Tue 12:02]
https://bitbucket.org/nubesecure/chef/issues/7/fix-chef-code-defects-found-by-rubocop

*** 2017-02-28: automate plugin installation: http://injenkins.fluigdata.com:48080/job/DockerDeployFeatureCookbooks/312/console
  CLOSED: [2017-02-28 Tue 18:00]
Bruno Volpato [4:43 PM] 
installing this plugin by default can be a good thing: `bin/plugin install delete-by-query`

[4:43]  
I think we already insteall head by default, so shouldn't be a big deal to add, am I right @denny.zhang ?

Denny Zhang [4:44 PM] 
Yea, we have installed head by default.

[4:44]  
Let me automate the installation of delete-by-query in 1.60
** 2017-03
*** 2017-03-01: (2) Nagios monitor disk capacity of all mounted volume, instead of rootfs only
  CLOSED: [2017-03-01 Wed 10:33]
https://trello.com/c/t5Epncaa

*** 2017-03-01: add bematech-cb-11 to cb cluster
   CLOSED: [2017-03-01 Wed 23:06]
*** 2017-03-02: Code build fail at npm install under carol-docs folder: http://injenkins.fluigdata.com:48080/job/BuildMDMRepoCodeNoAlert/8/console
   CLOSED: [2017-03-02 Thu 11:30]

*** 2017-03-03: Add wiki: How To Backup Demo Env In Linode: 1h
  CLOSED: [2017-03-03 Fri 18:17]
https://bitbucket.org/nubesecure/devops/wiki/How%20To%20Backup%20Demo%20Env%20In%20Linode
*** 2017-03-03: Soteria Jenkins: create a dashboard with history and performance metrics: 1h
   CLOSED: [2017-03-03 Fri 18:38]
*** 2017-03-04: Add bematech-cb-12 to Azure: address CB memory warning
   CLOSED: [2017-03-04 Sat 09:14]
*** 2017-03-04: #28 enable db backup for mysql container: 2h
  CLOSED: [2017-03-04 Sat 10:39]
> docker_backup_mysql.py && vim docker_backup_mysql.py

export container_name="db"
export db_name="shibgeek"
export db_username="shibgeek"
export db_password="password"
export dst_dir="/data/backup/mysql"

python ./docker_backup_mysql.py \
       --container_name $container_name --db_name $db_name --db_username $db_username
       --db_passwd $db_password --dst_dir $dst_dir
*** Enforce data rention
*** 2017-03-05: PollSystemMetric: get data volumes: 1h
  CLOSED: [2017-03-05 Sun 23:54]
http://jenkins.shibgeek.com:48084/job/PollSystemMetric/6/console
*** 2017-03-05: enable daily deployment for explore env
  CLOSED: [2017-03-06 Mon 09:34]
http://explorejenkins.fluigdata.com:18080/job/AutoDailyUpdateSandboxMDM/1/console

- Persist change
- crontab
- enable reporting
- jenkins job
- update wiki
#  --8<-------------------------- separator ------------------------>8--

Robson Poffo [12:53 PM] 
@kungwang, @bruno, @mitu What’s the pros/cons about auto update our explore env (like daily or other frequency)? Considering the data migration and other factors.

Kung Wang [12:55 PM] 
if it's a single sandbox node, I do not see any difficulty. the down side is, while you are using it, it may go down and come backup after couple minutes. And this can happen many times depends on the frequency of that auto-refresh

Mitu Singh [12:56 PM] 
@robson.poffo do you mean update the env with the current sprint branch everday? (edited)

Robson Poffo [12:57 PM] 
About the frequency, I was thinking daily or every 2 days or something like that.

@mitu, I was thinking about update only the Explore env with the master branch.

Mitu Singh [12:58 PM] 
the master branch is only updated at the end of the sprint after Wilson's finishes his test on the sprint branch

Kung Wang [12:58 PM] 
if it's very less frequency, then should be np for our staging env user. We can do that using jenkins scheduling.

Mitu Singh [12:58 PM] 
so there is no point in updating it every 2 days

Kung Wang [12:59 PM] 
Mitu, I think Robson only want current branch

Robson Poffo [12:59 PM] 
Got it @mitu.

Kung Wang [12:59 PM] 
because staging env is only for current branch

Robson Poffo [12:59 PM] 
Yes, I was thinking about something more frequent. (edited)

Kung Wang [1:01 PM] 
so, Robson, is that what you have in mind? deploy only the current branch, for example, 1.60?

Kung Wang [1:08 PM] 
@robson.poffo, the same question, is current branch(ex: 1.60) is the branch to deploy to explore?

Mitu Singh [1:13 PM] 
Kung, robson is in meeting

[1:13]  
He said he would want to deploy 1.59. The branch that we finished developing on and the one that Wilson is testing

Kung Wang [1:14 PM] 
got it, that also makes sense (edited)

Bruno Volpato [1:16 PM] 
The downside is exactly what Kung mentioned. For sandbox, we can have downtime. Also faster iterations can lead to more problems, because featured are untested. So not recommended for critic environments. For test environments it should be fine and it can be easily achieved through Jenkins scheduling.

Denny Zhang [1:17 PM] 
Note: in case we need.  we also have a Jenkins server and http server which runs  daily build for current sprint.

Robson Poffo [1:34 PM] 
Ok, the idea is what Mitu said, update 1.59 anticipating the production update. It doesn’t matter downtime, that’s fine since it’s a test environment.

[1:35]  
Is it possible to schedule this process?

[1:35]  
I mean, run automatically.

Kung Wang [1:56 PM] 
I believe so. Denny, can you help Robson to schedule Jenkins build/deploy to run once a day?

Denny Zhang [2:01 PM] 
Yes, fine to me.

Wilson Souza [2:03 PM] 
I think it’s a good idea, it will help us to anticipate eventual issues with Robson's use cases (edited)

Denny Zhang [2:04 PM] 
@robson.poffo

I’m in the middle of some other tasks.
Let me enforce that early tomorrow morning. And document details in wiki, and send the link to you.

How is that?

*** 2017-03-05: DevOps doc: Gap Analysis: 2h
  CLOSED: [2017-03-06 Mon 10:31]
https://docs.google.com/document/d/1jn9RAubuw-AGKxYbE76jblbHiIw-IXQY5ilCNTkqRXg/edit
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-03-06: #25: Deprecate and remove filebeat log docker: 1h
  CLOSED: [2017-03-06 Mon 13:52]
https://bitbucket.org/nubesecure/devops/issues/25/deprecate-and-remove-filebeat-log-docker
*** 2017-03-06: Add one more CB node for DO, to address low disk capacity
   CLOSED: [2017-03-06 Mon 18:16]
*** 2017-03-07: #16 [Docker] - Add some troubleshooting packages to base image: 2h
  CLOSED: [2017-03-07 Tue 08:47]
https://bitbucket.org/nubesecure/brozton/issues/16/docker-add-some-troubleshooting-packages
*** 2017-03-07: #9 Upgrade jetty from 9.3.x to the latest: 3h
  CLOSED: [2017-03-07 Tue 10:43]
https://bitbucket.org/nubesecure/brozton/issues/9/upgrade-jetty-from-93x-to-the-latest
*** 2017-03-07: Bug: delete branch and create tag result in conflicts
  CLOSED: [2017-03-07 Tue 12:11]
I’ve deleted 1.60 branch and created tag 1.60.

Then the deployment results in git conflict. That’s why this deployment fails.

Let me bypass this. And think about how to avoid this in the future.

*** 2017-03-07: DO CB cluster run into low disk: add a new CB node and change the default machine flavor
   CLOSED: [2017-03-07 Tue 12:12]
*** 2017-03-08: Technical discussion in google hangout: jetty/idp docker build; ci process refine: 2h
   CLOSED: [2017-03-08 Wed 14:12]
*** 2017-03-08: wrong slack notification: DeploySystem
   CLOSED: [2017-03-08 Wed 19:08]

*** 2017-03-09: [#A] Replace old CB with new machine flavor
  CLOSED: [2017-03-09 Thu 10:42]
138.197.208.92:2702
138.197.193.196:2702
138.197.201.198:2702
138.68.3.236:2702
**** 2017-03-09: add new 3 CB nodes
    CLOSED: [2017-03-07 Tue 23:15]
**** 2017-03-09: Remove nagios
    CLOSED: [2017-03-07 Tue 22:45]
**** 2017-03-09: Update mdm.yml: http://prodjenkins.carol.ai:18080/job/RunCommandOnServers/320/console
    CLOSED: [2017-03-07 Tue 23:57]
**** 2017-03-09: Update Jenkins jobs
    CLOSED: [2017-03-07 Tue 23:57]
**** 2017-03-09: Shutdown and remove old 3 CB nodes: cb-01
    CLOSED: [2017-03-08 Wed 06:25]
**** 2017-03-09: Shutdown and remove old 3 CB nodes: cb-02
    CLOSED: [2017-03-08 Wed 11:58]
**** #  --8<-------------------------- separator ------------------------>8--
**** TODO Shutdown and remove old 3 CB nodes: cb-04
**** #  --8<-------------------------- separator ------------------------>8--
**** TODO Update ufw for existing nodes
| ~~prod-cb-01~~ | ~~138.197.208.92/10.138.96.121~~ | ~~Type3~~ | ~~Couchbase~~ |
| ~~prod-cb-02~~ | ~~138.197.193.196~~              | ~~Type3~~ | ~~Couchbase~~ |
| ~~prod-cb-03~~ | ~~138.197.201.198~~              | ~~Type3~~ | ~~Couchbase~~ |
| ~~prod-cb-04~~ | ~~138.68.3.236~~                 | ~~Type3~~ | ~~Couchbase~~ |
*** 2017-03-09: (2) Test DigitalOcean machines with big extra hard drive, in order to cut numbers of couchbase nodes.
  CLOSED: [2017-03-09 Thu 10:23]
#+BEGIN_EXAMPLE
Denny Zhang [5:34 PM] 
Maintenance Notification In advance:

Avoid CB nodes to have extra hard drive, in order to address rebalancing performance issue.

Changeset summary:
1. Add 3 CB nodes without extra hard drive and private network enabled.
2. Retire existing CB nodes.
#+END_EXAMPLE
**** 2017-03-09: add prod-cb-01(138.68.227.193)
    CLOSED: [2017-03-08 Wed 19:42]
**** 2017-03-09: Remove prod-cb-1(138.68.10.138)
    CLOSED: [2017-03-08 Wed 21:23]
**** 2017-03-09: Add prod-cb-02(138.68.226.143)
    CLOSED: [2017-03-08 Wed 21:43]
**** 2017-03-09: update mdm.yml: http://prodjenkins.carol.ai:18080/job/RunCommandOnServers/328/console
    CLOSED: [2017-03-08 Wed 21:46]
**** 2017-03-09: Remove prod-cb-2(138.197.213.198)
    CLOSED: [2017-03-08 Wed 23:00]
**** 2017-03-09: Add prod-cb-03(138.197.193.175)
    CLOSED: [2017-03-08 Wed 23:10]
**** 2017-03-09: Remove prod-cb-3(138.197.214.120)
    CLOSED: [2017-03-09 Thu 07:50]
**** 2017-03-09: update jenkins jobs
    CLOSED: [2017-03-09 Thu 08:21]
**** 2017-03-09: Add prod-cb-4(138.68.57.16)
    CLOSED: [2017-03-09 Thu 09:55]
**** 2017-03-09: Remove prod-cb-04(138.68.3.236)
    CLOSED: [2017-03-09 Thu 09:55]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-03-09: update mdm.yml: second time: http://prodjenkins.carol.ai:18080/job/RunCommandOnServers/334/console
    CLOSED: [2017-03-09 Thu 09:21]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-03-09: update repo.carol.ai jenkins jobs
    CLOSED: [2017-03-09 Thu 10:13]
**** 2017-03-09: clean up ufw: http://prodjenkins.carol.ai:18080/job/RunCommandOnServers/336/console
   CLOSED: [2017-03-09 Thu 10:23]
http://prodjenkins.carol.ai:18080/job/RunCommandOnServers/335/console
*** #  --8<-------------------------- separator ------------------------>8--

*** 2017-03-09: add bematech-es-14(191.232.252.249) to address ES OOM issue
   CLOSED: [2017-03-09 Thu 11:53]

*** 2017-03-10: Technical discussion, code review and free chat: 1h
   CLOSED: [2017-03-10 Fri 07:44]
*** 2017-03-10: #2713 (1) Make accessible outside TOTVS Labs "prod-docs.carol.ai:8083" - protect by password
  CLOSED: [2017-03-10 Fri 11:54]
https://trello.com/c/RnHT0Mxv
Make accessible outside TOTVS Labs "prod-docs.carol.ai:8083" - protect by password

username: totvslabs
password: fluigdata

http://prod-doc.carol.ai
*** 2017-03-10: Add bematech-es-15 to address ES OOM issue
   CLOSED: [2017-03-10 Fri 23:16]
*** 2017-03-11: #9 Upgrade jetty from 9.3.x to the latest: 5h
  CLOSED: [2017-03-11 Sat 12:46]
https://bitbucket.org/nubesecure/brozton/issues/9/upgrade-jetty-from-93x-to-the-latest
curl http://localhost:80
*** 2017-03-11: Issue #19: Add docker healthcheck to Dockerfiles of all critical images: 3h
  CLOSED: [2017-03-11 Sat 21:27]
https://bitbucket.org/nubesecure/devops/issues/19/add-docker-healthcheck-to-dockerfiles-of
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-03-13: Add wiki: One Button Deployment: Setup MDM QA Cluster Env
  CLOSED: [2017-03-13 Mon 13:33]
https://github.com/TOTVS/mdmdevops/wiki/One-Button-Deployment:-Setup-MDM-QA-Cluster-Env
*** 2017-03-13: 2h: Enforce docker benchsecurity: http://jenkins.shibgeek.com:48084/job/RunDockerBenchSecurity/3/console
   CLOSED: [2017-03-13 Mon 18:38]

*** 2017-03-14: #1807 (2) check_elasticsearch_replica.py: confirm no SPOF and detect split brain: http://injenkins.carol.ai:48080/view/Basic/job/DockerDeployFeatureCookbooks/378/console
  CLOSED: [2017-03-14 Tue 10:44]
https://trello.com/c/larLt6vg

/Users/mac/Dropbox/private_data/project/devops_consultant/consultant_code/devops_public/python/check_elasticsearch/check_elasticsearch_replica.py

> /tmp/check_elasticsearch_replica.py && vim /tmp/check_elasticsearch_replica.py
python /tmp/check_elasticsearch_replica.py --min_replica_count 2 --es_pattern_regexp "master-index-.*|master-.*|staging-.*"
*** 2017-03-14: #1776 (2) Nagios get alerts, when any ES node is running in single node mode
  CLOSED: [2017-03-14 Tue 10:44]
https://trello.com/c/FAfrRwtQ
*** 2017-03-14: ES verify shard and replica count
   CLOSED: [2017-03-14 Tue 10:44]
*** 2017-03-14: check_elasticsearch_shard.py
  CLOSED: [2017-03-14 Tue 12:20]
> /tmp/check_elasticsearch_shard.py && vim /tmp/check_elasticsearch_shard.py
python /tmp/check_elasticsearch_shard.py --min_shard_count 3  --es_pattern_regexp "master-index-.*|master-.*|staging-.*"

es_host='172.17.0.3'
es_port='9200'
curl http://$es_host:$es_port/_cat/indices?v

curl http://$es_host:$es_port/master-index-5b010c60f21111e6a49e0242ac110003/_settings?pretty

master-index-5b010c60f21111e6a49e0242ac110003
*** 2017-03-14: Detect OOM happens in the prevoius several hours
  CLOSED: [2017-03-14 Tue 15:46]
/Users/mac/Dropbox/private_data/project/devops_consultant/consultant_code/devops_public/nagios_plugins/check_out_of_memory/check_out_of_memory.py

> /tmp/check_out_of_memory.py && vim /tmp/check_out_of_memory.py
python /tmp/check_out_of_memory.py --hours_to_check 10

dmesg -T | grep -i oom

#+BEGIN_EXAMPLE
denny zhang [3:20 PM]
Looks like OOM.

That machine only has 16GB RAM.

Bruno Volpato [3:20 PM]
no thread dumps in the server

denny zhang [3:20 PM]
Yes, confirmed OOM, by checking /var/log/syslog

Bruno Volpato [3:20 PM]
oomkiller?
#+END_EXAMPLE

#+BEGIN_EXAMPLE
Jan  6 20:46:52 prod-app-6 kernel: [2493603.523716] Out of memory: Kill process 7727 (java) score 940 or sacrifice child
Jan  6 20:46:52 prod-app-6 kernel: [2493603.533633] Killed process 7727 (java) total-vm:819983548kB, anon-rss:15801880kB, file-rss:0kB
Jan  6 20:47:00 prod-app-6 kernel: [2493611.436820] [UFW BLOCK] IN=eth0 OUT= MAC=0a:8a:0c:e1:f6:f6:3c:8a:b0:0d:3f:f0:08:00 SRC=218.161.16.129 DST=45.55.9.157 LEN=40 TOS=0x00 PREC=0x00 TTL=244 ID=50323 PROTO=TCP SPT=43935 DPT=23 WINDOW=14600 RES=0x00 SYN URGP=0
Jan  6 20:49:17 prod-app-6 kernel: [2493748.282652] [UFW BLOCK] IN=eth0 OUT= MAC=0a:8a:0c:e1:f6:f6:3c:8a:b0:0d:3f:f0:08:00 SRC=169.54.244.84 DST=45.55.9.157 LEN=40 TOS=0x00 PREC=0x00 TTL=245 ID=15564 PROTO=TCP SPT=13791 DPT=8088 WINDOW=1024 RES=0x00 SYN URGP=0
Jan  6 20:49:25 prod-app-6 kernel: [2493756.732625] [UFW BLOCK] IN=eth0 OUT= MAC=0a:8a:0c:e1:f6:f6:3c:8a:b0:0d:3f:f0:08:00 SRC=89.189.128.8 DST=45.55.9.157 LEN=40 TOS=0x00 PREC=0x00 TTL=242 ID=53100 PROTO=TCP SPT=35965 DPT=23 WINDOW=14600 RES=0x00 SYN URGP=0
Jan  6 20:50:07 prod-app-6 nagios3: SERVICE ALERT: localhost;HTTP;CRITICAL;SOFT;1;Connection refused
Jan  6 20:51:07 prod-app-6 nagios3: SERVICE ALERT: localhost;HTTP;CRITICAL;SOFT;2;Connection refused
#+END_EXAMPLE
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-03-13: TCPScanSecurity: http://jenkins.shibgeek.com:48084/job/TCPScanSecurity/: 2h
   CLOSED: [2017-03-12 Sun 20:39]
*** 2017-03-14: #32: Improve jetty image: install bash and use alpine version: 1h
  CLOSED: [2017-03-14 Tue 17:58]
https://bitbucket.org/nubesecure/penroz/issues/133/improve-jetty-image-install-bash-and-use
https://bitbucket.org/nubesecure/brozton/issues/32/improve-jetty-image-install-bash-and-use
*** 2017-03-14: #26: When start docker-compose in Dev env, perform memory check for docker machine: 3h
  CLOSED: [2017-03-14 Tue 18:13]
https://bitbucket.org/nubesecure/brozton/issues/26/when-start-docker-compose-in-dev-env

*** 2017-03-14: #30 In docker-compose.yml of brozton, only use .env and remove docker-compose.env: 1h + 2h
CLOSED: [2017-03-14 Tue 18:33]
https://bitbucket.org/nubesecure/brozton/issues/30/in-docker-composeyml-of-brozton-only-use
*** 2017-03-14: #28 Keep launch container alive: 3h
  CLOSED: [2017-03-14 Tue 23:08]
https://bitbucket.org/nubesecure/kumku-u/issues/28/keep-launch-container-alive

*** 2017-03-15: #130 Enforce docker healthcheck for images in penroz repo: 2h + 2h
  CLOSED: [2017-03-15 Wed 07:52]
https://bitbucket.org/nubesecure/penroz/issues/130/enforce-docker-healthcheck-for-images-in
export DB_DRIVER=com.mysql.jdbc.Driver
export DB_ROOT_PASSWORD=secret
export DB_USER=shibgeek
export DB_PASSWORD=password
export DB_NAME=shibgeek

mysql -u$DB_USER -p$DB_PASSWORD -P3306 $DB_NAME -e "select count(1) from iam_permission;"

mysql -u$DB_USER -p$DB_PASSWORD -P3306 $DB_NAME -e "select count(1) from iam_permissions;"
*** 2017-03-15: cosmetic improvement and shellcheck fix for both penroz and brozton repos: 2h
   CLOSED: [2017-03-15 Wed 10:59]
*** 2017-03-15: Demo env: Merge configuration change(.env, docker-compose.yml) from penroz/brozton to devops repo: 2h
   CLOSED: [2017-03-15 Wed 14:51]
*** 2017-03-16: Add bematech-es-16 to ES cluster, in order to help ES OOM issue.
   CLOSED: [2017-03-16 Thu 17:02]

*** 2017-03-17: #1: [DOCKER] - Figure out how to customize build args: shib image refinement: 5h
  CLOSED: [2017-03-17 Fri 17:39]
https://bitbucket.org/nubesecure/brozton/pull-requests/12/docker-figure-out-how-to-customize-build/diff

/ # cat  ./shibboleth-identity-provider-3.3.0/bin/install.sh
#! /bin/bash

declare LOCATION

LOCATION=$0
LOCATION=${LOCATION%/*}

$LOCATION/ant.sh "$@" install

> docker-compose.yml && vim docker-compose.yml
**** docker-compose.yml
version: '2'
services:
  shib:
    container_name: shib
    build:
      context: ./idp
      dockerfile: Dockerfile-shib
    volumes:
    - idp_home:${IDP_HOME}
    environment:
      IDP_HOME: ${IDP_HOME}
      HOST_NAME: ${HOST_NAME}
      IDP_URL_PROTOCOL: ${IDP_URL_PROTOCOL}
      IDP_SCOPE: ${IDP_SCOPE}
      IDP_KEYSTORE_PASSWORD: changme
      IDP_SEALER_PASSWORD: changme
      IDP_SEALER_ALIAS: secret
      TOTP_URL: http://totp/token
      DB_URL: jdbc:mysql://db/${DB_NAME}
      DB_USERNAME: ${DB_USER}
      DB_PASSWORD: ${DB_PASSWORD}
volumes:
  idp_home:
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-03-18: (0h) #38 Dockerfiles minor improvements: add expose instructions; shorten healthcheck to 2 mins: 2h
  CLOSED: [2017-03-18 Sat 14:16]
https://bitbucket.org/nubesecure/brozton/pull-requests/19/dockerfiles-minor-improvements-add-expose/diff
*** 2017-03-18: (0h) #37 In Docker entrypoint, add precheck logic before starting services: make errors more apparent: 2h
   CLOSED: [2017-03-18 Sat 15:02]
*** 2017-03-20: Keep config in sync: penroz -> brozton: 2h
   CLOSED: [2017-03-20 Mon 09:51]
*** 2017-03-20: iamsvc: wait for DB to be up: 1h
   CLOSED: [2017-03-20 Mon 11:52]
*** 2017-03-20: #41 [dofacdenny] - Please manually reduce size of dofacdenny repo under 1GB: 1h
  CLOSED: [2017-03-20 Mon 12:16]
https://bitbucket.org/nubesecure/devops/issues/41/dofacdenny-please-manually-reduce-size-of
*** #  --8<-------------------------- separator ------------------------>8--
*** [#A] 2017-03-18: enable audit ES feature for prod env, and fix the upcoming issues: SSL certificate, nagios, DNS entries
   CLOSED: [2017-03-18 Sat 11:37]
*** 2017-03-19: DNS issue about carol.ai: http://prodjenkins.carol.ai:18080/job/CommonServerCheckProd/: dns setting
  CLOSED: [2017-03-19 Sun 14:58]
jenkins@prod-nagios-jenkins-1:~$ echo | openssl s_client -connect app.carol.ai:443 2>/dev/null | openssl x509 -noout -dates
notBefore=Feb 28 00:00:00 2017 GMT
notAfter=Mar  5 12:00:00 2018 GMT
jenkins@prod-nagios-jenkins-1:~$ echo | openssl s_client -connect carol.ai:443 2>/dev/null | openssl x509 -noout -dates
unable to load certificate
140381957670560:error:0906D06C:PEM routines:PEM_read_bio:no start line:pem_lib.c:703:Expecting: TRUSTED CERTIFICATE

*** 2017-03-19: DNS issue: no senar.carol.ai, but senarn.carol.ai
  CLOSED: [2017-03-19 Sun 14:58]
Denny Zhang [9:25 PM] 
@robson.poffo

Today is a bit late. Let me enable the audit feature tomorrow morning?

We will need to:
1. [Denny] Start VMs for audit ES services from DO GUI
2. [Denny] Provision the audit ES cluster from Jenkins
3. [Bruno?] Update mdm nodes from Jenkins.

Robson Poffo [9:26 PM] 
Ok @denny.zhang
*** #  --8<-------------------------- separator ------------------------>8--

*** 2017-03-21: #1813 (1) Nagios improvement: bematech-cb-4 fail suddenly, port 8091 not listening but process is running
  CLOSED: [2017-03-21 Tue 09:47]
https://trello.com/c/P4cJeaBX
*** 2017-03-21: build docker images from mdmdevops repo instead of mdmpublic repo
  CLOSED: [2017-03-21 Tue 20:22]
> /root/code_dir/mdmdevops-totvslabs/docker/Dockerfile/build_image.py && vim /root/code_dir/mdmdevops-totvslabs/docker/Dockerfile/build_image.py

*** 2017-03-22: Support Daniel S3 issue
  CLOSED: [2017-03-22 Wed 18:20]
aws s3 ls s3://totvs-supermarket --profile supermarket
*** 2017-03-23: enable audit feature in Bematech env
   CLOSED: [2017-03-23 Thu 10:03]
*** 2017-03-23: Use docker-compose.yml to setup all servers
  CLOSED: [2017-03-23 Thu 10:27]
> docker-compose.yml && vim docker-compose.yml
*** TODO update /etc/hosts to bypass the test
*** 2017-03-24: (1) Redirect *.fluigdata.com requests to *.carol.ai with no complains of ssl certificate: http://injenkins.carol.ai:48080/view/Basic/job/DockerDeployFeatureCookbooks/419/console
  CLOSED: [2017-03-24 Fri 11:32]
https://trello.com/c/4XeEciOe
cd /var/lib/jenkins/code/DockerDeployFeatureCookbooks/1.61-ssl-redirection/mdmdevops-totvslabs/cookbooks/mdm-cluster
export INSTANCE_NAME=mdm-cluster-DockerDeployFeatureCookbooks-419
export KITCHEN_YAML=.kitchen.yml
export branch_name=1.61-ssl-redirection
export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true

export TEST_KITCHEN_YAML=".kitchen.yml"
# export DOCKER_PORT_FORWARD_PREFIX=31
export PACKAGE_URL='http://172.17.0.1:18000'
export APP_BRANCH_NAME=1.61
export FRAMEWORK_BRANCH_NAME=1.61
export CLUSTER_ID=kitchen-mdm-feature

#+BEGIN_EXAMPLE
curl -I https://app.carol.ai/mdm-ui
curl -I https://app.fluigdata.com/mdm-ui

Hack /etc/hosts:

127.0.0.1 www.fluigdata.com
127.0.0.1 admin.fluigdata.com
127.0.0.1 app.fluigdata.com
127.0.0.1 totvslabs.fluigdata.com
#+END_EXAMPLE

*** 2017-03-24: Add one more CB node and one more ES node, to be prepared for the expected 30 new million records
   CLOSED: [2017-03-24 Fri 14:58]

*** 2017-03-28: doc: procedure for the reshard
  CLOSED: [2017-03-28 Tue 11:13]
- only close, no backup
*** 2017-03-28: [#A] re-index DO ES indice which have only 1 shard
   CLOSED: [2017-03-28 Tue 11:05]
**** HALF staging-index-799e458055c611e6bb000401f8d88101
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
es_port=9200

curl $es_ip:9200/_cat/shards?v | grep "staging-index-799e458055c611e6bb000401f8d88101"

export old_es_index_name="staging-index-799e458055c611e6bb000401f8d88101"
export shard_count="5"
export es_alias_name="staging-799e458055c611e6bb000401f8d88101"

bash /tmp/es_reindex.sh "$old_es_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.sh

# List old Es indices
time curl -XGET "http://${es_ip}:${es_port}/_cat/indices?v" | grep "$old_es_index_name"
**** HALF master-index-860245c0841e11e6a8260401f8d88101
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
es_port=9200

curl $es_ip:9200/_cat/shards?v | grep "master-index-860245c0841e11e6a8260401f8d88101"

export old_es_index_name="master-index-860245c0841e11e6a8260401f8d88101"
export shard_count="5"
export es_alias_name="master-799e458055c611e6bb000401f8d88101"

bash /tmp/es_reindex.sh "$old_es_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.sh

# List old Es indices
time curl -XGET "http://${es_ip}:${es_port}/_cat/indices?v" | grep "$old_es_index_name"
**** #  --8<-------------------------- separator ------------------------>8--
**** HALF master-index-799e458055c611e6bb000401f8d88101: 3gb: 56m45.696s
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
es_port=9200

curl $es_ip:9200/_cat/shards?v | grep "master-index-799e458055c611e6bb000401f8d88101"

export old_es_index_name="master-index-799e458055c611e6bb000401f8d88101"
export shard_count="5"
export es_alias_name="master-799e458055c611e6bb000401f8d88101"

bash /tmp/es_reindex.sh "$old_es_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.sh

# List old Es indices
time curl -XGET "http://${es_ip}:${es_port}/_cat/indices?v" | grep "$old_es_index_name"
**** HALF staging-index-3366bf206f2a11e694f80401f8d88c01: 4.1mb
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
es_port=9200

curl $es_ip:9200/_cat/shards?v | grep "staging-index-3366bf206f2a11e694f80401f8d88c01"

export old_es_index_name="staging-index-3366bf206f2a11e694f80401f8d88c01"
export shard_count="5"
export es_alias_name="staging-3366bf206f2a11e694f80401f8d88c01"

bash /tmp/es_reindex.sh "$old_es_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.sh

# List old Es indices
time curl -XGET "http://${es_ip}:${es_port}/_cat/indices?v" | grep "$old_es_index_name"
**** HALF staging-index-860245c0841e11e6a8260401f8d88101: 109kb: 1s
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
es_port=9200

curl $es_ip:9200/_cat/shards?v | grep "staging-index-860245c0841e11e6a8260401f8d88101"

export old_es_index_name="staging-index-860245c0841e11e6a8260401f8d88101"
export shard_count="5"
export es_alias_name="staging-860245c0841e11e6a8260401f8d88101"

bash /tmp/es_reindex.sh "$old_es_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.sh

# List old Es indices
time curl -XGET "http://${es_ip}:${es_port}/_cat/indices?v" | grep "$old_es_index_name"
**** HALF master-index-3fa847206b9911e6b61d0401f8d88101: 33.4kb: 1s
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
es_port=9200

curl $es_ip:9200/_cat/shards?v | grep "master-index-3fa847206b9911e6b61d0401f8d88101"

export old_es_index_name="master-index-3fa847206b9911e6b61d0401f8d88101"
export shard_count="5"
export es_alias_name="master-3fa847206b9911e6b61d0401f8d88101"

bash /tmp/es_reindex.sh "$old_es_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.sh

# List old Es indices
time curl -XGET "http://${es_ip}:${es_port}/_cat/indices?v" | grep "$old_es_index_name"
**** HALF staging-index-3fa847206b9911e6b61d0401f8d88101: 18kb: 1s
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
es_port=9200

curl $es_ip:9200/_cat/shards?v | grep "staging-index-3fa847206b9911e6b61d0401f8d88101"

export old_es_index_name="staging-index-3fa847206b9911e6b61d0401f8d88101"
export shard_count="5"
export es_alias_name="staging-3fa847206b9911e6b61d0401f8d88101"

bash /tmp/es_reindex.sh "$old_es_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.sh

# List old Es indices
time curl -XGET "http://${es_ip}:${es_port}/_cat/indices?v" | grep "$old_es_index_name"
**** HALF master-index-3366bf206f2a11e694f80401f8d88c01: 372mb
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
es_port=9200

curl $es_ip:9200/_cat/shards?v | grep "master-index-3366bf206f2a11e694f80401f8d88c01"

export old_es_index_name="master-index-3366bf206f2a11e694f80401f8d88c01"
export shard_count="5"
export es_alias_name="master-3366bf206f2a11e694f80401f8d88c01"

bash /tmp/es_reindex.sh "$old_es_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.sh

# List old Es indices
time curl -XGET "http://${es_ip}:${es_port}/_cat/indices?v" | grep "$old_es_index_name"

*** 2017-03-28: add 4 nodes to Azure ES cluster: bematech-es-18, 19, 20, 21
   CLOSED: [2017-03-28 Tue 18:22]
*** 2017-03-29: [#A] Finished the maintainence in DO: make sure all indices have more than 1 shards. make sure no giant shards
   CLOSED: [2017-03-29 Wed 08:20]
*** 2017-03-29: Add prod-cb-06 to Prod DO for the frequent CB memory warning
   CLOSED: [2017-03-29 Wed 11:35]

*** 2017-03-29: [#A] Make sure all Azure ES indices has more than 1 shards
  CLOSED: [2017-03-29 Wed 15:28]
https://github.com/TOTVS/mdmdevops/wiki/Critical-Maintenance-History-Of-Prod-Env#mar-2017-make-sure-all-azure-es-indices-has-more-than-1-shards
**** 2017-03-29: master-index-839920f07e6b11e6b71d0401f8d88101: 120 GB, 3.3 hours
   CLOSED: [2017-03-28 Tue 23:12]
wget -O /tmp/es_reindex.sh \
  https://raw.githubusercontent.com/TOTVS/mdmpublic/master/bash/es_reindex.sh

export es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
export es_port=9200
export old_es_index_name="master-index-839920f07e6b11e6b71d0401f8d88101"
export shard_count="20"
export es_alias_name="master-839920f07e6b11e6b71d0401f8d88101"

bash /tmp/es_reindex.sh "$old_es_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.log
**** 2017-03-29: close index master-index-839920f07e6b11e6b71d0401f8d88101
    CLOSED: [2017-03-28 Tue 23:12]
**** 2017-03-29: staging-index-839920f07e6b11e6b71d0401f8d88101: 220GB, 2hrs36min
   CLOSED: [2017-03-29 Wed 15:26]
wget -O /tmp/es_reindex.sh \
  https://raw.githubusercontent.com/TOTVS/mdmpublic/master/bash/es_reindex.sh

export es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
export es_port=9200
export old_index_name="staging-index-839920f07e6b11e6b71d0401f8d88101"
export alias_index_name="staging-839920f07e6b11e6b71d0401f8d88101"
export shard_count="20"
export replica_count="5"
export log_file="/var/log/es_reindex.log"

curl $es_ip:9200/_cat/indices?v | grep $old_index_name
bash /tmp/es_reindex.sh "$old_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.log
**** 2017-03-29: create alias
    CLOSED: [2017-03-29 Wed 15:27]
**** 2017-03-29: close index staging-index-839920f07e6b11e6b71d0401f8d88101
    CLOSED: [2017-03-29 Wed 15:27]
*** 2017-03-29: make repo as private: documentation(confirm the git pull)
   CLOSED: [2017-03-29 Wed 18:47]
*** 2017-03-30: Add bematech-es-01(191.232.177.103) to ES cluster
   CLOSED: [2017-03-30 Thu 17:39]
*** CANCELED [#A] Add more ES nodes to the cluster
  CLOSED: [2017-03-30 Thu 23:33]
| bematech-es-04    |  191.232.178.8 |  10.1.1.54 | Type4          | Elasticsearch   |
| bematech-es-05    |  191.232.179.242 |  10.1.1.55 | Type4          | Elasticsearch   |

*** 2017-03-31: close index in Azure
  CLOSED: [2017-03-31 Fri 19:17]
curl -XGET "http://${es_ip}:9200/_aliases?pretty" | grep -C 5 master-839920f07e6b11e6b71d0401f8d88101

curl -XPOST "http://$es_ip:9200/master-index-839920f07e6b11e6b71d0401f8d88101-new/_close"

green  open  master-index-839920f07e6b11e6b71d0401f8d88101-new2  20 2   21403741    439420  91.7gb  30.6gb
green  open  master-index-839920f07e6b11e6b71d0401f8d88101-new   20 2    5529729      2073  53.2gb  17.7gb
*** #  --8<-------------------------- separator ------------------------>8--
*** CANCELED Finish migration in DO
  CLOSED: [2017-03-31 Fri 16:19]
curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep close

staging-index-3fa847206b9911e6b61d0401f8d88101: run the procedure twice
***** 2017-03-31: staging-index-3fa847206b9911e6b61d0401f8d88101: 27.4kb: 5min
    CLOSED: [2017-03-31 Fri 12:02]
***** 2017-03-31: staging-index-799e458055c611e6bb000401f8d88101: 50.2mb: 0m56.621s
   CLOSED: [2017-03-31 Fri 12:08]
export shard_count=5
export old_index_name="staging-index-799e458055c611e6bb000401f8d88101"
export intermediate_index_name="staging-index-799e458055c611e6bb000401f8d88101-new"
export new_index_name="staging-index-799e458055c611e6bb000401f8d88101-new2"
export alias_index_name="staging-799e458055c611e6bb000401f8d88101"

#+BEGIN_EXAMPLE
green  open   staging-index-799e458055c611e6bb000401f8d88101-new2   5   2     102475            0    154.2mb         51.5mb
green  open   master-index-799e458055c611e6bb000401f8d88101-new     5   2    2208887            0      7.2gb          2.4gb
#+END_EXAMPLE
***** 2017-03-31: staging-index-860245c0841e11e6a8260401f8d88101: 129.5kb: 1min
   CLOSED: [2017-03-31 Fri 12:10]
export shard_count=5
export old_index_name="staging-index-860245c0841e11e6a8260401f8d88101"
export intermediate_index_name="staging-index-860245c0841e11e6a8260401f8d88101-new"
export new_index_name="staging-index-860245c0841e11e6a8260401f8d88101-new2"
export alias_index_name="staging-860245c0841e11e6a8260401f8d88101"

***** 2017-03-31: staging-index-3366bf206f2a11e694f80401f8d88c01: 2.5mb: 1min
   CLOSED: [2017-03-31 Fri 12:15]
export shard_count=5
export old_index_name="staging-index-3366bf206f2a11e694f80401f8d88c01"
export intermediate_index_name="staging-index-3366bf206f2a11e694f80401f8d88c01-new"
export new_index_name="staging-index-3366bf206f2a11e694f80401f8d88c01-new2"
export alias_index_name="staging-3366bf206f2a11e694f80401f8d88c01"
**** #  --8<-------------------------- separator ------------------------>8--
***** 2017-03-31: master-index-3fa847206b9911e6b61d0401f8d88101: 28.9kb: 1min
   CLOSED: [2017-03-31 Fri 12:19]
export shard_count=5
export old_index_name="master-index-3fa847206b9911e6b61d0401f8d88101"
export intermediate_index_name="master-index-3fa847206b9911e6b61d0401f8d88101-new"
export new_index_name="master-index-3fa847206b9911e6b61d0401f8d88101-new2"
export alias_index_name="master-3fa847206b9911e6b61d0401f8d88101"

***** 2017-03-31: master-index-56b33120ef1311e6a81da2f42be00f79: 2.8mb: 1min: create index takes (0m58.447s)
   CLOSED: [2017-03-31 Fri 12:22]
export shard_count=5
export old_index_name="master-index-56b33120ef1311e6a81da2f42be00f79"
export intermediate_index_name="master-index-56b33120ef1311e6a81da2f42be00f79-new"
export new_index_name="master-index-56b33120ef1311e6a81da2f42be00f79-new2"
export alias_index_name="master-56b33120ef1311e6a81da2f42be00f79"
**** #  --8<-------------------------- separator ------------------------>8--
***** 2017-03-31: master-index-860245c0841e11e6a8260401f8d88101: 177.5mb: reindex(1m45.168s)
   CLOSED: [2017-03-31 Fri 12:32]
export shard_count=5
export old_index_name="master-index-860245c0841e11e6a8260401f8d88101"
export intermediate_index_name="master-index-860245c0841e11e6a8260401f8d88101-new"
export new_index_name="master-index-860245c0841e11e6a8260401f8d88101-new2"
export alias_index_name="master-860245c0841e11e6a8260401f8d88101"
***** 2017-03-31: master-index-3366bf206f2a11e694f80401f8d88c01: 202.5mb: reindex(4m2.876s)
   CLOSED: [2017-03-31 Fri 12:39]
export shard_count=5
export old_index_name="master-index-3366bf206f2a11e694f80401f8d88c01"
export intermediate_index_name="master-index-3366bf206f2a11e694f80401f8d88c01-new"
export new_index_name="master-index-3366bf206f2a11e694f80401f8d88c01-new2"
export alias_index_name="master-3366bf206f2a11e694f80401f8d88c01"
**** #  --8<-------------------------- separator ------------------------>8--
***** 2017-03-31: master-index-799e458055c611e6bb000401f8d88101: 2.4gb: 32m1.935s
   CLOSED: [2017-03-31 Fri 13:23]
export shard_count=5
export old_index_name="master-index-799e458055c611e6bb000401f8d88101"
export intermediate_index_name="master-index-799e458055c611e6bb000401f8d88101-new"
export new_index_name="master-index-799e458055c611e6bb000401f8d88101-new2"
export alias_index_name="master-799e458055c611e6bb000401f8d88101"
**** TODO staging-index-8cd6e43115e9416eb23609486fa053e3: 59gb: 
export shard_count=20
export old_index_name="staging-index-8cd6e43115e9416eb23609486fa053e3"
export intermediate_index_name="staging-index-8cd6e43115e9416eb23609486fa053e3-new"
export new_index_name="staging-index-8cd6e43115e9416eb23609486fa053e3-new2"
export alias_index_name="staging-8cd6e43115e9416eb23609486fa053e3"
**** #  --8<-------------------------- separator ------------------------>8--
**** CANCELED staging-index-799e458055c611e6bb000401f8d88101: try to use new one, but it has failed
   CLOSED: [2017-03-31 Fri 10:33]
curl -XPOST "http://$es_ip:9200/staging-index-799e458055c611e6bb000401f8d88101-new/_open"
curl -XPOST "http://$es_ip:9200/staging-index-799e458055c611e6bb000401f8d88101/_close"
time curl -XPOST "http://${es_ip}:9200/_aliases" -d "
{
    \"actions\": [
    { \"remove\": {
    \"alias\": \"staging-799e458055c611e6bb000401f8d88101\",
    \"index\": \"staging-index-799e458055c611e6bb000401f8d88101\"
    }},
    { \"add\": {
    \"alias\": \"staging-799e458055c611e6bb000401f8d88101\",
    \"index\": \"staging-index-799e458055c611e6bb000401f8d88101-new\"
    }}
    ]
}" | tee -a "/var/log/es_reindex_manual.log"

curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep 799e458055c611e6bb000401f8d88101
curl -XGET "http://${es_ip}:9200/_aliases?pretty" | grep -C 5 799e458055c611e6bb000401f8d88101
** 2017-04
*** 2017-04-01: Force merge in DO: 13.65 hours
  CLOSED: [2017-04-01 Sat 18:13]
**** 2017-04-01: staging-index-96a19880fa7111e6bdb0a2f42be00f79: 164.4mb (0.093s)
   CLOSED: [2017-03-31 Fri 17:16]
export index_name="staging-index-96a19880fa7111e6bdb0a2f42be00f79"
#+BEGIN_EXAMPLE
root@prod-es-01:~# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  7844  100  7844    0     0   7110      0  0:00:01  0:00:01 --:--:--  7111
green open  staging-index-96a19880fa7111e6bdb0a2f42be00f79       5 2     63169    18884  510.3mb 164.4mb
root@prod-es-01:~#
root@prod-es-01:~# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge"
{"_shards":{"total":15,"successful":15,"failed":0}}
real    0m0.093s
user    0m0.004s
sys     0m0.004s
root@prod-es-01:~#
root@prod-es-01:~# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  7844  100  7844    0     0   8236      0 --:--:-- --:--:-- --:--:--  8230
green open  staging-index-96a19880fa7111e6bdb0a2f42be00f79       5 2     63169    18884  510.3mb 164.4mb
#+END_EXAMPLE
**** 2017-04-01: staging-index-cf5e90403d5e11e68ac00401f8d88501: 539.8mb (2m59.036s)
   CLOSED: [2017-03-31 Fri 17:23]
export index_name="staging-index-cf5e90403d5e11e68ac00401f8d88501"

#+BEGIN_EXAMPLE
root@prod-es-01:~# export index_name="staging-index-cf5e90403d5e11e68ac00401f8d88501"
root@prod-es-01:~# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  7844  100  7844    0     0   9585      0 --:--:-- --:--:-- --:--:--  9577
green open  staging-index-cf5e90403d5e11e68ac00401f8d88501       5 2    168810    82147    1.5gb 539.8mb
root@prod-es-01:~#
root@prod-es-01:~# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"


{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    2m59.036s
user    0m0.020s
sys     0m0.000s
root@prod-es-01:~#
root@prod-es-01:~#
root@prod-es-01:~# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  7844  100  7844    0     0  10123      0 --:--:-- --:--:-- --:--:-- 10121
green open  staging-index-cf5e90403d5e11e68ac00401f8d88501       5 2    168810      318    1.1gb 405.7mb
#+END_EXAMPLE
*** #  --8<-------------------------- separator ------------------------>8--
**** 2017-04-01: staging-index-56b33120ef1311e6a81da2f42be00f79: 93.9gb (819m32.165s)
   CLOSED: [2017-04-01 Sat 08:57]
export index_name="staging-index-56b33120ef1311e6a81da2f42be00f79"
#+BEGIN_EXAMPLE
root@prod-es-01:~# export index_name="staging-index-56b33120ef1311e6a81da2f42be00f79"
root@prod-es-01:~# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  7844  100  7844    0     0  10033      0 --:--:-- --:--:-- --:--:-- 10030
      close staging-index-56b33120ef1311e6a81da2f42be00f79-new
green open  staging-index-56b33120ef1311e6a81da2f42be00f79       5 2  60563213 32263761  284.8gb  93.9gb
root@prod-es-01:~#
root@prod-es-01:~#
root@prod-es-01:~# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"

{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    819m32.165s
user    0m3.712s
sys     0m1.068s
You have new mail in /var/mail/root
root@prod-es-01:~#
root@prod-es-01:~# # check index status
root@prod-es-01:~# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  7844  100  7844    0     0  10776      0 --:--:-- --:--:-- --:--:-- 10774
      close staging-index-56b33120ef1311e6a81da2f42be00f79-new
green open  staging-index-56b33120ef1311e6a81da2f42be00f79       5 2  60563213   230053  233.5gb  76.7gb
#+END_EXAMPLE
**** 2017-04-01: staging-index-8a18aa800e5911e785f24a8136534b63: 95.2gb (496m20.504s)
   CLOSED: [2017-04-01 Sat 18:13]
export index_name="staging-index-8a18aa800e5911e785f24a8136534b63"
#+BEGIN_EXAMPLE
root@prod-es-01:~# export index_name="staging-index-8a18aa800e5911e785f24a8136534b63"
root@prod-es-01:~# # check index status
root@prod-es-01:~# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  7844  100  7844    0     0   8273      0 --:--:-- --:--:-- --:--:--  8274
green open  staging-index-8a18aa800e5911e785f24a8136534b63       5 2 101120550 11836466  286.2gb    96gb
root@prod-es-01:~#
root@prod-es-01:~# # only delete those indices that has deletes in it
root@prod-es-01:~# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"


{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    496m20.504s
user    0m2.400s
sys     0m0.624s
You have new mail in /var/mail/root
root@prod-es-01:~#
root@prod-es-01:~# # check index status
root@prod-es-01:~# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  7770  100  7770    0     0   8012      0 --:--:-- --:--:-- --:--:-green open  staging-index-8a18aa800e5911e785f24a8136534b63       5 2 101005873 2012627  276.2gb  97.7gb
-  8010
#+END_EXAMPLE
*** 2017-04-01: DO repiar: watch staging-index-8cd6e43115e9416eb23609486fa053e3-new2
  CLOSED: [2017-03-31 Fri 22:59]
                                 Dload  Upload   Total   Spent    Left  Speed
100   466    0   210    0   256      0      0 --:--:--  7:46:15 --:--:--     0
{
  "took" : 27975011,
  "timed_out" : false,
  "total" : 28650377,
  "updated" : 0,
  "created" : 28650377,
  "batches" : 57301,
  "version_conflicts" : 0,
  "noops" : 0,
  "retries" : 0,
  "failures" : [ ]
}

real    466m15.023s
user    0m2.692s
sys     0m2.764s

root@prod-es-01:/tmp# curl "$es_ip:9200/_cat/indices?v" | grep "$(echo $alias_index_name | sed "s/.*-//g")"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  9375  100  9375    0     0  14065      0 --:--:-- --:--:-- --:--:-- 14055
       close  staging-index-8cd6e43115e9416eb23609486fa053e3
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3-new2  20   2   85762478            0    229.2gb         79.1gb
green  open   master-index-8cd6e43115e9416eb23609486fa053e3         5   2   43316715        65279       93gb         30.4gb
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3-new   20   2   28650079       397653    177.9gb         59.2gb
root@prod-es-01:/tmp#
root@prod-es-01:/tmp# # Update alias
root@prod-es-01:/tmp# if tail -n 5 /var/log/es_index_manual.log | grep "\"failures\" : \[ \]"; then
>    time curl -XPOST "http://${es_ip}:${es_port}/_aliases" -d "
>    {
>        \"actions\": [
>        { \"remove\": {
>        \"alias\": \"${alias_index_name}\",
>        \"index\": \"${intermediate_index_name}\"
>        }},
>        { \"add\": {
>        \"alias\": \"${alias_index_name}\",
>        \"index\": \"${new_index_name}\"
>        }}
>        ]
>    }"
> fi
  "failures" : [ ]
{"acknowledged":true}
real    0m18.384s
user    0m0.004s
sys     0m0.000s
**** TODO Remember: DO reindex: use staging-index-56b33120ef1311e6a81da2f42be00f79-new2, and from staging-index-56b33120ef1311e6a81da2f42be00f79
**** CANCELED check ES re-indexing status in DO: giant indices: staging-index-56b33120ef1311e6a81da2f42be00f79
   CLOSED: [2017-03-31 Fri 17:05]
**** re-index fail
real    1881m29.846s
user    0m16.364s
sys     0m16.696s
{
  "took" : 112889822,
  "timed_out" : false,
  "total" : 52700923,
  "updated" : 0,
  "created" : 38152699,
  "batches" : 381527,
  "version_conflicts" : 0,
  "noops" : 0,
  "retries" : 0,
  "failures" : [ {
    "index" : "staging-index-56b33120ef1311e6a81da2f42be00f79-new",
    "type" : "mdmdisciplineregistrationRejected",
    "id" : "00a8ab80f86811e6912826b7a582476b",
    "cause" : {
      "type" : "mapper_parsing_exception",
      "reason" : "failed to parse [mdmMasterFieldAndValues.registrationdate]",
      "caused_by" : {
        "type" : "illegal_argument_exception",
        "reason" : "Invalid format: \"\""
      }
    },
    "status" : 400
  } ]
}
**** status after re-index
Every 2.0s: curl 138.68.250.138:9200/_cat/indices?v | grep staging-index-56b33120ef1311e6a81da2f42be00f79            Fri Mar 31 15:22:22 2017

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0 100  8060  100  8060    0     0   8763      0 --:--:-- --:--:
-- --:--:--  8760 100  8060  100  8060    0     0   8763      0 --:--:-- --:--:-- --:--:--  8760
green  open   staging-index-56b33120ef1311e6a81da2f42be00f79-new  20   2   38152699            0     84.6gb         28.3gb
green  open   staging-index-56b33120ef1311e6a81da2f42be00f79       5   2   60563213     32263761    284.8gb         93.9gb
***** 2017-04-02: update alias
   CLOSED: [2017-03-31 Fri 10:23]
time curl -XPOST "http://${es_ip}:9200/_aliases" -d "
{
    \"actions\": [
    { \"remove\": {
    \"alias\": \"staging-56b33120ef1311e6a81da2f42be00f79\",
    \"index\": \"staging-index-56b33120ef1311e6a81da2f42be00f79-new\"
    }},
    { \"add\": {
    \"alias\": \"staging-56b33120ef1311e6a81da2f42be00f79\",
    \"index\": \"staging-index-56b33120ef1311e6a81da2f42be00f79\"
    }}
    ]
}" | tee -a "/var/log/es_reindex_manual.log"

curl -XPOST "http://$es_ip:9200/staging-index-56b33120ef1311e6a81da2f42be00f79-new/_close"
*** 2017-04-01: dofacdenny iamge build bug fixing and bypass 2GB bitbucket limitation: 2h
  CLOSED: [2017-04-01 Sat 23:22]
*** 2017-04-02: [#A] #39 Next gen CI for brozton: 7h
  CLOSED: [2017-04-02 Sun 15:04]
https://bitbucket.org/nubesecure/brozton/issues/39/next-gen-ci-for-brozton

*** 2017-04-02: #39 Next gen CI for brozton: 1h
  CLOSED: [2017-04-02 Sun 18:01]
*** 2017-04-03: enforce healthcheck for IoT repo: 3h
  CLOSED: [2017-04-03 Mon 12:53]
*** 2017-04-02: docker-compose external network: external_links: 1h
  CLOSED: [2017-04-02 Sun 11:39]
https://docs.docker.com/compose/compose-file/compose-file-v2/#externallinks
*** 2017-04-03: Next gen CI for brozton: 1h
  CLOSED: [2017-04-03 Mon 11:57]
*** 2017-04-03: create auto build for all major repos: penroz, launch, iot: 1h
  CLOSED: [2017-04-03 Mon 15:45]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-04-02 Force merge in Azure:
  CLOSED: [2017-04-02 Sun 10:18]
bematech-es-21 

ssh -p 2702 root@191.232.247.40
**** DONE staging-index-8cd6e43115e9416eb23609486fa053e3: 53gb (30m42.853s)
   CLOSED: [2017-03-31 Fri 18:14]
export index_name="staging-index-8cd6e43115e9416eb23609486fa053e3"

#+BEGIN_EXAMPLE
root@bematech-es-1:/tmp# export index_name="staging-index-8cd6e43115e9416eb23609486fa053e3"
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  2916  100  2916    0     0   7866      0 --:--:-- --:--:-- --:--:--  7881
green  open  staging-index-8cd6e43115e9416eb23609486fa053e3       5 2   15113050  44001407 166.7gb    53gb
       close staging-index-8cd6e43115e9416eb23609486fa053e3-tmp
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"

{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    30m42.853s
user    0m0.040s
sys     0m0.012s
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2916  100  2916    0     0   7787      0 --:--:-- --:--:-- --:--:--  7796
green  open  staging-index-8cd6e43115e9416eb23609486fa053e3       5 2   15113050       428  66.4gb  22.1gb
       close staging-index-8cd6e43115e9416eb23609486fa053e3-tmp
root@bematech-es-1:/tmp#
#+END_EXAMPLE
**** #  --8<-------------------------- separator ------------------------>8--
**** DONE staging-index-abae8b30ac9b11e692000401f8d88101: 92.3gb (61m59.982s)
   CLOSED: [2017-03-31 Fri 19:28]
export index_name="staging-index-abae8b30ac9b11e692000401f8d88101"

Force-merged one index: 91.4gb -> 72.5gb (61m59.982s). Just save 20% capacity
#+BEGIN_EXAMPLE
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"                                          [20/735]
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  2916  100  2916    0     0   7699      0 --:--:-- --:--:-- --:--:--  7693
green  open  staging-index-abae8b30ac9b11e692000401f8d88101       5 2   85524478 100411798 275.5gb  91.4gb
root@bematech-es-1:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"


{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}
real    61m59.982s
user    0m0.076s
sys     0m0.024s
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0   7074      0 --:--:-- --:--:-- --:--:--  7080
green open  staging-index-abae8b30ac9b11e692000401f8d88101       5 2   83989321   2250350 198.3gb  72.5gb
root@bematech-es-1:/tmp#
#+END_EXAMPLE
**** DONE master-index-e4010da4110ba377d100f050cb4440db: 229.8mb (0m9.961s)
   CLOSED: [2017-03-31 Fri 19:30]
export index_name="master-index-e4010da4110ba377d100f050cb4440db"
#+BEGIN_EXAMPLE
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0  14175      0 --:--:-- --:--:-- --:--:-- 14231
green open  master-index-e4010da4110ba377d100f050cb4440db        5 2     156684     55758 689.4mb 229.8mb
root@bematech-es-1:/tmp# # only delete those indices that has deletes in it
root@bematech-es-1:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"
{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    0m9.961s
user    0m0.000s
sys     0m0.004s
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # check index status
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0  12645      0 --:--:-- --:--:-- --:--:-- 12615
green open  master-index-e4010da4110ba377d100f050cb4440db        5 2     156684       438 499.7mb 166.5mb
root@bematech-es-1:/tmp#
#+END_EXAMPLE
**** DONE staging-index-e4010da4110ba377d100f050cb4440db: 125.7mb (0m1.539s)
   CLOSED: [2017-03-31 Fri 19:30]
export index_name="staging-index-e4010da4110ba377d100f050cb4440db"
#+BEGIN_EXAMPLE
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  2889  100  2889    0     0  12498      0 --:--:-- --:--:-- --:--:-- 12452
green open  staging-index-e4010da4110ba377d100f050cb4440db       5 2      33737      4082 377.3mb 125.7mb
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # only delete those indices that has deletes in it
root@bematech-es-1:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"
{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    0m1.539s
user    0m0.004s
sys     0m0.000s
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # check index status
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0   9999      0 --:--:-- --:--:-- --:--:--  9996
green open  staging-index-e4010da4110ba377d100f050cb4440db       5 2      33737       220   375mb 130.8mb
#+END_EXAMPLE
**** DONE staging-index-839920f07e6b11e6b71d0401f8d88101-new2: 14.2gb (0m20.556s)
   CLOSED: [2017-03-31 Fri 19:31]
export index_name="staging-index-839920f07e6b11e6b71d0401f8d88101-new2"

#+BEGIN_EXAMPLE
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0   8209      0 --:--:-- --:--:-- --:--:--  8207
green open  staging-index-839920f07e6b11e6b71d0401f8d88101-new2 20 2    4962499     67015  42.7gb  14.2gb
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # only delete those indices that has deletes in it
root@bematech-es-1:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"

{
  "_shards" : {
    "total" : 60,
    "successful" : 60,
    "failed" : 0
  }
}

real    0m20.556s
user    0m0.000s
sys     0m0.004s
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # check index status
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0  16030      0 --:--:-- --:--:-- --:--:-- 16050
green open  staging-index-839920f07e6b11e6b71d0401f8d88101-new2 20 2    4962499     15907  42.6gb  14.2gb
#+END_EXAMPLE
**** #  --8<-------------------------- separator ------------------------>8--
**** DONE staging-index-8cd6e43115e9416eb23609486fa053e3: 22.1gb (0m0.091s)
   CLOSED: [2017-03-31 Fri 19:32]
export index_name="staging-index-8cd6e43115e9416eb23609486fa053e3"
#+BEGIN_EXAMPLE
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0  12350      0 --:--:-- --:--:-- --:--:-- 12399
green open  staging-index-8cd6e43115e9416eb23609486fa053e3       5 2   15113050       428  66.4gb  22.1gb
      close staging-index-8cd6e43115e9416eb23609486fa053e3-tmp
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # only delete those indices that has deletes in it
root@bematech-es-1:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"

{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    0m0.091s
user    0m0.000s
sys     0m0.004s
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # check index status
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0  11127      0 --:--:-- --:--:-- --:--:-- 11154
green open  staging-index-8cd6e43115e9416eb23609486fa053e3       5 2   15113050       428  66.4gb  22.1gb
      close staging-index-8cd6e43115e9416eb23609486fa053e3-tmp
#+END_EXAMPLE
**** DONE master-index-8cd6e43115e9416eb23609486fa053e3: 59.1gb(0m0.849s)
   CLOSED: [2017-03-31 Fri 19:33]
export index_name="master-index-8cd6e43115e9416eb23609486fa053e3"
#+BEGIN_EXAMPLE
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0  15040      0 --:--:-- --:--:-- --:--:-- 15125
green open  master-index-8cd6e43115e9416eb23609486fa053e3        5 2   74178728    559878 177.4gb  59.1gb
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # only delete those indices that has deletes in it
root@bematech-es-1:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"
{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    0m0.849s
user    0m0.004s
sys     0m0.000s
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # check index status
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0  13972      0 --:--:-- --:--:-- --:--:-- 13956
green open  master-index-8cd6e43115e9416eb23609486fa053e3        5 2   74178728    559040 177.4gb  59.1gb
#+END_EXAMPLE
**** DONE master-index-839920f07e6b11e6b71d0401f8d88101-new2: 30.6gb (1m27.294s)
   CLOSED: [2017-03-31 Fri 19:36]
export index_name="master-index-839920f07e6b11e6b71d0401f8d88101-new2"
#+BEGIN_EXAMPLE
root@bematech-es-1:/tmp# export index_name="master-index-839920f07e6b11e6b71d0401f8d88101-new2"
root@bematech-es-1:/tmp# # check index status
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0  12448      0 --:--:-- --:--:-- --:--:-- 12452
green open  master-index-839920f07e6b11e6b71d0401f8d88101-new2  20 2   21423490    432825  91.9gb  30.8gb
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # only delete those indices that has deletes in it
root@bematech-es-1:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"
{
  "_shards" : {
    "total" : 60,
    "successful" : 60,
    "failed" : 0
  }
}

real    1m27.294s
user    0m0.000s
sys     0m0.004s
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # check index status
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0  11243      0 --:--:-- --:--:-- --:--:-- 11285
green open  master-index-839920f07e6b11e6b71d0401f8d88101-new2  20 2   21425541    353120  90.8gb  30.2gb
#+END_EXAMPLE
**** #  --8<-------------------------- separator ------------------------>8--
**** DONE staging-index-abae8b30ac9b11e692000401f8d88101: 80.7gb -> TODO(TOD hours)
   CLOSED: [2017-04-02 Sun 00:05]

Denny Zhang [12:05 AM] 
Force-merge for staging-index-abae8b30ac9b11e692000401f8d88101 is done.

It took 295m36.016s to finish.

Primary storage size has dropped from 81gb to 44.8gb

```root@bematech-es-21:/var/log/elasticsearch# export index_name="staging-index-abae8b30ac9b11e692000401f8d88101"
root@bematech-es-21:/var/log/elasticsearch# # check index status
root@bematech-es-21:/var/log/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0  19902      0 --:--:-- --:--:-- --:--:-- 19924
green open  staging-index-abae8b30ac9b11e692000401f8d88101       5 2   26447167    804387   243gb    81gb
root@bematech-es-21:/var/log/elasticsearch#
root@bematech-es-21:/var/log/elasticsearch# # only delete those indices that has deletes in it
root@bematech-es-21:/var/log/elasticsearch# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"


{
  "_shards" : {
    "total" : 15,
    "successful" : 14,
    "failed" : 0
  }
}

real    295m36.016s
user    0m0.264s
sys     0m0.272s
root@bematech-es-21:/var/log/elasticsearch#
root@bematech-es-21:/var/log/elasticsearch# # check index status
root@bematech-es-21:/var/log/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2889  100  2889    0     0  17758      0 --:--:-- --:--:-- --:--:-- 17833
green open  staging-index-abae8b30ac9b11e692000401f8d88101       5 2   26520955    350286 134.5gb  44.8gb```
**** DONE master-index-abae8b30ac9b11e692000401f8d88101: 1.1tb (TODO hours)
   CLOSED: [2017-04-02 Sun 09:59]
watch "curl $es_ip:9200/_cat/shards?v | grep ' p ' | grep master-index-abae8b30ac9b11e692000401f8d88101; echo ; curl 10.1.1.50:9200/_cat/indices?v | grep master-index-abae8b30ac9b11e692000401f8d88101"

export index_name="master-index-abae8b30ac9b11e692000401f8d88101"

es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

watch "curl $es_ip:9200/_cat/indices?v | grep $index_name"

#+BEGIN_EXAMPLE
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  2889  100  2889    0     0  11740      0 --:--:-- --:--:-- --:--:-- 11791
green open  master-index-abae8b30ac9b11e692000401f8d88101        5 2 1295291710 410760728   3.5tb   1.1tb
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # only delete those indices that has deletes in it
root@bematech-es-1:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"
curl: (52) Empty reply from server

real    1m37.911s
user    0m0.000s
sys     0m0.004s
root@bematech-es-1:/tmp#
root@bematech-es-1:/tmp# # check index status
root@bematech-es-1:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to 10.1.1.12 port 9200: Connection refused

root@bematech-es-21:~# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2862  100  2862    0     0  15995      0 --:--:-- --:--:-- --:--:-- 16078
green open  master-index-abae8b30ac9b11e692000401f8d88101        5 2 1305742820 12360299   2.9tb 937.5gb
You have new mail in /var/mail/root
#+END_EXAMPLE

#+BEGIN_EXAMPLE
Denny Zhang [8:11 AM] 
Also the force-merge has finally finished for master-index-*, after 1554m22.809s.

It has failed with 2 nodes, which have run into OOMs.

```root@bematech-es-21:~# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"
{
  "_shards" : {
    "total" : 15,
    "successful" : 11,
    "failed" : 2,
    "failures" : [ {
      "shard" : 4,
      "index" : "master-index-abae8b30ac9b11e692000401f8d88101",
      "status" : "INTERNAL_SERVER_ERROR",
      "reason" : {
        "type" : "failed_node_exception",
        "reason" : "Failed node [G7Tic_KrRwyqmiq7YaT7CA]",
        "caused_by" : {
          "type" : "node_disconnected_exception",
          "reason" : "[bematech-es-10][10.1.1.28:9300][indices:admin/optimize[n]] disconnected"
        }
      }
    }, {
      "shard" : 2,
      "index" : "master-index-abae8b30ac9b11e692000401f8d88101",
      "status" : "INTERNAL_SERVER_ERROR",
      "reason" : {
        "type" : "failed_node_exception",
        "reason" : "Failed node [qZhbFyX7TRWoZD93BbU7sw]",
        "caused_by" : {
          "type" : "node_disconnected_exception",
          "reason" : "[bematech-es-1][10.1.1.12:9300][indices:admin/optimize[n]] disconnected"
        }
      }
    } ]
  }
}

real    1554m22.809s
user    0m1.468s
sys     0m1.064s
```

[8:11]  
I think it’s fine. Maybe we just re-run the force-merge again.
#+END_EXAMPLE

*** 2017-04-02: DO scale down prod-es-17(138.197.216.23)
  CLOSED: [2017-04-02 Sun 11:38]
*** 2017-04-02: Azure scale down 4 nodes
  CLOSED: [2017-04-02 Sun 11:42]
**** 2017-04-02: Scale down bematech-es-2 and bematech-es-3, since they don't hold shards of giant indices
  CLOSED: [2017-04-02 Sun 09:58]
curl $es_ip:9200/_cat/shards?v | grep "abae8b30ac9b11e692000401f8d88101"

bematech-es-2
bematech-es-3
bematech-es-5

bematech-es-7
bematech-es-8

curl $es_ip:9200/_cat/shards?v | grep "bematech-es-7 "  | grep abae8b30ac9b11e692000401f8d88101
**** existings nodes
#+BEGIN_EXAMPLE
root@bematech-es-21:~# curl $es_ip:9200/_cat/shards?v | grep "bematech-es-2 " 
curl $es_ip:9200/_cat/shards?v | grep "bematech-es-2 " 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
staging-fd1125e03cb711e6878f0401f8d88c01            3     r      STARTED         0    159b 10.1.1.13 bematech-es-2  
100 26master-index-839920f07e6b11e6b71d0401f8d88101-new2  7     p      STARTED   1071686   1.5gb 10.1.1.13 bematech-es-2  
793  10master-index-839920f07e6b11e6b71d0401f8d88101-new2  6     r      STARTED   1071475   1.4gb 10.1.1.13 bematech-es-2  
0 26793    0     0   170k   staging-index-e4010da4110ba377d100f050cb4440db      1     r      STARTED      6758  24.6mb 10.1.1.13 bematech-es-2  
   master-index-e4010da4110ba377d100f050cb4440db       2     r      STARTED     31868  33.8mb 10.1.1.13 bematech-es-2  
0 staging-index-839920f07e6b11e6b71d0401f8d88101-new2 7     r      STARTED    248870 756.9mb 10.1.1.13 bematech-es-2  
--:--:-- staging-index-839920f07e6b11e6b71d0401f8d88101-new2 13    p      STARTED    247647 711.8mb 10.1.1.13 bematech-es-2  
--:master-index-8cd6e43115e9416eb23609486fa053e3       4     p      STARTED  14656168  11.7gb 10.1.1.13 bematech-es-2  
--:-- --:--:--  171k
staging-index-8cd6e43115e9416eb23609486fa053e3      3     r      STARTED   1199080   3.2gb 10.1.1.13 bematech-es-2  
root@bematech-es-21:~# 
#+END_EXAMPLE

#+BEGIN_EXAMPLE
root@bematech-es-21:~# curl $es_ip:9200/_cat/shards?v | grep "bematech-es-3 " 
curl $es_ip:9200/_cat/shards?v | grep "bematech-es-3 " 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 26793  100 26793    0  staging-fd1125e03cb711e6878f0401f8d88c01            3     p      STARTED         0    159b 10.1.1.14 bematech-es-3  
   0 master-index-839920f07e6b11e6b71d0401f8d88101-new2  5     r      STARTED   1069612   1.4gb 10.1.1.14 bematech-es-3  
  179kmaster-index-839920f07e6b11e6b71d0401f8d88101-new2  11    p      STARTED   1067016   1.5gb 10.1.1.14 bematech-es-3  
      master-index-839920f07e6b11e6b71d0401f8d88101-new2  3     r      STARTED   1069093   1.4gb 10.1.1.14 bematech-es-3  
0 --:--:-- --:--:-- --:--staging-index-e4010da4110ba377d100f050cb4440db      3     r      STARTED      6801  21.2mb 10.1.1.14 bematech-es-3  
:--  180k
staging-index-839920f07e6b11e6b71d0401f8d88101-new2 9     r      STARTED    248150 761.2mb 10.1.1.14 bematech-es-3  
staging-index-839920f07e6b11e6b71d0401f8d88101-new2 13    r      STARTED    247647 711.8mb 10.1.1.14 bematech-es-3  
staging-index-839920f07e6b11e6b71d0401f8d88101-new2 0     p      STARTED    248211 725.2mb 10.1.1.14 bematech-es-3  
master-index-8cd6e43115e9416eb23609486fa053e3       2     r      STARTED  14640114  11.7gb 10.1.1.14 bematech-es-3  
root@bematech-es-21:~# 
#+END_EXAMPLE


#+BEGIN_EXAMPLE
root@bematech-es-21:~# curl $es_ip:9200/_cat/shards?v | grep "bematech-es-5 " 
curl $es_ip:9200/_cat/shards?v | grep "bematech-es-5 " 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 26793  100 26793    0     0   139k      0 --:--:-- --:--:-- --:--:--  139k
staging-fd1125e03cb711e6878f0401f8d88c01            0     r      STARTED         0    159b 10.1.1.16 bematech-es-5  
master-index-839920f07e6b11e6b71d0401f8d88101-new2  18    r      STARTED   1071648   1.5gb 10.1.1.16 bematech-es-5  
master-index-839920f07e6b11e6b71d0401f8d88101-new2  14    p      STARTED   1070058   1.4gb 10.1.1.16 bematech-es-5  
staging-index-e4010da4110ba377d100f050cb4440db      1     p      STARTED      6758  24.6mb 10.1.1.16 bematech-es-5  
master-index-e4010da4110ba377d100f050cb4440db       1     p      STARTED     30821  33.2mb 10.1.1.16 bematech-es-5  
staging-index-839920f07e6b11e6b71d0401f8d88101-new2 16    p      STARTED    247591   718mb 10.1.1.16 bematech-es-5  
staging-index-839920f07e6b11e6b71d0401f8d88101-new2 12    r      STARTED    248486   722mb 10.1.1.16 bematech-es-5  
master-index-8cd6e43115e9416eb23609486fa053e3       0     p      STARTED  14639524  11.7gb 10.1.1.16 bematech-es-5  
staging-index-8cd6e43115e9416eb23609486fa053e3      0     r      STARTED   3926917   4.9gb 10.1.1.16 bematech-es-5  
root@bematech-es-21:~# 
#+END_EXAMPLE

#+BEGIN_EXAMPLE
root@bematech-es-21:~# curl $es_ip:9200/_cat/shards?v | grep "bematech-es-7 " 
curl $es_ip:9200/_cat/shards?v | grep "bematech-es-7 " 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 26793  100 26793    0     0   182k      0 --:--:-- --:--:-- --:--:--  182k
staging-fd1125e03cb711e6878f0401f8d88c01            0     p      STARTED         0    159b 10.1.1.18 bematech-es-7  
master-index-839920f07e6b11e6b71d0401f8d88101-new2  7     r      STARTED   1071686   1.5gb 10.1.1.18 bematech-es-7  
master-index-839920f07e6b11e6b71d0401f8d88101-new2  12    p      STARTED   1068992   1.5gb 10.1.1.18 bematech-es-7  
master-index-839920f07e6b11e6b71d0401f8d88101-new2  11    r      STARTED   1067016   1.5gb 10.1.1.18 bematech-es-7  
master-index-e4010da4110ba377d100f050cb4440db       0     p      STARTED     31274  32.6mb 10.1.1.18 bematech-es-7  
staging-index-839920f07e6b11e6b71d0401f8d88101-new2 18    p      STARTED    247845 723.9mb 10.1.1.18 bematech-es-7  
staging-index-839920f07e6b11e6b71d0401f8d88101-new2 16    r      STARTED    247591 724.4mb 10.1.1.18 bematech-es-7  
staging-index-839920f07e6b11e6b71d0401f8d88101-new2 17    r      STARTED    247840 710.5mb 10.1.1.18 bematech-es-7  
master-index-8cd6e43115e9416eb23609486fa053e3       0     r      STARTED  14639524  11.7gb 10.1.1.18 bematech-es-7  
root@bematech-es-21:~# 
#+END_EXAMPLE

#+BEGIN_EXAMPLE
root@bematech-es-21:~# curl $es_ip:9200/_cat/shards?v | grep "bematech-es-8 " 
curl $es_ip:9200/_cat/shards?v | grep "bematech-es-8 " 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 26793  100 26793    0     0   180k      0 --:--:-- --:--:-- --:--:--  181k
master-index-839920f07e6b11e6b71d0401f8d88101-new2  10    p      STARTED   1069811   1.4gb 10.1.1.19 bematech-es-8  
master-index-839920f07e6b11e6b71d0401f8d88101-new2  9     p      STARTED   1069620   1.4gb 10.1.1.19 bematech-es-8  
master-index-839920f07e6b11e6b71d0401f8d88101-new2  6     p      STARTED   1071475   1.4gb 10.1.1.19 bematech-es-8  
staging-index-e4010da4110ba377d100f050cb4440db      2     r      STARTED      6684  22.5mb 10.1.1.19 bematech-es-8  
master-index-e4010da4110ba377d100f050cb4440db       2     r      STARTED     31868  33.8mb 10.1.1.19 bematech-es-8  
staging-index-839920f07e6b11e6b71d0401f8d88101-new2 5     r      STARTED    247857 723.6mb 10.1.1.19 bematech-es-8  
staging-index-839920f07e6b11e6b71d0401f8d88101-new2 15    p      STARTED    247173 743.9mb 10.1.1.19 bematech-es-8  
staging-index-839920f07e6b11e6b71d0401f8d88101-new2 14    r      STARTED    248825 703.6mb 10.1.1.19 bematech-es-8  
staging-index-8cd6e43115e9416eb23609486fa053e3      3     p      STARTED   1199080   3.2gb 10.1.1.19 bematech-es-8  
root@bematech-es-21:~# 
#+END_EXAMPLE
**** shards
#+BEGIN_EXAMPLE
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-01 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-02 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-04 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-05 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-1  
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-11 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-12 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-13 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-14 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-15 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-16 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-17 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-19 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-20 
master-index-abae8b30ac9b11e692000401f8d88101       bematech-es-21 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-01 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-02 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-03 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-04 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-05 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-1  
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-10 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-12 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-15 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-18 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-19 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-21 
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-4  
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-6  
staging-index-abae8b30ac9b11e692000401f8d88101      bematech-es-9  
#+END_EXAMPLE

#+BEGIN_EXAMPLE
root@bematech-es-21:~# curl $es_ip:9200/_cat/shards?v | grep "abae8b30ac9b11e692000401f8d88101"
curl $es_ip:9200/_cat/shards?v | grep "abae8b30ac9b11e692000401f8d88101"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 26793  100 26793    0     0   169k      0 --:--:-- --:--:-- --:--:--  169k
staging-index-abae8b30ac9b11e692000401f8d88101      4     r      STARTED   5294263   8.6gb 10.1.1.57 bematech-es-05 
staging-index-abae8b30ac9b11e692000401f8d88101      4     r      STARTED   5294263   8.6gb 10.1.1.27 bematech-es-9  
staging-index-abae8b30ac9b11e692000401f8d88101      4     p      STARTED   5294263   8.6gb 10.1.1.40 bematech-es-15 
staging-index-abae8b30ac9b11e692000401f8d88101      3     r      STARTED   5297358   8.9gb 10.1.1.12 bematech-es-1  
staging-index-abae8b30ac9b11e692000401f8d88101      3     p      STARTED   5297358    16gb 10.1.1.48 bematech-es-19 
staging-index-abae8b30ac9b11e692000401f8d88101      3     r      STARTED   5297358   8.9gb 10.1.1.47 bematech-es-18 
staging-index-abae8b30ac9b11e692000401f8d88101      2     r      STARTED   5232199   9.2gb 10.1.1.56 bematech-es-04 
staging-index-abae8b30ac9b11e692000401f8d88101      2     p      STARTED   5232199   9.2gb 10.1.1.53 bematech-es-03 
staging-index-abae8b30ac9b11e692000401f8d88101      2     r      STARTED   5232199   9.2gb 10.1.1.50 bematech-es-21 
staging-index-abae8b30ac9b11e692000401f8d88101      1     p      STARTED   5307911   8.9gb 10.1.1.51 bematech-es-01 
staging-index-abae8b30ac9b11e692000401f8d88101      1     r      STARTED   5307911   8.9gb 10.1.1.17 bematech-es-6  
staging-index-abae8b30ac9b11e692000401f8d88101      1     r      STARTED   5307911   8.9gb 10.1.1.15 bematech-es-4  
staging-index-abae8b30ac9b11e692000401f8d88101      0     r      STARTED   5315436   8.5gb 10.1.1.52 bematech-es-02 
staging-index-abae8b30ac9b11e692000401f8d88101      0     p      STARTED   5315436   8.5gb 10.1.1.34 bematech-es-12 
staging-index-abae8b30ac9b11e692000401f8d88101      0     r      STARTED   5315436   8.5gb 10.1.1.28 bematech-es-10 
master-index-abae8b30ac9b11e692000401f8d88101       4     p      STARTED 261537292 185.5gb 10.1.1.41 bematech-es-16 
master-index-abae8b30ac9b11e692000401f8d88101       4     r      STARTED 261537292 239.6gb 10.1.1.12 bematech-es-1  
master-index-abae8b30ac9b11e692000401f8d88101       4     r      STARTED 261537292 185.7gb 10.1.1.34 bematech-es-12 
master-index-abae8b30ac9b11e692000401f8d88101       3     r      STARTED 260342520 184.9gb 10.1.1.39 bematech-es-14 
master-index-abae8b30ac9b11e692000401f8d88101       3     p      STARTED 260342520 187.6gb 10.1.1.40 bematech-es-15 
master-index-abae8b30ac9b11e692000401f8d88101       3     r      STARTED 260342520 187.5gb 10.1.1.35 bematech-es-13 
master-index-abae8b30ac9b11e692000401f8d88101       2     r      STARTED 260125589   248gb 10.1.1.52 bematech-es-02 
master-index-abae8b30ac9b11e692000401f8d88101       2     r      STARTED 260125589   248gb 10.1.1.57 bematech-es-05 
master-index-abae8b30ac9b11e692000401f8d88101       2     p      STARTED 260125589 188.9gb 10.1.1.50 bematech-es-21 
master-index-abae8b30ac9b11e692000401f8d88101       1     p      STARTED 262254452 187.7gb 10.1.1.33 bematech-es-11 
master-index-abae8b30ac9b11e692000401f8d88101       1     r      STARTED 262254452 218.9gb 10.1.1.56 bematech-es-04 
master-index-abae8b30ac9b11e692000401f8d88101       1     r      STARTED 262254452   188gb 10.1.1.51 bematech-es-01 
master-index-abae8b30ac9b11e692000401f8d88101       0     r      STARTED 261482967 363.3gb 10.1.1.49 bematech-es-20 
master-index-abae8b30ac9b11e692000401f8d88101       0     r      STARTED 261482967   364gb 10.1.1.48 bematech-es-19 
master-index-abae8b30ac9b11e692000401f8d88101       0     p      STARTED 261482967 364.1gb 10.1.1.46 bematech-es-17 
root@bematech-es-21:~# 
#+END_EXAMPLE
**** 2017-04-02: Scale down bematech-es-5(191.234.184.158)
   CLOSED: [2017-04-02 Sun 09:58]
**** 2017-04-02: Scale down bematech-es-7(191.234.191.10)
   CLOSED: [2017-04-02 Sun 09:58]
*** 2017-04-05: [#A] DO reindex two giant indices: 36 hours
  CLOSED: [2017-04-06 Thu 09:47]
pri.store.size
**** 2017-04-06: staging-index-8cd6e43115e9416eb23609486fa053e3: 57.6gb: (18.45 hours)
   CLOSED: [2017-03-29 Wed 20:02]
wget -O /tmp/es_reindex.sh \
  https://raw.githubusercontent.com/TOTVS/mdmpublic/master/bash/es_reindex.sh

export es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
export es_port=9200
export old_es_index_name="staging-index-8cd6e43115e9416eb23609486fa053e3"
export shard_count="20"
export es_alias_name="staging-8cd6e43115e9416eb23609486fa053e3"

bash /tmp/es_reindex.sh "$old_es_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.log

time curl -XGET "http://${es_ip}:${es_port}/_cat/indices?v" | grep $old_es_index_name
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-04-06: close staging-index-8cd6e43115e9416eb23609486fa053e3
   CLOSED: [2017-03-29 Wed 20:02]
**** HALF staging-index-56b33120ef1311e6a81da2f42be00f79: 93.9gb (36 hours)
wget -O /tmp/es_reindex.sh \
  https://raw.githubusercontent.com/TOTVS/mdmpublic/master/bash/es_reindex.sh

export es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
export es_port=9200
export old_es_index_name="staging-index-56b33120ef1311e6a81da2f42be00f79"
export shard_count="20"
export es_alias_name="staging-56b33120ef1311e6a81da2f42be00f79"

bash /tmp/es_reindex.sh "$old_es_index_name" "$shard_count" "$es_alias_name" \
   | tee -a /var/log/es_reindex.log
**** TODO close staging-index-56b33120ef1311e6a81da2f42be00f79

*** #  --8<-------------------------- separator ------------------------>8--
**** 2017-04-03: volume-sfo2-11
   CLOSED: [2017-04-02 Sun 17:49]
**** 2017-04-03: volume-sfo2-10
   CLOSED: [2017-04-02 Sun 20:35]
**** 2017-04-03: volume-sfo2-08
   CLOSED: [2017-04-03 Mon 15:18]
*** 2017-04-03: remove DO volumes
  CLOSED: [2017-04-03 Mon 15:18]
**** 2017-04-03: volume-sfo2-15
   CLOSED: [2017-04-02 Sun 13:31]
**** 2017-04-03: volume-sfo2-2
   CLOSED: [2017-04-02 Sun 14:55]
**** 2017-04-03: volume-sfo2-12
   CLOSED: [2017-04-02 Sun 14:55]
*** 2017-04-04: [#A] Azure reindex staging-index-839920f07e6b11e6b71d0401f8d88101-new2
  CLOSED: [2017-04-03 Mon 20:30]
#+BEGIN_EXAMPLE
root@bematech-es-21:/var/log/elasticsearch# # Double confirm indices status
root@bematech-es-21:/var/log/elasticsearch# curl "$es_ip:9200/_cat/indices?v" | grep "$(echo $alias_index_name | sed "s/.*-//g")"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  3500  100  3500    0     0  17346      0 --:--:-- --:--:-- --:--:-- 17412
green  open   staging-index-839920f07e6b11e6b71d0401f8d88101-new2  20   2    4962666        59652     42.6gb         14.2gb
green  open   staging-index-839920f07e6b11e6b71d0401f8d88101        1   2   23674907      3600764      219gb           73gb
green  open   master-index-839920f07e6b11e6b71d0401f8d88101-new2   20   2   21788120       522743     93.3gb           31gb
       close  master-index-839920f07e6b11e6b71d0401f8d88101-new
       close  staging-index-839920f07e6b11e6b71d0401f8d88101-new
       close  master-index-839920f07e6b11e6b71d0401f8d88101
root@bematech-es-21:/var/log/elasticsearch#
root@bematech-es-21:/var/log/elasticsearch# echo $new_index_name
staging-index-839920f07e6b11e6b71d0401f8d88101-new2
root@bematech-es-21:/var/log/elasticsearch# echo $REINDEX_BATCH_SIZE
500
root@bematech-es-21:/var/log/elasticsearch# # Re-index
root@bematech-es-21:/var/log/elasticsearch# time curl -XPOST "http://${es_ip}:${es_port}/_reindex?pretty" -d "
>     {
>     \"conflicts\": \"proceed\",
>     \"source\": {
>     \"index\": \"${old_index_name}\",
>     \"size\": \"${REINDEX_BATCH_SIZE}\"
>     },
>     \"dest\": {
>     \"index\": \"${new_index_name}\",
>     \"op_type\": \"create\",
>     \"version_type\": \"external\"
>     }
> }" | tee -a /var/log/es_index_manual.log
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   500    0   216    0   284      0      0 --:--:--  3:11:42 --:--:--     0^Nc
{
  "took" : 11502817,
  "timed_out" : false,
  "total" : 23603839,
  "updated" : 0,
  "created" : 18654673,
  "batches" : 47208,
  "version_conflicts" : 4949166,
  "noops" : 0,
  "retries" : 0,
  "failures" : [ ]
}

real    191m48.797s
user    0m0.316s
sys     0m0.820s
root@bematech-es-21:/var/log/elasticsearch# # Double confirm indices status
root@bematech-es-21:/var/log/elasticsearch# curl "$es_ip:9200/_cat/indices?v" | grep "$(echo $alias_index_name | sed "s/.*-//g")"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  3500  100  3500    0     0   3508      0 --:--:-- --:--:-- --:--:--  3507
green  open   staging-index-839920f07e6b11e6b71d0401f8d88101-new2  20   2   23701518        36804    228.6gb         73.5gb
green  open   staging-index-839920f07e6b11e6b71d0401f8d88101        1   2   23674907      3600764      219gb           73gb
green  open   master-index-839920f07e6b11e6b71d0401f8d88101-new2   20   2   21828859       520381     94.1gb         31.4gb
       close  master-index-839920f07e6b11e6b71d0401f8d88101-new
       close  staging-index-839920f07e6b11e6b71d0401f8d88101-new
       close  master-index-839920f07e6b11e6b71d0401f8d88101
root@bematech-es-21:/var/log/elasticsearch#
root@bematech-es-21:/var/log/elasticsearch# # Close old index
root@bematech-es-21:/var/log/elasticsearch# curl -XPOST "http://$es_ip:9200/$old_index_name/_close"
{"acknowledged":true}root@bematech-es-21:/var/log/elasticsearch#
#+END_EXAMPLE

*** 2017-04-04: update jq version in DO and Azure env
  CLOSED: [2017-04-04 Tue 11:10]
http://prodjenkins.carol.ai:18080/job/RunCommandOnServers/365/parameters/
http://bematechjenkins.carol.ai:18080/job/RunCommandOnServers/116/
*** 2017-04-04: Manually fix sshd setting in DO and Azure
  CLOSED: [2017-04-04 Tue 14:07]
http://prodjenkins.carol.ai:18080/job/RunCommandOnServers/367/console
http://bematechjenkins.carol.ai:18080/job/RunCommandOnServers/117/console

PasswordAuthentication noCiphers aes256-ctr,aes192-ctr,aes128-ctr

-> 
PasswordAuthentication no
Ciphers aes256-ctr,aes192-ctr,aes128-ctr

cp /etc/ssh/sshd_config /root/

grep "aes256-ctr,aes192-ctr,aes128-ctr" /etc/ssh/sshd_config

sed -i "s/PasswordAuthentication noCiphers aes256-ctr,aes192-ctr,aes128-ctr/PasswordAuthentication no\nCiphers aes256-ctr,aes192-ctr,aes128-ctr/g" /etc/ssh/sshd_config

grep "aes256-ctr,aes192-ctr,aes128-ctr" /etc/ssh/sshd_config

diff /root/sshd_config /etc/ssh/sshd_config

*** 2017-04-04: (2) After DO rebooted prod-app-02 for their maintenance, sshd fail to start
  CLOSED: [2017-04-04 Tue 14:08]
https://trello.com/c/dvj2u4fT

Azure:

ClientAliveInterval 120Ciphers aes256-ctr,aes192-ctr,aes128-ctr

tail -n 20 /etc/ssh/sshd_config

PasswordAuthentication noCiphers aes256-ctr,aes192-ctr,aes128-ctr
MACs hmac-sha1
*** 2017-04-04: examine all Azure and DO machines for sshd configuration
  CLOSED: [2017-04-04 Tue 16:02]
*** 2017-04-04: Remove SQLServer VM: william-aimssql
  CLOSED: [2017-04-04 Tue 22:32]
https://williamresourcegroup614.blob.core.windows.net/vhds/william-aimssql-disk-1-20170120152416.vhd
https://williamresourcegroup614.blob.core.windows.net/vhds/william-aimssql-disk-8-20170130150903.vhd
*** 2017-04-06: Add DNS Cname for audit ES: Roboson
  CLOSED: [2017-04-06 Thu 09:41]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-04-06: Disable auto_create_index in elasticsearch.yml by default
  CLOSED: [2017-04-06 Thu 20:23]
@bruno had found the root cause. here is the reason.
1. although index was deleted because tenant was removed. "Someone" accepted the invite flow and since BE did not check if index is created, it just create the index again after sending first record into staging.

2. if ES found no index for a record send to it, it default creates the index and use default shard and replica settings from elasticsearch.yml file.

[5:29]  
let me test this flag in elasticsearch.yml:
action.auto_create_index: false

so, it will not create index if index does not exists

Denny Zhang [5:30 PM] 
That makes sense. Thanks!

Kung Wang [5:32 PM] 
in ES, here is the comment for replica count if we did not set it, the default is 1


# Set the number of replicas (additional copies) of an index (1 by default):
#
index.number_of_replicas: 1

Denny Zhang [5:38 PM] 
Let me change it as well in the code.

[5:38]  
Sounds good?

Kung Wang [5:39 PM] 
here is the local test:


$ curl -XPUT localhost:9200/twitter/tweet/1 -d '
> {
>     "user" : "kimchy",
>     "post_date" : "2009-11-15T14:12:12",
>     "message" : "trying out Elasticsearch"
> }'

{"error":{"root_cause":[{"type":"index_not_found_exception","reason":"no such index","resource.type":"index_expression","resource.id":"twitter","index":"twitter"}],"type":"index_not_found_exception","reason":"no such index","resource.type":"index_expression","resource.id":"twitter","index":"twitter"},"status":404}

[5:39]  
yes, let's add that to prevent index from randomly created

[5:43]  
if without that flag set in elasticsearch.yml, here is the local test:

$ curl -XPUT localhost:9200/twitter/tweet/1 -d '
> {
>     "user" : "kimchy",
>     "post_date" : "2009-11-15T14:12:12",
>     "message" : "trying out Elasticsearch"
> }'
{"_index":"twitter","_type":"tweet","_id":"1","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}

[5:43]  
so, without that flag, any document created, if index does not exist, it creates it.

Denny Zhang [5:44 PM] 
So we disable auto_create_index in elasticsearch.yml by default?

Kung Wang [5:45 PM] 
yes, I would vote to disable it.

Denny Zhang [5:45 PM] 
Got it. Will file a ticket and propose a fix.

Should be easy

Mitu Singh [5:46 PM] 
yes I agree
*** 2017-04-06: [#A] suspicious index in DO                                :IMPORTANT:
  CLOSED: [2017-04-06 Thu 20:24]
Denny Zhang [4:10 PM] 
@bruno

Any idea about 860245c0841e11e6a8260401f8d88101 index?

Looks like we have closed master-index-860245c0841e11e6a8260401f8d88101* and staging-index-860245c0841e11e6a8260401f8d88101* on purpose.

Now a new index of staging-860245c0841e11e6a8260401f8d88101 was created automatically.
And its replica is 1, shard count is 5.

```root@prod-es-01:/tmp# curl $es_ip:9200/_cat/indices?v | grep 860245c0841e11e6a8260401f8d88101
       close  master-index-860245c0841e11e6a8260401f8d88101
       close  staging-index-860245c0841e11e6a8260401f8d88101
       close  staging-index-860245c0841e11e6a8260401f8d88101-new
       green  open   staging-860245c0841e11e6a8260401f8d88101
       close  master-index-860245c0841e11e6a8260401f8d88101-new
```

[4:14]  
Right now staging-860245c0841e11e6a8260401f8d88101 has 0 documents.

Should I close this suspicious index?

[4:17]  
Three questions:
1. Looks like our application has created staging-860245c0841e11e6a8260401f8d88101? Why?
2. When we create it, why the replica is 1?
3. Should we change the default shard count from 5 to 10?

[4:21]  
Just closed the staging-860245c0841e11e6a8260401f8d88101, to make the error apparently, if we have some.
*** 2017-04-07: (1) PoC with Couchbase Community version: Community 4.5.0
  CLOSED: [2017-04-07 Fri 23:45]
4.1.0-5005-1

wget -O /root/couchbase-server-community_4.5.0-ubuntu14.04_amd64.deb http://repo.carol.ai:18000/couchbase-server-community_4.5.0-ubuntu14.04_amd64.deb
wget -O /root/couchbase-server-enterprise_4.1.0-ubuntu14.04_amd64.deb http://repo.carol.ai:18000/couchbase-server-enterprise_4.1.0-ubuntu14.04_amd64.deb

# rebuild containers
image_name="totvslabs/mdm"
container_name="mdm-all-in-one"
container_hostname="aio"

docker stop $container_name; docker rm $container_name

docker run -d -t --privileged -v /root/couchbase/:/opt/couchbase/ -h $container_hostname --name $container_name -p 8443:8443 -p 80:80 -p 443:443 -p 6022:22 $aio_port_forwarding $image_name /usr/sbin/sshd -D

rm -rf /root/couchbase/*

# manual test
docker cp /root/couchbase-server-community_4.5.0-ubuntu14.04_amd64.deb mdm-all-in-one:/root/

# docker cp /root/couchbase-server-enterprise_4.1.0-ubuntu14.04_amd64.deb mdm-all-in-one:/root/

# login to container
docker exec -it mdm-all-in-one bash

# stop services
service mdm stop
grep flush /opt/mdm/config/mdm.yml
sed -i 's/flushDatabaseOnConnect: false/flushDatabaseOnConnect: true/g' /opt/mdm/config/mdm.yml
grep flush /opt/mdm/config/mdm.yml

apt-get remove couchbase-server
dpkg -r couchbase-server
dpkg --purge couchbase-server
dpkg --list | grep couchbase

dpkg --remove couchbase-server
dpkg -i /root/couchbase-server-community_4.5.0-ubuntu14.04_amd64.deb

# check couchbase status
service couchbase-server status
telnet localhost 8091
telnet localhost 11211

# start mdm
tail -f /opt/mdm/logs/mdm-initscript.log

service mdm start

ls -lth /opt/couchbase/

cd /opt/couchbase/

# check status
service couchbase-server stop
dpkg -s couchbase-server
dpkg -S couchbase-server

dpkg --remove couchbase-server
dpkg -s couchbase-server

dpkg --remove couchbase-server-community

apt-get remove couchbase-server

service couchbase-server start

cd /opt/couchbase
dpkg -i /root/couchbase-server-community_4.5.0-ubuntu14.04_amd64.deb
dpkg -s couchbase-server

# check status
service couchbase-server status
telnet localhost 8091
telnet localhost 11211

# check couchbase log
cd /opt/couchbase/var/lib/couchbase/logs
**** highlight Note
1. No way to downgrade
2. Have the risk of data flush

#+BEGIN_EXAMPLE
[06 Apr 2017;15:33:02.294]-[WARN ][SpeechletServlet - getTimetampVerifier:190 - main - T_mdm] - No timestamp tolerance has been configured, disabling timestamp verification
Scanned package: com/totvslabs/mdm/connector/model/builders
Exception in thread "main" com.couchbase.client.core.CouchbaseException: NOT_EXISTS: "unknown pool"
        at com.couchbase.client.java.cluster.DefaultAsyncClusterManager$3.call(DefaultAsyncClusterManager.java:113)
        at com.couchbase.client.java.cluster.DefaultAsyncClusterManager$3.call(DefaultAsyncClusterManager.java:106)
        at rx.internal.util.ActionObserver.onNext(ActionObserver.java:39)
        at rx.internal.operators.OnSubscribeDoOnEach$DoOnEachSubscriber.onNext(OnSubscribeDoOnEach.java:96)
        at rx.internal.operators.OnSubscribeRedo$2$1.onNext(OnSubscribeRedo.java:244)
        at rx.internal.operators.OperatorMerge$MergeSubscriber.emitScalar(OperatorMerge.java:395)
        at rx.internal.operators.OperatorMerge$MergeSubscriber.tryEmit(OperatorMerge.java:355)
        at rx.internal.operators.OperatorMerge$InnerSubscriber.onNext(OperatorMerge.java:846)
        at rx.internal.producers.SingleProducer.request(SingleProducer.java:65)
        at rx.Subscriber.setProducer(Subscriber.java:211)
        at rx.subjects.AsyncSubject.onCompleted(AsyncSubject.java:103)
        at com.couchbase.client.core.endpoint.AbstractGenericHandler.completeResponse(AbstractGenericHandler.java:376)
        at com.couchbase.client.core.endpoint.AbstractGenericHandler.access$000(AbstractGenericHandler.java:69)
        at com.couchbase.client.core.endpoint.AbstractGenericHandler$1.call(AbstractGenericHandler.java:394)
        at rx.internal.schedulers.ScheduledAction.run(ScheduledAction.java:55)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: rx.exceptions.OnErrorThrowable$OnNextValue: OnError while emitting onNext value: com.couchbase.client.core.message.config.ClusterConfigResponse.class
        at rx.exceptions.OnErrorThrowable.addValueAsLastCause(OnErrorThrowable.java:118)
        at rx.exceptions.Exceptions.throwOrReport(Exceptions.java:190)
        at rx.internal.operators.OnSubscribeDoOnEach$DoOnEachSubscriber.onNext(OnSubscribeDoOnEach.java:98)
        ... 18 more
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B
#+END_EXAMPLE

*** 2017-04-08: [#A] force-merge in DO
  CLOSED: [2017-04-08 Sat 07:14]
https://github.com/TOTVS/mdmdevops/wiki/Critical-Maintenance-History-Of-Prod-Env#april-2017-force-merge-in-do
**** 2017-04-08: staging-index-8cd6e43115e9416eb23609486fa053e3-new2		72.7gb -> 	88935160 -> 
   CLOSED: [2017-04-07 Fri 12:17]
export index_name="staging-index-8cd6e43115e9416eb23609486fa053e3-new2"

root@prod-es-01:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  6572  100  6572    0     0   5436      0  0:00:01  0:00:01 --:--:--  5440
green open  staging-index-8cd6e43115e9416eb23609486fa053e3-new2 20 2  88935160  1123871  218.2gb  72.7gb
root@prod-es-01:/tmp#
root@prod-es-01:/tmp# # only delete those indices that has deletes in it
root@prod-es-01:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"
{
  "_shards" : {
    "total" : 60,
    "successful" : 60,
    "failed" : 0
  }
}

real    0m0.794s
user    0m0.004s
sys     0m0.004s
root@prod-es-01:/tmp#
root@prod-es-01:/tmp# # check index status
root@prod-es-01:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  6572  100  6572    0     0   6955      0 --:--:-- --:--:-- --:--:--  6954
green open  staging-index-8cd6e43115e9416eb23609486fa053e3-new2 20 2  88935160  1123871  218.2gb  72.7gb
**** 2017-04-08: staging-index-8a18aa800e5911e785f24a8136534b63		99.4gb -> 	103142902 -> 
   CLOSED: [2017-04-07 Fri 17:07]
export index_name="staging-index-8a18aa800e5911e785f24a8136534b63"

root@prod-es-01:/tmp# export index_name="staging-index-8a18aa800e5911e785f24a8136534b63"
root@prod-es-01:/tmp# # check index status
root@prod-es-01:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  6572  100  6572    0     0   6525      0  0:00:01  0:00:01 --:--:--  6526
green open  staging-index-8a18aa800e5911e785f24a8136534b63       5 2 103181083  7093738  299.1gb  99.4gb
root@prod-es-01:/tmp#
root@prod-es-01:/tmp# # only delete those indices that has deletes in it
root@prod-es-01:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"
{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    159m1.969s
user    0m0.668s
sys     0m0.208s
You have new mail in /var/mail/root
root@prod-es-01:/tmp#
root@prod-es-01:/tmp# # check index status
root@prod-es-01:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  6572  100  6572    0     0   5848      0  0:00:01  0:00:01 --:--:--  5852
green open  staging-index-8a18aa800e5911e785f24a8136534b63       5 2 103881519  5427672  303.1gb 102.1gb

**** 2017-04-08: master-index-5e527960c34211e6b3ff0401f8d88c01		22.3gb -> 	31040884 -> 
   CLOSED: [2017-04-07 Fri 17:06]
export index_name="master-index-5e527960c34211e6b3ff0401f8d88c01"

root@prod-es-01:/var/log/elasticsearch# export index_name="master-index-5e527960c34211e6b3ff0401f8d88c01"
You have new mail in /var/mail/root
root@prod-es-01:/var/log/elasticsearch# # check index status
root@prod-es-01:/var/log/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  6572  100  6572    0     0   4457      0  0:00:01  0:00:01 --:--:--  4458
green open  master-index-5e527960c34211e6b3ff0401f8d88c01        5 2  31040884  1838995   66.8gb  22.3gb
root@prod-es-01:/var/log/elasticsearch#
root@prod-es-01:/var/log/elasticsearch# # only delete those indices that has deletes in it
root@prod-es-01:/var/log/elasticsearch# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"


{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    222m30.871s
user    0m1.100s
sys     0m0.296s
You have new mail in /var/mail/root
root@prod-es-01:/var/log/elasticsearch#
root@prod-es-01:/var/log/elasticsearch# # check index status
root@prod-es-01:/var/log/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  6572  100  6572    0     0   5545      0  0:0
green open  master-index-5e527960c34211e6b3ff0401f8d88c01        5 2  31040884   529975     59gb  19.7gb
0:01  0:00:01 --:--:--  5550
root@prod-es-01:/var/log/elasticsearch#
*** #  --8<-------------------------- separator ------------------------>8--
**** 2017-04-08: staging-index-5e527960c34211e6b3ff0401f8d88c01		4.9gb -> 	1886257 -> 
   CLOSED: [2017-04-07 Fri 19:42]
export index_name="staging-index-5e527960c34211e6b3ff0401f8d88c01"
root@prod-es-01:/var/log/elasticsearch# export index_name="staging-index-5e527960c34211e6b3ff0401f8d88c01"
root@prod-es-01:/var/log/elasticsearch# # check index status
root@prod-es-01:/var/log/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  6572  100  6572    0     0   3691      0  0:00:01  0:00:01 --:--:--  3692
green open  staging-index-5e527960c34211e6b3ff0401f8d88c01       5 2   1912803   510423     14gb   4.5gb
root@prod-es-01:/var/log/elasticsearch#
root@prod-es-01:/var/log/elasticsearch# # only delete those indices that has deletes in it
root@prod-es-01:/var/log/elasticsearch# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"
{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    120m55.560s
user    0m0.640s
sys     0m0.200s
You have new mail in /var/mail/root
root@prod-es-01:/var/log/elasticsearch#
root@prod-es-01:/var/log/elasticsearch# # check index status
root@prod-es-01:/var/log/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  6572  100  6572    0     0   7895      0 --:--:-- --:--:-- --:--:--  7889
green open  staging-index-5e527960c34211e6b3ff0401f8d88c01       5 2   1918099   109793   12.6gb   3.7gb
root@prod-es-01:/var/log/elasticsearch#
**** 2017-04-08: master-index-cf5e90403d5e11e68ac00401f8d88501		2.5gb -> 	2908303 -> 
   CLOSED: [2017-04-07 Fri 19:42]
export index_name="master-index-cf5e90403d5e11e68ac00401f8d88501"
root@prod-es-01:/tmp# export index_name="master-index-cf5e90403d5e11e68ac00401f8d88501"
root@prod-es-01:/tmp# # check index status
root@prod-es-01:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  6572  100  6572    0     0   4948      0  0:00:01
green open  master-index-cf5e90403d5e11e68ac00401f8d88501        5 2   2908303  1094968    7.6gb   2.5gb
  0:00:01 --:--:--  4945
root@prod-es-01:/tmp#
root@prod-es-01:/tmp# # only delete those indices that has deletes in it
root@prod-es-01:/tmp# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"
{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    124m46.425s
user    0m0.564s
sys     0m0.204s
You have new mail in /var/mail/root
root@prod-es-01:/tmp#
root@prod-es-01:/tmp# # check index status
root@prod-es-01:/tmp# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  6572  100  6572    0     0   6518      0  0:00:01  0:00:01 --:--:--  6526
green open  master-index-cf5e90403d5e11e68ac00401f8d88501        5 2   2908303    16248      6gb   2.2gb
**** 2017-04-08: staging-index-56b33120ef1311e6a81da2f42be00f79	
   CLOSED: [2017-04-08 Sat 07:14]
export index_name="staging-index-56b33120ef1311e6a81da2f42be00f79"


#+BEGIN_EXAMPLE
root@prod-es-01:/var/log/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  6572  100  6572    0     0   3953      0  0:00:01  0:00:01 --:--:--  3951
      close staging-index-56b33120ef1311e6a81da2f42be00f79-new
green open  staging-index-56b33120ef1311e6a81da2f42be00f79       5 2  16947543 35332601  222.8gb  68.9gb
root@prod-es-01:/var/log/elasticsearch#
root@prod-es-01:/var/log/elasticsearch# # only delete those indices that has deletes in it
root@prod-es-01:/var/log/elasticsearch# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"

{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    257m47.927s
user    0m1.388s
sys     0m0.256s
You have new mail in /var/mail/root
root@prod-es-01:/var/log/elasticsearch#
root@prod-es-01:/var/log/elasticsearch# # check index status
root@prod-es-01:/var/log/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  6510  100  6510    0     0   7211      0 --:--:-- --:--:-- --:--:--  7217
      close staging-index-56b33120ef1311e6a81da2f42be00f79-new
green open  staging-index-56b33120ef1311e6a81da2f42be00f79       5 2   8860578 4919014  137.2gb    53gb
#+END_EXAMPLE
*** 2017-04-08: manually patch elasticserch.yml: http://bematechjenkins.carol.ai:18080/job/RunCommandOnServers/126/console
  CLOSED: [2017-04-07 Fri 20:54]
discovery.zen.ping.unicast.hosts: ["10.1.1.28:9300","10.1.1.22:9300","10.1.1.18:9300","10.1.1.41:9300","10.1.1.17:9300","10.1.1.50:9300","10.1.1.49:9300","10.1.1.13:9300","10.1.1.27:9300","10.1.1.47:9300","10.1.1.39:9300","10.1.1.52:9300","10.1.1.29:9300","10.1.1.12:9300","10.1.1.19:9300","10.1.1.46:9300","10.1.1.34:9300","10.1.1.53:9300","10.1.1.35:9300","10.1.1.21:9300","10.1.1.51:9300","10.1.1.16:9300","10.1.1.40:9300","10.1.1.24:9300","10.1.1.23:9300","10.1.1.15:9300","10.1.1.48:9300","10.1.1.14:9300","10.1.1.20:9300","10.1.1.33:9300","bematech-es-1:9300","bematech-es-2:9300","bematech-es-3:9300","bematech-es-4:9300","bematech-es-5:9300","bematech-es-6:9300","bematech-es-7:9300","bematech-es-8:9300","bematech-es-9:9300","bematech-es-10:9300","bematech-es-11:9300","bematech-es-12:9300","bematech-es-13:9300","bematech-es-14:9300","bematech-es-15:9300","bematech-es-16:9300","bematech-es-17:9300","bematech-es-18:9300","bematech-es-19:9300","bematech-es-20:9300","bematech-es-21:9300","bematech-es-01:9300","bematech-es-02:9300","bematech-es-03:9300","bematech-es-04:9300","bematech-es-05:9300"]

discovery.zen.ping.unicast.hosts:["10.1.1.12:9300", "10.1.1.15:9300", "10.1.1.17:9300", "10.1.1.19:9300", "10.1.1.27:9300", "10.1.1.28:9300", "10.1.1.33:9300", "10.1.1.34:9300", "10.1.1.35:9300", "10.1.1.39:9300", "10.1.1.40:9300", "10.1.1.41:9300", "10.1.1.46:9300", "10.1.1.47:9300", "10.1.1.48:9300", "10.1.1.49:9300", "10.1.1.50:9300", "10.1.1.51:9300", "10.1.1.52:9300", "10.1.1.53:9300", "10.1.1.56:9300", "10.1.1.57:9300", "bematech-es-1:9300", "bematech-es-4:9300", "bematech-es-6:9300", "bematech-es-8:9300", "bematech-es-9:9300", "bematech-es-10:9300", "bematech-es-11:9300", "bematech-es-12:9300", "bematech-es-13:9300", "bematech-es-14:9300", "bematech-es-15:9300", "bematech-es-16:9300", "bematech-es-17:9300", "bematech-es-18:9300", "bematech-es-19:9300", "bematech-es-20:9300", "bematech-es-21:9300", "bematech-es-01:9300", "bematech-es-02:9300", "bematech-es-03:9300", "bematech-es-04:9300", "bematech-es-05:9300"]

cp /etc/elasticsearch/elasticsearch.yml /root/elasticsearch.yml

sed -i "s/discovery.zen.ping.unicast.hosts: \[.*/discovery.zen.ping.unicast.hosts: \[\"10.1.1.12:9300\", \"10.1.1.15:9300\", \"10.1.1.17:9300\", \"10.1.1.19:9300\", \"10.1.1.27:9300\", \"10.1.1.28:9300\", \"10.1.1.33:9300\", \"10.1.1.34:9300\", \"10.1.1.35:9300\", \"10.1.1.39:9300\", \"10.1.1.40:9300\", \"10.1.1.41:9300\", \"10.1.1.46:9300\", \"10.1.1.47:9300\", \"10.1.1.48:9300\", \"10.1.1.49:9300\", \"10.1.1.50:9300\", \"10.1.1.51:9300\", \"10.1.1.52:9300\", \"10.1.1.53:9300\", \"10.1.1.56:9300\", \"10.1.1.57:9300\", \"bematech-es-1:9300\", \"bematech-es-4:9300\", \"bematech-es-6:9300\", \"bematech-es-8:9300\", \"bematech-es-9:9300\", \"bematech-es-10:9300\", \"bematech-es-11:9300\", \"bematech-es-12:9300\", \"bematech-es-13:9300\", \"bematech-es-14:9300\", \"bematech-es-15:9300\", \"bematech-es-16:9300\", \"bematech-es-17:9300\", \"bematech-es-18:9300\", \"bematech-es-19:9300\", \"bematech-es-20:9300\", \"bematech-es-21:9300\", \"bematech-es-01:9300\", \"bematech-es-02:9300\", \"bematech-es-03:9300\", \"bematech-es-04:9300\", \"bematech-es-05:9300\"\]/g"  /etc/elasticsearch/elasticsearch.yml

diff /root/elasticsearch.yml /etc/elasticsearch/elasticsearch.yml

grep  "discovery.zen.ping.unicast.hosts: \["  /etc/elasticsearch/elasticsearch.yml
*** 2017-04-08: manually patch /etc/hosts: http://bematechjenkins.carol.ai:18080/job/DeploySystemRehearsal/131/console
  CLOSED: [2017-04-07 Fri 23:45]
http://bematechjenkins.carol.ai:18080/job/RunCommandOnServers/128/console
*** 2017-04-08: force-merge for Azure index
  CLOSED: [2017-04-08 Sat 13:03]

Finished force-merge on master-index-abae8b30ac9b11e692000401f8d88101.

It took 12.9hours(778m7.370s)

- Before: 1297276283(doc count) 165002778(deleted doc)   3.2tb(total storage)   1.2tb(primary storage)
- After: 1297276283(doc count) 18470771(deleted doc)   2.9tb(total storage) 932.4gb(primary storage)

export index_name="master-index-abae8b30ac9b11e692000401f8d88101"

# check index status
curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"

# only delete those indices that has deletes in it
time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"

# check index status
curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"


root@bematech-es-21:/var/log/elasticsearch# export index_name="master-index-abae8b30ac9b11e692000401f8d88101"
root@bematech-es-21:/var/log/elasticsearch# # check index status
root@bematech-es-21:/var/log/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  2889  100  2889    0     0   9285      0 --:--:-- --:--:-- --:--:--  9289
green open  master-index-abae8b30ac9b11e692000401f8d88101        5 2 1297276283 165002778   3.2tb   1.2tb
root@bematech-es-21:/var/log/elasticsearch#
root@bematech-es-21:/var/log/elasticsearch# # only delete those indices that has deletes in it
root@bematech-es-21:/var/log/elasticsearch# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"
{
  "_shards" : {
    "total" : 15,
    "successful" : 14,
    "failed" : 0
  }
}

real    778m7.370s
user    0m0.680s
sys     0m0.628s
You have new mail in /var/mail/root
root@bematech-es-21:/var/log/elasticsearch#
root@bematech-es-21:/var/log/elasticsearch# # check index status
root@bematech-es-21:/var/log/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2862  100  2862    0     0  12068      0 --:--:-- --:--:-- --:--:-- 12075
green open  master-index-abae8b30ac9b11e692000401f8d88101        5 2 1297276283 18470771   2.9tb 932.4gb
*** 2017-04-08: backup /etc/hosts: http://prodjenkins.carol.ai:18080/job/RunCommandOnServers/369/console
  CLOSED: [2017-04-08 Sat 15:36]
cp /etc/hosts /root/hosts.bak
*** 2017-04-08: bematech remove duplicate entries in /etc/hosts
  CLOSED: [2017-04-08 Sat 15:50]
cp /etc/hosts /tmp/hosts.bak

uniq  /etc/hosts > /tmp/hosts

cp /tmp/hosts /etc/hosts

diff /etc/hosts /tmp/hosts.bak
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-04-03: #20 [Docker] - Prepare for cloud deployment for IoT: 3h
  CLOSED: [2017-04-03 Mon 15:13]
https://bitbucket.org/nubesecure/iot/issues/20/docker-prepare-for-cloud-deployment-for

*** 2017-04-08: Trouble shooting why kumku-u docker image is not up-to-date: 1h
  CLOSED: [2017-04-08 Sat 23:21]
*** 2017-04-09: [#A] 3h: penroz repo can't work for latest brozton ci: java "$JAVA_OPTS" -jar ...
  CLOSED: [2017-04-09 Sun 10:33]
#+BEGIN_EXAMPLE
root@totvsjenkins:/home/denny/soteria_code/penroz-soterianetworks# docker-compose logs oauth2
Attaching to oauth2
oauth2     | Using JAVA_OPTS=
oauth2     | [2017-04-09 04:52:25] Wait for: test -f /opt/penroz-idp/war/idp.war
oauth2     | [2017-04-09 04:52:28] Action pass
oauth2     |
oauth2     | Prepare to generate jks key and metadata for module: oauth2
oauth2     | Certificate stored in file <oauth2.cer>
oauth2     | /oauth2.jks file created
oauth2     | SP metadata file created: oauth2-sp-metadata.xml
oauth2     | SP metadata file copied to: /opt/penroz-idp/metadata
oauth2     | <?xml version="1.0" encoding="UTF-8"?>
oauth2     | <!-- This file is an EXAMPLE metadata configuration file. -->
oauth2     | <MetadataProvider id="ShibbolethMetadata" xsi:type="ChainingMetadataProvider"
oauth2     |     xmlns="urn:mace:shibboleth:2.0:metadata"
oauth2     |     xmlns:resource="urn:mace:shibboleth:2.0:resource"
oauth2     |     xmlns:security="urn:mace:shibboleth:2.0:security"
oauth2     |     xmlns:md="urn:oasis:names:tc:SAML:2.0:metadata"
oauth2     |     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
oauth2     |     xsi:schemaLocation="urn:mace:shibboleth:2.0:metadata http://shibboleth.net/schema/idp/shibboleth-metadata.xsd
oauth2     |                         urn:mace:shibboleth:2.0:resource http://shibboleth.net/schema/idp/shibboleth-resource.xsd
oauth2     |                         urn:mace:shibboleth:2.0:security http://shibboleth.net/schema/idp/shibboleth-security.xsd
oauth2     |                         urn:oasis:names:tc:SAML:2.0:metadata http://docs.oasis-open.org/security/saml/v2.0/saml-schema-metadata-2.0.xsd">
oauth2     |
oauth2     |
oauth2     | </MetadataProvider>
oauth2     | Update SP metadata providers
oauth2     | Error: Could not find or load main class
root@totvsjenkins:/home/denny/soteria_code/penroz-soterianetworks# docker-compose ^C
#+END_EXAMPLE
*** 2017-04-09: update demo env and do the trouble shooting: 1h
  CLOSED: [2017-04-09 Sun 22:48]
*** 2017-04-10: close suspicious ES indices in DO
  CLOSED: [2017-04-10 Mon 13:03]
curl -XPOST "http://$es_ip:9200/messagebroker/_close"
curl -XPOST "http://$es_ip:9200/phppath/_close"
curl -XPOST "http://$es_ip:9200/staging-fd1125e03cb711e6878f0401f8d88c01/_close"
curl -XPOST "http://$es_ip:9200/scroll/_close"
curl -XPOST "http://$es_ip:9200/spipe/_close"
curl -XPOST "http://$es_ip:9200/blazeds/_close"
curl -XPOST "http://$es_ip:9200/sawmillcl.exe/_close"
curl -XPOST "http://$es_ip:9200/flex2gateway/_close"
curl -XPOST "http://$es_ip:9200/webui/_close"
curl -XPOST "http://$es_ip:9200/perl/_close"
curl -XPOST "http://$es_ip:9200/sawmill6cl.exe/_close"
curl -XPOST "http://$es_ip:9200/egnrgo1.html/_close"
curl -XPOST "http://$es_ip:9200/lcds/_close"

curl $es_ip:9200/_cat/indices?v | grep -v close | grep -v master-index | grep -v staging-index
#+BEGIN_EXAMPLE
root@prod-es-01:/etc/nagios/nrpe.d# curl $es_ip:9200/_cat/indices?v | grep -v master-index | grep -v staging-index
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  7875  100  7875    0     0   4554      0  0:00:01  0:00:01 --:--:--  4554
health status index                                               pri rep docs.count docs.deleted store.size pri.store.size
green  open   messagebroker                                         5   1          0            0      1.5kb           795b
green  open   phppath                                               5   1          0            0      1.5kb           795b
green  open   staging-fd1125e03cb711e6878f0401f8d88c01              5   2          0            0      2.3kb           795b
green  open   scroll                                                5   1          0            0      1.5kb           795b
green  open   spipe                                                 5   1          0            0      1.5kb           795b
green  open   blazeds                                               5   1          0            0      1.5kb           795b
green  open   sawmillcl.exe                                         5   1          0            0      1.5kb           795b
green  open   flex2gateway                                          5   1          0            0      1.5kb           795b
       close  staging-860245c0841e11e6a8260401f8d88101
green  open   webui                                                 5   1          0            0      1.5kb           795b
green  open   .kibana                                               1   1        108            2      1.6mb        841.8kb
       close  staging-56b33120ef1311e6a81da2f42be00f79
green  open   perl                                                  5   1          0            0      1.5kb           795b
green  open   sawmill6cl.exe                                        5   1          0            0      1.5kb           795b
green  open   egnrgo1.html                                          5   1          0            0      1.5kb           795b
green  open   lcds                                                  5   1          0            0      1.5kb           795b
#+END_EXAMPLE
*** 2017-04-10: DO: suspicious ES index: staging-fd1125e03cb711e6878f0401f8d88c01
  CLOSED: [2017-04-10 Mon 13:04]
green  open   staging-fd1125e03cb711e6878f0401f8d88c01              5   2          0            0      2.3kb           795b

*** 2017-04-10: email sending is not working in TOTVSLabs
  CLOSED: [2017-04-10 Mon 14:43]
https://emailsrvr.com/index.php
http://status.emailsrvr.com

https://emailsrvr.com/index.php
support@carol.ai

mailConfiguration:
    mailServerHost: secure.emailsrvr.com
    mailServerPort: 465
    mailFromName: Carol AI Support
    mailFrom: support@carol.ai
    enableSmtpAuth: true
    adminUserId: support@carol.ai
    adminPassword: "89mZtd12*/w#0X7"
    useSsl: true
    enableTls: false
    debug: true
*** 2017-04-10: change password of totvslabs email account
  CLOSED: [2017-04-10 Mon 14:43]
Change.This@that
Change.That@that
Change.This@this

Change.This@that
*** 2017-04-10: #21 Add more check logics for docker healthcheck of IoT images: 3h
  CLOSED: [2017-04-10 Mon 22:35]
https://bitbucket.org/nubesecure/iot/issues/21/add-more-check-logics-for-docker
*** 2017-04-11: [#A] In DO: prod-es-18 not monitored by nagios
  CLOSED: [2017-04-11 Tue 16:32]
http://prodjenkins.carol.ai:18080/job/DeploySystemRehearsal/455/
*** 2017-04-12: Enable slack notification for git push of all major repo; Delete old dofacdenny repo: 1h
  CLOSED: [2017-04-12 Wed 11:39]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-04-12: wiki: Capacity planning for new DO env
  CLOSED: [2017-04-12 Wed 14:27]
*** 2017-04-12: inject /tmp/copy_id_rsa in Azure: http://bematechjenkins.carol.ai:18080/job/RunCommandOnServers/137/
  CLOSED: [2017-04-12 Wed 17:38]
echo "-----BEGIN RSA PRIVATE KEY-----
MIIEowIBAAKCAQEA46bmaefxd9r5lf37ds5bYIyUGk5splOnzrgCDuC+Xk+dj8ld
4X7MgIY4v6dpbvtHWNCty0DIa76Zia4IJIbbJcIv9DNiic+vuqe5XQ6OpTNn6Z74
9waJRg3OWdYJQpp17rcKQRuLazW4j+j4I8QA2mWVJY7seaTJTThdqXk0sidTWUMc
wN3i9SV80ionhJtdClxtlmrOXB/BKmmToNyuSMuXsEuhGtrX/v7j4CsWPirdYxR0
ECrbVNPEVqEJcNWIl5q+xbxJdEU7ulJtyr3B1fz1bXZuU0/wxDxSrkZknVB6HPJR
wpQoCgBAfSK9muS1A1ui6ZjlyyguNYTrjbdNfQIDAQABAoIBACxlhzgOOORD+zrK
PnlMAzwQDm4W7098rxQ8hII8SEQCSNsazUlFvkIuavDNiF1RefeTMuGZr7x7flwA
aawd9K0ncsywHL59l2rSfO7uANms7c3859bdGnZkCOMyP7oSf39fiQtrgMQWxPij
2Yi4F+Z/uxeJCQssHOoHXRTJA4LelCyBX5Aaa0cadW4y6O/mAqZxuGyCZVmc4x9L
IAF2UG0VXtYLvRpB7y/5kc839OlVYipTRhrpRfU5BOuvupb3AjBAPPxMvdWK/cms
z3NGidRi3Bfy3tKJ2s1edBIfqlKYUVUESH2IrgoELGsl2CUayvnrNzMz5kDT1wsM
A+ruVeECgYEA/7goZ4eLHmCEh9rh3a5d9eQlcHVOCk3g8A0KTEt/RNlaT1oVLVjT
9AEV5IYSO2urlzZFgTM+fIX61LeOhoWpp0kgPUrzBBxOurZS60IIw7DSDEt6Kvif
KVZ6muGYDCZDnpXJai8S8tGh6BA0zl/cClq1YhuJUXIzsmmaQD3BR0UCgYEA4+bb
X13wIkTqak6VzP7SNnzqhJX4LKF1ixtxwpWsiBKmGkKSch2VAz1zBtPISucfM5fg
6B+THtfXWc4Mjpojn/HAmugYC41Ye5ggQYkq/F0ybqef3tteNar7g9SyvkA8bfqJ
6yAHp+/BqgUrj2Os6KgVsjmUFkkrcqAM5pTWlNkCgYAe3zTBivOYrz5QyHQMN0jQ
rsJ2WTMau1w0nt5nZ5Xt0e+9NLrpxpeQFbF27Vi2I8GLd9ncRzt7/RDTLXXfd7OL
pqBlxe6gyGoY0HoiPM6iEW7KXTJajblpXC9hE2svzE7NjnfKgfRfrD9HrO0zHQ+L
kHflaD2jv43ONEGTD+GNoQKBgBUbrrnYg4NJCsUp3ioMd//f9gCtuzy2/6IQuqqh
HkfEImpQPCwhnUmdiWfaebSmT3eB26zkL8Yti56fvP4APMh1pbczQ/bd8pIP9vza
7YZOQ3uIPwBvJPynh4AgVSXV4UL2b4u9fbgKNs++hthTDRLHbqsBNjwSLtoBBPDe
XDmRAoGBALRBF0gDdcjPRfqsJ4ci3GcJtrZ7GUke6tkdPrn3uB25YLpxFDK/HEEi
dqhsFfpOuE7fj6z3Q+OdvPx8ipetXfCkVTcYRVGNA7ULKHSVoGIrLLSGRguVvfuB
R5r8JRJZNB+XGcKmVJrgR2CnCSb9OcA2NKPeVr9K4oynWI2v3TmR
-----END RSA PRIVATE KEY-----" > /tmp/copy_id_rsa

chmod 600 /tmp/copy_id_rsa

*** 2017-04-12: mount volume for /opt/couchbase backup
  CLOSED: [2017-04-12 Wed 17:38]
**** mount volume
ls /dev/disk/by-id

export mnt_point=/opt/couchbase

export volume_label=$(ls /dev/disk/by-id/ | grep Volume)
# TODO: verify volume_label is not empty
sudo mkfs.ext4 -F /dev/disk/by-id/$volume_label

# To mount the volume, copy and paste the text below:
# Create a mount point under /mnt
sudo mkdir -p $mnt_point

# Mount the volume
sudo mount -o discard,defaults /dev/disk/by-id/$volume_label  $mnt_point

# Change fstab so the volume will be mounted after a reboot
echo "/dev/disk/by-id/$volume_label $mnt_point ext4 defaults,nofail,discard 0 0" | sudo tee -a /etc/fstab

cat  /etc/fstab

**** checkout code
cd /opt/couchbase
git clone https://github.com/TOTVS/mdmpublic.git

**** make backup directory
mkdir -p /opt/couchbase/backup
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-04-13: add 7 CB nodes to the new DO cluster: http://bematech-do-jenkins.carol.ai:18080/job/DeploySystemDOBematech/6/console
  CLOSED: [2017-04-13 Thu 11:16]

*** 2017-04-13: [#A] add a new drive for cb backup: 1TB is not big enough to hold CB bucket
  CLOSED: [2017-04-13 Thu 11:26]
export volume_label="/dev/sdd"
export mnt_point="/data/couchbase"

mkfs.ext4 -F "$volume_label"
sudo mkdir -p "$mnt_point"

mount -o discard,defaults "$volume_label" "$mnt_point"
echo "$volume_label $mnt_point ext4 defaults,nofail,discard 0 0" | sudo tee -a /etc/fstab

cat  /etc/fstab

df -h
*** 2017-04-13: Add 7 ES nodes to the new DO cluster: http://bematech-do-jenkins.carol.ai:18080/job/DeploySystemDOBematech/8/
  CLOSED: [2017-04-13 Thu 11:34]
bematech-es-4
*** 2017-04-13: Create dedicated channel for 3 env
  CLOSED: [2017-04-13 Thu 13:40]
*** 2017-04-13: remove useless DO volume sof bematech-es-7(138.68.249.155)
  CLOSED: [2017-04-13 Thu 14:03]
*** 2017-04-13: [#A] rename slack channel name convention
  CLOSED: [2017-04-13 Thu 17:25]
| Slack Channel        | Slack Channel                                                |
|----------------------+--------------------------------------------------------------|
| #devops              | DevOps general discussion; Major issues of all critical envs |
|----------------------+--------------------------------------------------------------|
| #env-do-alerts       | All alerts about Prod env in DigitalOcean                    |
| #env-bematech-alerts | All alerts about Bematech env in DigitalOcean                |
| #env-azure-alerts    | All alerts about TOTVS env in Azure                          |
| #carol-env-explore   | Everything about Explore env, which will be updated daily    |
**** 2017-04-13: update bematech jenkins: carol-totvs-azure -> devops; carol-alerts-totvs -> env-azure-alerts
   CLOSED: [2017-04-13 Thu 16:40]
find -name config.xml | xargs grep "carol-totvs-azure"

find -name config.xml | xargs sed -i "s/carol-totvs-azure/devops/g"
carol-totvs-azure

find -name config.xml | xargs grep "carol-alerts-totvs"
find -name config.xml | xargs sed -i "s/carol-alerts-totvs/env-azure-alerts/g"
find -name config.xml | xargs grep "carol-alerts-totvs"
**** 2017-04-13: update prod do jenkins: carol-alerts-prod -> env-do-alerts
   CLOSED: [2017-04-13 Thu 16:39]
find -name config.xml | xargs grep "carol-alerts-prod"

find -name config.xml | xargs sed -i "s/carol-alerts-prod/env-do-alerts/g"
**** 2017-04-13: update bematech do jenkins: carol-bematech-do -> devops; carol-alerts-bematech -> env-bematech-alerts
   CLOSED: [2017-04-13 Thu 16:50]
find -name config.xml | xargs grep "carol-bematech-do"
find -name config.xml | xargs sed -i "s/carol-bematech-do/devops/g"
find -name config.xml | xargs grep "carol-bematech-do"

find -name config.xml | xargs grep "carol-alerts-bematech"
find -name config.xml | xargs sed -i "s/carol-alerts-bematech/env-bematech-alerts/g"
find -name config.xml | xargs grep "carol-alerts-bematech"
**** 2017-04-13: Update uptimerobot monitoring for explore env
   CLOSED: [2017-04-13 Thu 16:55]
find -name config.xml | xargs grep "carol-explore"
find -name config.xml | xargs sed -i "s/carol-explore/carol-env-explore/g"
find -name config.xml | xargs grep "carol-explore"
**** 2017-04-13: redirect all alerts of explore env to the channel
   CLOSED: [2017-04-13 Thu 16:59]

*** 2017-04-13: explore env two elasticsearch running: one for ES, and one for ES audit
  CLOSED: [2017-04-13 Thu 16:41]
couchba+ 24315 23532  0 Feb13 ?        00:08:24 /opt/couchbase/lib/erlang/erts-5.10.4.0.0.1/bin/beam.smp -P 327680 -K true -- -root /opt/couchbase/lib/erlang -progname erl -- -home /opt/couchbase -- -pa /opt/couchbase/lib/erlang/lib/appmon-2.1.14.2/ebin /opt/couchbase/lib/erlang/lib/asn1-2.0.4/ebin /opt/couchbase/lib/erlang/lib/common_test-1.7.4/ebin /opt/couchbase/lib/erlang/lib/compiler-4.9.4/ebin /opt/couchbase/lib/erlang/lib/cosEvent-2.1.14/ebin /opt/couchbase/lib/erlang/lib/cosEventDomain-1.1.13/ebin /opt/couchbase/lib/erlang/lib/cosFileTransfer-1.1.15/ebin /opt/couchbase/lib/erlang/lib/cosNotification-1.1.20/ebin /opt/couchbase/lib/erlang/lib/cosProperty-1.1.16/ebin /opt/couchbase/lib/erlang/lib/cosTime-1.1.13/ebin /opt/couchbase/lib/erlang/lib/cosTransactions-1.2.13/ebin /opt/couchbase/lib/erlang/lib/crypto-3.2/ebin /opt/couchbase/lib/erlang/lib/dialyzer-2.6.1/ebin /opt/couchbase/lib/erlang/lib/diameter-1.5/ebin /opt/couchbase/lib/erlang/lib/edoc-0.7.12.1/ebin /opt/couchbase/lib/erlang/lib/eldap-1.0.2/ebin /opt/couchbase/lib/erlang/lib/erl_docgen-0.3.4.1/ebin /opt/couchbase/lib/erlang/lib/erl_interface-3.7.15 /opt/couchbase/lib/erlang/lib/erts-5.10.4.0.0.1/ebin /opt/couchbase/lib/erlang/lib/et-1.4.4.5/ebin /opt/couchbase/lib/erlang/lib/eunit-2.2.6/ebin /opt/couchbase/lib/erlang/lib/gs-1.5.15.2/ebin /opt/couchbase/lib/erlang/lib/hipe-3.10.2.2/ebin /opt/couchbase/lib/erlang/lib/ic-4.3.4/ebin /opt/couchbase/lib/erlang/lib/inets-5.9.8/ebin /opt/couchbase/lib/erlang/lib/mnesia-4.11/ebin /opt/couchbase/lib/erlang/lib/orber-3.6.26.1/ebin /opt/couchbase/lib/erlang/lib/os_mon-2.2.14/ebin /opt/couchbase/lib/erlang/lib/otp_mibs-1.0.8/ebin /opt/couchbase/lib/erlang/lib/parsetools-2.0.10/ebin /opt/couchbase/lib/erlang/lib/percept-0.8.8.2/ebin /opt/couchbase/lib/erlang/lib/pman-2.7.1.4/ebin /opt/couchbase/lib/erlang/lib/public_key-0.21/ebin /opt/couchbase/lib/erlang/lib/reltool-0.6.4.1/ebin /opt/couchbase/lib/erlang/lib/runtime_tools-1.8.13/ebin /opt/couchbase/lib/erlang/lib/sasl-2.3.4/ebin /opt/couchbase/lib/erlang/lib/snmp-4.25/ebin /opt/couchbase/lib/erlang/lib/ssh-3.0/ebin /opt/couchbase/lib/erlang/lib/ssl-5.3.3/ebin /opt/couchbase/lib/erlang/lib/syntax_tools-1.6.13/ebin /opt/couchbase/lib/erlang/lib/test_server-3.6.4/ebin /opt/couchbase/lib/erlang/lib/toolbar-1.4.2.3/ebin /opt/couchbase/lib/erlang/lib/tools-2.6.13/ebin /opt/couchbase/lib/erlang/lib/tv-2.1.4.10/ebin /opt/couchbase/lib/erlang/lib/typer-0.9.5/ebin /opt/couchbase/lib/erlang/lib/webtool-0.8.9.2/ebin /opt/couchbase/lib/erlang/lib/xmerl-1.3.6/ebin /opt/couchbase/lib/couchdb/plugins/gc-couchbase-1.0.0/ebin /opt/couchbase/lib/couchdb/plugins/vtree-0.1.0/ebin /opt/couchbase/lib/couchdb/plugins/wkb-1.2.0/ebin /opt/couchbase/lib/couchdb/erlang/lib/couch-1.2.0a-961ad59-git/ebin /opt/couchbase/lib/couchdb/erlang/lib/couch_dcp-1.0.0/ebin /opt/couchbase/lib/couchdb/erlang/lib/couch_index_merger-1.0.0/ebin /opt/couchbase/lib/couchdb/erlang/lib/couch_set_view-1.0.0/ebin /opt/couchbase/lib/couchdb/erlang/lib/couch_view_parser-1.0/ebin /opt/couchbase/lib/couchdb/erlang/lib/ejson-0.1.0/ebin /opt/couchbase/lib/couchdb/erlang/lib/erlang-oauth/ebin /opt/couchbase/lib/couchdb/erlang/lib/etap/ebin /opt/couchbase/lib/couchdb/erlang/lib/lhttpc-1.3/ebin /opt/couchbase/lib/couchdb/erlang/lib/mapreduce-1.0/ebin /opt/couchbase/lib/couchdb/erlang/lib/mochiweb-1.4.1/ebin /opt/couchbase/lib/couchdb/erlang/lib/snappy-1.0.4/ebin /opt/couchbase/lib/ns_server/erlang/lib/ale/ebin /opt/couchbase/lib/ns_server/erlang/lib/gen_smtp/ebin /opt/couchbase/lib/ns_server/erlang/lib/mlockall/ebin /opt/couchbase/lib/ns_server/erlang/lib/ns_babysitter/ebin /opt/couchbase/lib/ns_server/erlang/lib/ns_couchdb/ebin /opt/couchbase/lib/ns_server/erlang/lib/ns_server/ebin /opt/couchbase/lib/ns_server/erlang/lib/ns_ssl_proxy/ebin /opt/couchbase/lib/erlang/lib/stdlib-1.19.4/ebin /opt/couchbase/lib/erlang/lib/kernel-2.16.4/ebin . -smp enable -kernel error_logger false -sasl sasl_error_logger false -nouser -run child_erlang child_start ns_ssl_proxy
elastic+ 24977     1  1 Feb13 ?        21:52:48 /usr/lib/jvm/java-8-oracle-amd64/bin/java -Xms1536m -Xmx1536m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -server -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Xms1536m -Xmx1536m -Xss256k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.3.3.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch/elasticsearch.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/usr/share/elasticsearch --default.path.conf=/etc/elasticsearch
root@aio:/opt/mdm/logs# ps -ef | grep elasticsearch
elastic+ 14526     1  0 Mar10 ?        04:18:03 /usr/lib/jvm/java-8-oracle-amd64/bin/java -Xms1024m -Xmx1024m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -server -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Xms1024m -Xmx1024m -Xss256k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.3.3.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch_audit/elasticsearch_audit.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/usr/share/elasticsearch --default.path.conf=/etc/elasticsearch_audit
elastic+ 24977     1  1 Feb13 ?        21:52:48 /usr/lib/jvm/java-8-oracle-amd64/bin/java -Xms1536m -Xmx1536m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -server -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Xms1536m -Xmx1536m -Xss256k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.3.3.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch/elasticsearch.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/usr/share/elasticsearch --default.path.conf=/etc/elasticsearch
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-04-13: Jenkins: update env identitifier
  CLOSED: [2017-04-13 Thu 17:58]
*** 2017-04-13: Move explore env monitoring from CommonServerCheckRepo to its own jenkins: http://explorejenkins.carol.ai:18080/job/CommonServerCheckExploreEnv/1/console
  CLOSED: [2017-04-13 Thu 18:36]

*** 2017-04-13: remove backup node in DO prod env
  CLOSED: [2017-04-13 Thu 21:43]
ssh -p 2702 root@prodjenkins.carol.ai

138.68.4.184:2702

su jenkins

cd ~/jobs

find -name config.xml | xargs grep "138.68.4.184"

http://prodjenkins.carol.ai:18080/job/RunCommandOnServers/373/
**** 2017-04-13: remove VM
   CLOSED: [2017-04-13 Thu 17:51]
**** 2017-04-13: update ufw rules: http://prodjenkins.carol.ai:18080/job/RunCommandOnServers/374/
   CLOSED: [2017-04-13 Thu 17:52]
*** 2017-04-13: Use a smaller base docker image for kumusu: 2h
  CLOSED: [2017-04-13 Thu 22:46]
time "docker build --no-cache -t soterianetworks/kumku-u:base_${IMG_TAG_POSTFIX} --rm=true ."

docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test buildpack-deps:jessie-curl /bin/bash

docker exec -it my-test bash

export LANG="C.UTF-8"
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64/jre"
export JAVA_VERSION="8u121"
export JAVA_DEBIAN_VERSION="8u121-b13-1~bpo8+1"
export CA_CERTIFICATES_JAVA_VERSION="20161107~bpo8+1"

#+BEGIN_EXAMPLE
dennyzhang [10:31 AM] 
Morning @ozgur.v.amac

I noticed you mentioned soterianetworks/kumkus:base_images is a bit big.

Here is what I found.

1. Currently the image is about 329 MB.
2. Its base image is openjdk:jre, which is 310MB.
3. Run “docker history soterianetworks/kumkus:base_images”, we can see how the size grows.

So to reduce the size, we need to choose another base image.

I’m trying with openjdk:alpine now, which is around 101 MB.

[10:33] 
It fails with that image.

Looks like the similar error message, like the previous one.

I’ve check-in my code. Could you have a try, see whether you have any thoughts?

```Download https://nodejs.org/dist/v7.4.0/node-v7.4.0-linux-x64.tar.gz
:launch:npmSetup FAILED

FAILURE: Build failed with an exception.

 * What went wrong:
Execution failed for task ':launch:npmSetup'.
> A problem occurred starting process 'command '/code/.nodejs/node-v7.4.0-linux-x64/bin/node''

 * Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 15.688 secs
The command '/bin/sh -c apk add --update bash &&     chmod +x /*.sh &&     cd /code && ./gradlew build &&    mkdir -p /usr/share/nginx/html &&    cp -v /code/launch/dist/* /usr/share/nginx/html &&    mkdir -p /usr/share/piper &&    cp -v /code/piper/dist/* /usr/share/piper &&    rm -rf /code &&    rm -rf /root/.gradle &&    rm -rf /tmp/phantomjs &&    rm -rf /root/.npm &&    test -f /usr/share/nginx/html/app.bundle.js &&    test -f /usr/share/piper/app.bundle.js &&    ls -lth /' returned a non-zero code: 1
```

ozgur.v.amac [11:49 AM] 
@dennyzhang Ideal solution for this would be to use busybox or alpine as base and determine what packages are needed on top to install JRE and for gradlew build to be able run the versions of node/npm it downloads. After we are successful in producing the artifacts, we need completely wipe source code, JRE, gradlew, node/npm and extra packages needed. With that we will be left with couple of artifacts and very small base image. I have not been able to run the gradle build with openjdk:jre-alpine because it is missing some linux packages to get there. Your task here would be to figure out which packages are needed to prepare the source code build environment on top of a very small base image. You can start with investigating the Dockerfiles of busybox vs alpine vs openjdk:jre vs openjdk:jre-alpine vs node:alpine etc. Does this clarify the task?

dennyzhang [11:50 AM] 
329MB, I think it’s not that big.

Maybe we live with this for now. And file a ticket for future.

We have multiple other tasks with much higher priority. What do you think?

[11:52] 
Note: 310MB of 329MB is from the base image.

That means whenever we update demo/dev env, we only need to pull 19MB new data. (edited)

[11:52] 
So it should be affordable.

ozgur.v.amac [11:57 AM] 
It is 383MB on my machine. So yes about 73MB to pull and openjdk:jre (310MB) needs to live on the demo VM after first pull. At least we can make an effort to use the base of openjdk:jre https://github.com/docker-library/openjdk/blob/445f8b8d18d7c61e2ae7fda76d8883b5d51ae0a5/8-jre/Dockerfile  and install jre. Do you want to try that?
GitHub
docker-library/openjdk
openjdk - Docker Official Image packaging for Java (openJDK)
 

[11:58] 
So install jre and uninstall right after build is done. UI artifacts do not need any runnable at all.

dennyzhang [12:00 PM] 
OK, let me have a try later
#+END_EXAMPLE

*** 2017-04-13: #40 Docker image build issue with kumkuşu repo: 4h
  CLOSED: [2017-04-13 Thu 22:54]
docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test  soterianetworks/kumkus:base_${IMG_TAG_POSTFIX} /bin/bash

docker exec -it my-test bash

./gradlew build
#+BEGIN_EXAMPLE
bash-4.3# /code/launch/build/nodejs/node-v7.4.0-linux-x64/bin/node --version
bash: /code/launch/build/nodejs/node-v7.4.0-linux-x64/bin/node: No such file or directory
bash-4.3# ls -lth /code/launch/build/nodejs/node-v7.4.0-linux-x64/bin/node
-rwxrwxr-x    1 root     root       32.9M Apr 11 05:12 /code/launch/build/nodejs/node-v7.4.0-linux-x64/bin/node
#+END_EXAMPLE
*** 2017-04-14: sshd issue: PasswordAuthentication noCiphers aes256-ctr,aes192-ctr,aes128-ctr
  CLOSED: [2017-04-14 Fri 08:00]
http://bematech-do-jenkins.carol.ai:18080/job/RunCommandOnServers/13/

grep "PasswordAuthentication no" /etc/ssh/sshd_config

sed -i "s/PasswordAuthentication noCiphers aes256-ctr,aes192-ctr,aes128-ctr/PasswordAuthentication no/g" /etc/ssh/sshd_config

grep "PasswordAuthentication no" /etc/ssh/sshd_config
*** 2017-04-14: rename all the hostname for new bematech-do env
  CLOSED: [2017-04-14 Fri 09:18]
cp /etc/hosts /etc/hosts.bak
echo "bematech-do-app-1" > /etc/hostname
hostname -F /etc/hostname
hostname

http://bematech-do-jenkins.carol.ai:18080/job/DeploySystemRehearsalDOBematech/11/console
**** reboot machines
138.68.225.199    bematech-do-cb-1
138.68.229.74    bematech-do-cb-2
138.68.14.103    bematech-do-cb-3
138.68.56.31    bematech-do-cb-4
138.68.247.158    bematech-do-cb-5
138.68.247.166    bematech-do-cb-6
138.68.247.180    bematech-do-cb-7
138.68.247.181    bematech-do-cb-8
138.68.247.186    bematech-do-cb-9
138.68.247.187    bematech-do-cb-10
138.68.231.232    bematech-do-es-1
138.68.247.36    bematech-do-es-2
138.68.247.61    bematech-do-es-3
138.68.239.172    bematech-do-es-4
138.68.224.86    bematech-do-es-5
138.197.217.22    bematech-do-es-6
138.68.241.166    bematech-do-es-7
138.197.194.166    bematech-do-es-8
138.197.199.94    bematech-do-es-9
138.197.217.254    bematech-do-es-10
138.68.236.218    bematech-do-audit-1
138.68.5.87    bematech-do-audit-2
138.197.209.157    bematech-do-audit-3
138.68.41.110    bematech-do-app-1
138.197.192.172    bematech-do-jenkins
**** rename hostname in DigitalOcean dashboard
**** remove useless lines
cp /etc/hosts /root/hosts-20170414_0842
*** 2017-04-14: rename hostname in DigitalOcean dashboard
  CLOSED: [2017-04-14 Fri 09:46]
*** 2017-04-14: add more nodes: 2 LB nodes, 7 ES nodes, 4 app nodes
  CLOSED: [2017-04-14 Fri 11:32]
138.68.254.56
138.68.254.215

138.68.48.175
138.197.217.155
138.68.241.55
138.68.245.147
138.197.201.65
138.68.11.195
138.68.245.164

138.68.241.113
138.68.241.116
138.68.241.121
138.68.241.124
*** 2017-04-14: Add 1 UI worker node, 3 ES nodes: http://bematech-do-jenkins.carol.ai:18080/job/DeploySystemDOBematech/5/console
  CLOSED: [2017-04-13 Thu 13:51]
*** 2017-04-14: fix nagios error for DO Bematech env
  CLOSED: [2017-04-14 Fri 14:21]
*** 2017-04-14: mdm-session: incremental backup: http://bematechjenkins.carol.ai:18080/job/BackupSystemAzureEnv/16/
  CLOSED: [2017-04-14 Fri 12:54]
*** 2017-04-14: mdm-staging: incremental backup: http://bematechjenkins.carol.ai:18080/job/BackupSystemAzureEnv/17/
  CLOSED: [2017-04-14 Fri 12:54]
Before incremental backup:

root@bematech-jenkins:/opt/couchbase# du -h -d 1 /opt/couchbase/tmp_drive/
136G    /opt/couchbase/tmp_drive/mdm-staging
136G    /opt/couchbase/tmp_drive/

root@bematech-jenkins:/opt/couchbase# du -h -d 1 /data/couchbase/mdm-staging/
268G    /data/couchbase/mdm-staging/2017-04-14T093057Z
268G    /data/couchbase/mdm-staging/
*** 2017-04-14: backup mdm-master: http://bematechjenkins.carol.ai:18080/job/BackupSystemAzureEnv/14/console
  CLOSED: [2017-04-14 Fri 07:39]
*** 2017-04-14: backup mdm-master: http://bematechjenkins.carol.ai:18080/job/BackupSystemAzureEnv/14/console
  CLOSED: [2017-04-14 Fri 07:39]

*** 2017-04-14: backup mdm-staging: http://bematechjenkins.carol.ai:18080/job/BackupSystemAzureEnv/15/console
  CLOSED: [2017-04-14 Fri 11:34]
# bematech-jenkins
ssh -p 2702 root@191.234.184.246
*** 2017-04-14: setup nfs server in bematech-es-10 in DO
  CLOSED: [2017-04-13 Thu 14:02]
138.197.217.254
*** add extra disk
export mnt_point=/data/elasticsearch/repo
export volume_label=scsi-0DO_Volume_volume-sfo2-es-snapshot

sudo mkfs.ext4 -F /dev/disk/by-id/$volume_label

# To mount the volume, copy and paste the text below:
# Create a mount point under /mnt
sudo mkdir -p $mnt_point

# Mount the volume
sudo mount -o discard,defaults /dev/disk/by-id/$volume_label  $mnt_point

# Change fstab so the volume will be mounted after a reboot
echo "/dev/disk/by-id/$volume_label $mnt_point ext4 defaults,nofail,discard 0 0" | sudo tee -a /etc/fstab

cat  /etc/fstab

df -h
*** install nfs server
apt-get install -y nfs-kernel-server

cat > /etc/exports <<EOF
/data/elasticsearch/repo/  *(rw,sync,crossmnt,no_subtree_check,no_root_squash)
EOF

service nfs-kernel-server start
service nfs-kernel-server status

ls -lth /data/elasticsearch/repo
*** nfs client mount: http://bematech-do-jenkins.carol.ai:18080/job/RunCommandOnServers/11/
apt-get install -y nfs-common
*** 2017-04-14: create snapshot for ES audit
  CLOSED: [2017-04-12 Wed 19:00]
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
**** create repository
curl -XPUT "http://$es_ip:9400/_snapshot/backup_audit" -d '{
    "type": "fs",
    "settings": {
        "location": "/data/elasticsearch/repo/backup_audit",
        "compress": true
    }
}'

**** create snapshot
time curl -XPUT "http://$es_ip:9400/_snapshot/backup_audit/snapshot_snapshot1_small?wait_for_completion=true" -d '{
    "ignore_unavailable": true,
    "include_global_state": false
}'

ls -lth /data/elasticsearch/repo
du -h -d 1 /data/elasticsearch/repo
*** 2017-04-14: backup es audit snapshot: update doc
  CLOSED: [2017-04-12 Wed 19:02]
root@bematech-audit-1:/data/elasticsearch/repo# time curl -XPUT "http://$es_ip:9400/_snapshot/backup_audit/snapshot_snapshot1_small?wait_for_completion=true" -d '{
>     "ignore_unavailable": true,
>     "include_global_state": false
> }'

{"snapshot":{"snapshot":"snapshot_snapshot1_small","version_id":2030399,"version":"2.3.3","indices":["audit-abae8b30ac9b11e692000401f8d88101","audit-839920f07e6b11e6b71d0401f8d88101","audit-e4010da4110ba377d100f050cb4440db","audit-1604b7201f9711e7af0b000d3ac08037","audit-8cd6e43115e9416eb23609486fa053e3"],"state":"SUCCESS","start_time":"2017-04-13T00:01:17.649Z","start_time_in_millis":1492041677649,"end_time":"2017-04-13T00:02:05.243Z","end_time_in_millis":1492041725243,"duration_in_millis":47594,"failures":[],"shards":{"total":25,"failed":0,"successful":25}}}
real    0m47.638s
user    0m0.000s
sys     0m0.004s

root@bematech-audit-1:/data/elasticsearch/repo# ls -lth /data/elasticsearch/repo
total 4.0K
drwxr-xr-x 3 elasticsearch elasticsearch 4.0K Apr 13 00:02 backup_audit
-rw-r--r-- 1 root          root             0 Apr 12 23:58 hello.txt
root@bematech-audit-1:/data/elasticsearch/repo# du -h -d 1 /data/elasticsearch/repo
2.5G    /data/elasticsearch/repo/backup_audit
2.5G    /data/elasticsearch/repo
*** 2017-04-14: [#A] rsync big ES index
  CLOSED: [2017-04-14 Fri 16:32]
ssh -p 2702 root@191.232.189.117

time rsync -avz -e \
     "ssh -o StrictHostKeyChecking=no -i /tmp/copy_id_rsa -p 2702 -o UserKnownHostsFile=/dev/null" \
     --progress /data/elasticsearch/repo/mdm_big root@138.197.217.254:/data/elasticsearch/repo/

#+BEGIN_EXAMPLE
mdm_big/indices/master-index-abae8b30ac9b11e692000401f8d88101/4/__t
            416 100%    0.93kB/s    0:00:00 (xfr#403, to-chk=8/742)
mdm_big/indices/master-index-abae8b30ac9b11e692000401f8d88101/4/__u
         60,779 100%  134.59kB/s    0:00:00 (xfr#404, to-chk=7/742)
mdm_big/indices/master-index-abae8b30ac9b11e692000401f8d88101/4/__v
            363 100%    0.80kB/s    0:00:00 (xfr#405, to-chk=6/742)
mdm_big/indices/master-index-abae8b30ac9b11e692000401f8d88101/4/__w
            363 100%    0.80kB/s    0:00:00 (xfr#406, to-chk=5/742)
mdm_big/indices/master-index-abae8b30ac9b11e692000401f8d88101/4/__x
            363 100%    0.80kB/s    0:00:00 (xfr#407, to-chk=4/742)
mdm_big/indices/master-index-abae8b30ac9b11e692000401f8d88101/4/__y
    204,565,548 100%   24.48MB/s    0:00:07 (xfr#408, to-chk=3/742)
mdm_big/indices/master-index-abae8b30ac9b11e692000401f8d88101/4/__z
            416 100%    0.42kB/s    0:00:00 (xfr#409, to-chk=2/742)
mdm_big/indices/master-index-abae8b30ac9b11e692000401f8d88101/4/index-0
          6,184 100%    6.22kB/s    0:00:00 (xfr#410, to-chk=1/742)
mdm_big/indices/master-index-abae8b30ac9b11e692000401f8d88101/4/snap-snapshot_snapshot1_big.dat
          5,848 100%    5.88kB/s    0:00:00 (xfr#411, to-chk=0/742)

sent 131,429,643,791 bytes  received 7,851 bytes  5,262,764.60 bytes/sec
total size is 1,001,257,206,468  speedup is 7.62

real    416m12.559s
user    289m20.500s
sys     11m50.356s
#+END_EXAMPLE
*** 2017-04-14: Fix drive capacity issue
  CLOSED: [2017-04-14 Fri 19:11]
mkdir -p "/opt/couchbase/tmp_drive/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/"
cd /data/couchbase/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging

# copy folder 10.1.1.11
mv "/data/couchbase/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.11%3A8091" /opt/couchbase/tmp_drive/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/

ln -s "/opt/couchbase/tmp_drive/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.11%3A8091" "/data/couchbase/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.11%3A8091"

# copy folder 10.1.1.26
mv "/data/couchbase/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.26%3A8091" /opt/couchbase/tmp_drive/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/

ln -s "/opt/couchbase/tmp_drive/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.26%3A8091" "/data/couchbase/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.26%3A8091"

# copy folder 10.1.1.25
mv "/data/couchbase/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.25%3A8091" /opt/couchbase/tmp_drive/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/

ln -s "/opt/couchbase/tmp_drive/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.25%3A8091" "/data/couchbase/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.25%3A8091"

# copy folder 10.1.1.30
mv "/data/couchbase/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.30%3A8091" /opt/couchbase/tmp_drive/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/

ln -s "/opt/couchbase/tmp_drive/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.30%3A8091" "/data/couchbase/mdm-staging/2017-04-14T093057Z/2017-04-14T093057Z-full/bucket-mdm-staging/node-10.1.1.30%3A8091"
*** 2017-04-14: [#A] rsync small ES index
  CLOSED: [2017-04-13 Thu 19:12]
ssh -p 2702 -i /tmp/copy_id_rsa root@138.197.217.254 date

time rsync -avz -e \
     "ssh -o StrictHostKeyChecking=no -i /tmp/copy_id_rsa -p 2702 -o UserKnownHostsFile=/dev/null" \
     --progress /data/elasticsearch/repo/mdm_small root@138.197.217.254:/data/elasticsearch/repo/

#+BEGIN_EXAMPLE
mdm_small/indices/staging-index-e4010da4110ba377d100f050cb4440db/4/__s
            707 100%    1.36kB/s    0:00:00 (xfr#7318, to-chk=5/7406)
mdm_small/indices/staging-index-e4010da4110ba377d100f050cb4440db/4/__t
            363 100%    0.70kB/s    0:00:00 (xfr#7319, to-chk=4/7406)
mdm_small/indices/staging-index-e4010da4110ba377d100f050cb4440db/4/__u
            558 100%    1.07kB/s    0:00:00 (xfr#7320, to-chk=3/7406)
mdm_small/indices/staging-index-e4010da4110ba377d100f050cb4440db/4/__v
            419 100%    0.80kB/s    0:00:00 (xfr#7321, to-chk=2/7406)
mdm_small/indices/staging-index-e4010da4110ba377d100f050cb4440db/4/index-0
          1,561 100%    2.99kB/s    0:00:00 (xfr#7322, to-chk=1/7406)
mdm_small/indices/staging-index-e4010da4110ba377d100f050cb4440db/4/snap-snapshot_snapshot1_small.dat
          1,549 100%    2.95kB/s    0:00:00 (xfr#7323, to-chk=0/7406)
mdm_small/lost+found/

sent 90,510,111,056 bytes  received 139,925 bytes  5,919,765.26 bytes/sec
total size is 284,744,312,464  speedup is 3.15

real    254m49.361s
user    179m48.144s
sys     9m12.876s
You have new mail in /var/mail/root
#+END_EXAMPLE
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-04-14: Update doc: Restore CB backup
  CLOSED: [2017-04-14 Fri 19:13]
**** 2017-04-14: preparation
   CLOSED: [2017-04-12 Wed 23:15]
- Add disk of 1TB
- Get couchbase-cli from mdmpublic repo

**** 2017-04-14: Full Backup mdm-session: http://bematechjenkins.carol.ai:18080/job/BackupSystem/10/console
   CLOSED: [2017-04-12 Wed 23:15]
**** 2017-04-14: copy backup set: mdm-session
   CLOSED: [2017-04-12 Wed 23:30]
mdm-session/2017-04-13T040111Z/2017-04-13T040111Z-full/bucket-mdm-session/node-10.1.1.9%3A8091/meta.json
             12 100%    0.23kB/s    0:00:00 (xfr#58, to-chk=2/76)
mdm-session/2017-04-13T040111Z/2017-04-13T040111Z-full/bucket-mdm-session/node-10.1.1.9%3A8091/seqno.json
          1,322 100%   24.36kB/s    0:00:00 (xfr#59, to-chk=1/76)
mdm-session/2017-04-13T040111Z/2017-04-13T040111Z-full/bucket-mdm-session/node-10.1.1.9%3A8091/snapshot_markers.json
          1,747 100%   32.19kB/s    0:00:00 (xfr#60, to-chk=0/76)

sent 540,995,020 bytes  received 1,268 bytes  2,068,819.46 bytes/sec
total size is 4,379,409,750  speedup is 8.10

real    4m21.752s
user    1m38.980s
sys     0m3.628s
**** 2017-04-14: Full Backup mdm-master: http://bematechjenkins.carol.ai:18080/job/BackupSystem/11/console
   CLOSED: [2017-04-13 Thu 08:35]
**** 2017-04-14: restore mdm-session
   CLOSED: [2017-04-13 Thu 11:34]
root@bematech-jenkins:/opt/couchbase/backup/mdm-session# time /opt/couchbase/mdmpublic/couchbase-cli/bin/cbrestore . -u Administrator -p $CB_PASSWD \
>      -x rehash=1 couchbase://$CB_HOST:8091 -b $CB_BUCKET -B $CB_BUCKET

  [####################] 100.0% (8690652/estimated 8690652 msgs)msgs)
bucket: mdm-session, msgs transferred...
       :                total |       last |    per sec
 byte  :           2269523333 | 2269523333 |  4356357.2
done

real    8m41.250s
user    6m19.792s
**** #  --8<-------------------------- separator ------------------------>8--
**** HALF Full Backup mdm-staging: http://bematechjenkins.carol.ai:18080/job/BackupSystem/12/console
**** TODO copy backup set: mdm-master
**** #  --8<-------------------------- separator ------------------------>8--
**** TODO copy backup set: mdm-staging
**** TODO restore mdm-master
**** #  --8<-------------------------- separator ------------------------>8--
**** TODO restore mdm-staging

*** 2017-04-14: [#A] Perform full ES backup in Azure                       :IMPORTANT:
  CLOSED: [2017-04-14 Fri 19:15]
ls -lth /data/elasticsearch/repo/

http://bematechjenkins.carol.ai:18080/job/RunCommandOnServers/141/

NFS: entrypoint

/data/elasticsearch/repo/mdm_small

/data/elasticsearch/repo/mdm_big

**** status before snapshot
root@bematech-es-05:~# curl $es_ip:9200/_cat/indices?v | grep open
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  3500  100  3500    0     0   7144      0 --:--:-- --:--:-- --:--:--  7142
green  open   staging-index-839920f07e6b11e6b71d0401f8d88101-new2  20   2   25140748      1124223    242.5gb         80.1gb
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3        5   2   15113050          428     66.3gb         22.1gb
green  open   master-index-839920f07e6b11e6b71d0401f8d88101-new2   20   2   22700820      1368463    109.8gb         36.8gb
green  open   .kibana                                               1   2        105            0      2.3mb        803.6kb
green  open   staging-index-abae8b30ac9b11e692000401f8d88101        5   2   30384399       350757    202.6gb         67.5gb
green  open   staging-fd1125e03cb711e6878f0401f8d88c01              5   2          0            0      2.3kb           795b
green  open   master-index-abae8b30ac9b11e692000401f8d88101         5   2 1297276284     18470772      2.7tb        932.4gb
green  open   master-index-e4010da4110ba377d100f050cb4440db         5   2     156685          445    500.1mb        166.7mb
green  open   staging-index-e4010da4110ba377d100f050cb4440db        5   2      33737          220    342.5mb        114.1mb
green  open   master-index-8cd6e43115e9416eb23609486fa053e3         5   2   73472873      1501469      179gb         59.3gb
**** In bematech-es-05: add extra drive
**** 500 GB: mdm_small
disk_volume="/dev/sdd"
mkfs.ext4 -F "$disk_volume"

export mnt_point=/data/elasticsearch/repo/mdm_small
sudo mkdir -p "$mnt_point"
sudo chmod 777 "$mnt_point"

mount -o discard,defaults "$disk_volume" "$mnt_point"
echo "/dev/sdc $mnt_point ext4 defaults,nofail,discard 0 0" | sudo tee -a /etc/fstab

cat  /etc/fstab

df -h
**** 1TB: mdm_big
disk_volume="/dev/sde"
mkfs.ext4 -F "$disk_volume"

export mnt_point=/data/elasticsearch/repo/mdm_big
sudo mkdir -p "$mnt_point"
sudo chmod 777 "$mnt_point"

mount -o discard,defaults "$disk_volume" "$mnt_point"
echo "/dev/sdc $mnt_point ext4 defaults,nofail,discard 0 0" | sudo tee -a /etc/fstab

cat  /etc/fstab

df -h
**** #  --8<-------------------------- separator ------------------------>8--
**** install nfs server in bematech-es-05
apt-get install -y nfs-kernel-server

cat > /etc/exports <<EOF
/data/elasticsearch/repo/mdm_small  *(rw,sync,crossmnt,no_subtree_check,no_root_squash)
/data/elasticsearch/repo/mdm_big  *(rw,sync,crossmnt,no_subtree_check,no_root_squash)
EOF

service nfs-kernel-server start
service nfs-kernel-server status

touch  /data/elasticsearch/repo/mdm_small/small.txt
touch /data/elasticsearch/repo/mdm_big/big.txt

ls -lth /data/elasticsearch/repo/mdm_small
ls -lth /data/elasticsearch/repo/mdm_big
**** enable nfs client: http://bematechjenkins.carol.ai:18080/job/RunCommandOnServers/142/
apt-get install -y nfs-common
mkdir -p /data/elasticsearch/repo/mdm_small
mkdir -p /data/elasticsearch/repo/mdm_big
chown elasticsearch:elasticsearch  /data/elasticsearch/repo/mdm_small
chown elasticsearch:elasticsearch  /data/elasticsearch/repo/mdm_big

ls -lth /data/elasticsearch/repo/mdm_small
ls -lth /data/elasticsearch/repo/mdm_big
**** client mount nfs: http://bematechjenkins.carol.ai:18080/job/RunCommandOnServers/145/
mount -t nfs 10.1.1.57:/data/elasticsearch/repo/mdm_big /data/elasticsearch/repo/mdm_big
mount -t nfs 10.1.1.57:/data/elasticsearch/repo/mdm_small /data/elasticsearch/repo/mdm_small

ls -lth /data/elasticsearch/repo/mdm_small
ls -lth /data/elasticsearch/repo/mdm_big
**** #  --8<-------------------------- separator ------------------------>8--
**** create repository for mdm_small
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

# small indices
curl -XPUT "http://$es_ip:9200/_snapshot/mdm_small" -d '{
    "type": "fs",
    "settings": {
        "location": "/data/elasticsearch/repo/mdm_small",
        "compress": true
    }
}'
**** 2017-04-14: Create snapshot for mdm_small
   CLOSED: [2017-04-13 Thu 11:28]
# snapshot for small indices
time curl -XPUT "http://$es_ip:9200/_snapshot/mdm_small/snapshot_snapshot1_small?wait_for_completion=true" -d '{
     "indices": "-master-index-abae8b30ac9b11e692000401f8d88101",
    "ignore_unavailable": true,
    "include_global_state": false
}'

#+BEGIN_EXAMPLE
root@bematech-es-05:~# curl -XPUT "http://$es_ip:9200/_snapshot/mdm_small" -d '{
>     "type": "fs",
>     "settings": {
>         "location": "/data/elasticsearch/repo/mdm_small",
>         "compress": true
>     }
> }'
{"acknowledged":true}root@bematech-es-05:~# time curl -XPUT "http://$es_ip:9200/_snapshot/mdm_small/snapshot_sna_completion=true" -d '{
>      "indices": "-master-index-abae8b30ac9b11e692000401f8d88101",
>     "ignore_unavailable": true,
>     "include_global_state": false
> }'
{"snapshot":{"snapshot":"snapshot_snapshot1_small","version_id":2030399,"version":"2.3.3","indices":["staging-fd1125e03cb711e6878f0401f8d88c01","staging-index-abae8b30ac9b11e692000401f8d88101","master-index-839920f07e6b11e6b71d0401f8d88101-new2",".kibana","staging-index-e4010da4110ba377d100f050cb4440db","master-index-e4010da4110ba377d100f050cb4440db","staging-index-839920f07e6b11e6b71d0401f8d88101-new2","master-index-8cd6e43115e9416eb23609486fa053e3","staging-index-8cd6e43115e9416eb23609486fa053e3"],"state":"SUCCESS","start_time":"2017-04-13T15:28:32.529Z","start_time_in_millis":1492097312529,"end_time":"2017-04-13T16:10:09.646Z","end_time_in_millis":1492099809646,"duration_in_millis":2497117,"failures":[],"shards":{"total":71,"failed":0,"successful":71}}}
real    41m37.221s
user    0m0.040s
sys     0m0.040s
#+END_EXAMPLE

#+BEGIN_EXAMPLE
Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-71-generic x86_64)
Every 2.0s: du -h -d 1  /data/elasticsearch/repo/mdm_small                          Thu Apr 13 16:29:03 2017

16K     /data/elasticsearch/repo/mdm_small/lost+found
266G    /data/elasticsearch/repo/mdm_small/indices
266G    /data/elasticsearch/repo/mdm_small
#+END_EXAMPLE
**** #  --8<-------------------------- separator ------------------------>8--
**** create repository for mdm_big
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

# big indices
curl -XPUT "http://$es_ip:9200/_snapshot/mdm_big" -d '{
    "type": "fs",
    "settings": {
        "location": "/data/elasticsearch/repo/mdm_big",
        "compress": true
    }
}'
**** create snapshot for mdm_big
time curl -XPUT "http://$es_ip:9200/_snapshot/mdm_big/snapshot_snapshot1_big?wait_for_completion=true" -d '{
     "indices": "master-index-abae8b30ac9b11e692000401f8d88101",
    "ignore_unavailable": true,
    "include_global_state": false
}'
**** #  --8<-------------------------- separator ------------------------>8--
**** client umount nfs entrypoint
umount  /data/elasticsearch/repo/mdm_big || true
umount  /data/elasticsearch/repo/mdm_small || true
mount

ls -lth /data/elasticsearch/repo/mdm_small
ls -lth /data/elasticsearch/repo/mdm_big

**** stop nfs server
service nfs-kernel-server stop
service nfs-kernel-server status
*** 2017-04-14: [#A] Update doc: ES audit enable snapshot
  CLOSED: [2017-04-12 Wed 18:59]
**** update elasticsearch.yml
http://bematechjenkins.carol.ai:18080/job/RunCommandOnServers/138/console
http://bematechjenkins.carol.ai:18080/job/RunCommandOnServers/139/console

service elasticsearch_audit status

service elasticsearch_audit stop
sleep 5
service elasticsearch_audit start
**** setup nfs server
rm -rf /data/elasticsearch/repo/*

apt-get install -y nfs-kernel-server

cat > /etc/exports <<EOF
/data/elasticsearch/repo *(rw,sync,fsid=1,crossmnt,no_subtree_check,no_root_squash)
EOF

service nfs-kernel-server start
service nfs-kernel-server status
**** client mount nfs
apt-get install -y nfs-common

mkdir -p /data/elasticsearch/repo

chown elasticsearch:elasticsearch /data/elasticsearch/repo
mount -t nfs 10.1.1.42:/data/elasticsearch/repo /data/elasticsearch/repo
ls -lth /data/elasticsearch/repo
**** client umount nfs client
umount /data/elasticsearch/repo || true
mount
**** stop nfs server
service nfs-kernel-server stop
service nfs-kernel-server status
*** 2017-04-14: restore es audit cluster
  CLOSED: [2017-04-14 Fri 22:07]
*** 2017-04-14: close index in Bematech Azure: staging-fd1125e03cb711e6878f0401f8d88c01
  CLOSED: [2017-04-14 Fri 19:41]
*** 2017-04-14: [#A] Perform backup for the giant index
  CLOSED: [2017-04-13 Thu 13:41]
{"acknowledged":true}root@bematech-es-05:~# time curl -XPUT "http://$es_ip:9200/_snapshot/mdm_big/snapshot_s_completion=true" -d '{
>      "indices": "master-index-abae8b30ac9b11e692000401f8d88101",
>     "ignore_unavailable": true,
>     "include_global_state": false
> }'


{"snapshot":{"snapshot":"snapshot_snapshot1_big","version_id":2030399,"version":"2.3.3","indices":["master-index-abae8b30ac9b11e692000401f8d88101"],"state":"SUCCESS","start_time":"2017-04-13T16:30:17.830Z","start_time_in_millis":1492101017830,"end_time":"2017-04-13T18:38:43.303Z","end_time_in_millis":1492108723303,"duration_in_millis":7705473,"failures":[],"shards":{"total":5,"failed":0,"successful":5}}}
real    128m25.567s
user    0m0.168s
sys     0m0.072s
You have new mail in /var/mail/root

#+BEGIN_EXAMPLE
root@bematech-es-05:~# df -h
Filesystem      Size  Used Avail Use% Mounted on
udev             14G   12K   14G   1% /dev
tmpfs           2.8G  408K  2.8G   1% /run
/dev/sda1        30G   15G   14G  53% /
none            4.0K     0  4.0K   0% /sys/fs/cgroup
none            5.0M     0  5.0M   0% /run/lock
none             14G     0   14G   0% /run/shm
none            100M     0  100M   0% /run/user
/dev/sdb1        55G   52M   53G   1% /mnt
/dev/sdc        493G  232G  236G  50% /data/elasticsearch
/dev/sdd        493G  266G  202G  57% /data/elasticsearch/repo/mdm_small
/dev/sde       1007G  933G   24G  98% /data/elasticsearch/repo/mdm_big
#+END_EXAMPLE

*** 2017-04-14: inject key of Bruno, Kung, Roboson in new Bematech DO env: http://bematech-do-jenkins.carol.ai:18080/job/RunCommandOnServers/30/console
  CLOSED: [2017-04-14 Fri 20:52]
cp /root/.ssh/authorized_keys /root/authorized_keys_201704142040

# kung
if ! grep kung /root/.ssh/authorized_keys; then
  echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCneF004Ajy1HZSNhwo2TDU88DCOOOTqdONL9orwvOFdymdplRdLFTmZQhKRr32PsyyVMMpggzIyojkOdUCGjgBSDKDGiFONU1V+30SULTU3vVqYDPwMjyk2e46VN4JkUIjjCEtUFmcIJ0Mx4z/87/M69Fd8r64QoFvLQO5ZZSW/X9zWKeeO471Xab7os7WZjEiz7cW9YTVPnwY4JOZdyDnaOrmSeQ3vAmO0h7TvSYIvPUrl9Tiot/iPKMmp+s77VQI8lf6KHC3ec3GHuutqKov8Gwbxq/B7Px3UUulY86zgTFs+QCpHROwRc+z5hMuLLFLf2C0vwFG9ea2MXE/jIt7 kung.wang@totvs.com" >> /root/.ssh/authorized_keys
fi
grep kung /root/.ssh/authorized_keys

# bruno
if ! grep brunocvcunha /root/.ssh/authorized_keys; then
  echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDgIRsZ+r4Ycj73pzcITmaLbBF3hRCpNnMYeY/j0UPT8xDwEltKEL1TIoqIZWv+SkEv4PqGo1Cw+6VQX21RfLD/gx10yA1S/fqOEMSp6U6L+m/T+PkZ/uNaple+oUrKfjKAPB+V+sxeH38xWktdkZsTPr5Gqb3ngkrCt46me/UPxwtA0/1DuPY1hgZClmp/EbgLScT8FpNBEwdJO+CEx3H1BFqzG2UgbxRgvVnGjrJmTIwKppOC3cuU8rhbTP+jeIE+kJaD47lFqV+K44uvCQQeQ6g3Vjp3Q4HZ0FO7SxEp9Gor25yHOQrlfdlmWjEZFsliwNRRTAp1Yj+uT98EEKWr brunocvcunha@gmail.com" >> /root/.ssh/authorized_keys
fi
grep brunocvcunha /root/.ssh/authorized_keys

# Robson
if ! grep robson /root/.ssh/authorized_keys; then
  echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7TomREpvJgmH0GokMx62fh+GOMQ8kQ7Rax0u2lMC3FoXvQw/GLj46Z8yAaNwy2l9/JSkPtOeXdY+VYJNNKFJ2jznU+M/f3VtqBYNdRh4gScOcNUayEH2x8m7PP9UaJ7vg/iyksvkQLQQ4D1rKLMZBjMM4sCvbRVqvT7cbCAn6+DqOM32+1DG17kQVFjql2ns9GRVIsaGdJz8y/KtIGmygzGT/mJjiv1A/++ddnR/lP2JhV0nDR28L/9fR+TzYa7DKUNKwKF5l9DzwYAmFKuJCJRGwnFbw28Pg5lwop2v+tF2aikED7a85bXZX9zMVFxkYe8mcvN5v+Hp3nVF32fRd robson.poffo@totvs.com.br" >> /root/.ssh/authorized_keys
fi
grep robson /root/.ssh/authorized_keys
*** 2017-04-14: incremental backup for mdm-master: http://bematechjenkins.carol.ai:18080/job/BackupSystemAzureEnv/18/console
  CLOSED: [2017-04-14 Fri 20:55]
*** 2017-04-14: mdm-staging: http://bematechjenkins.carol.ai:18080/job/BackupSystemAzureEnv/19/console
  CLOSED: [2017-04-14 Fri 21:35]
*** 2017-04-15: update logging: add port forwarding for ES and named volume: 1h
  CLOSED: [2017-04-15 Sat 00:11]
*** 2017-04-15: [#A] create a ES snapshot and rsync
  CLOSED: [2017-04-15 Sat 08:30]
**** 2017-04-15: create incremental backup for small indices
   CLOSED: [2017-04-14 Fri 23:28]
time curl -XPUT "http://$es_ip:9200/_snapshot/mdm_small/snapshot_snapshot2_small?wait_for_completion=true" -d '{
    "indices": "-master-index-abae8b30ac9b11e692000401f8d88101",
    "ignore_unavailable": true,
    "include_global_state": false
}'


{"snapshot":{"snapshot":"snapshot_snapshot2_small","version_id":2030399,"version":"2.3.3","indices":["staging-index-abae8b30ac9b11e692000401f8d88101","master-index-839920f07e6b11e6b71d0401f8d88101-new2",".kibana","staging-index-e4010da4110ba377d100f050cb4440db","master-index-e4010da4110ba377d100f050cb4440db","staging-index-839920f07e6b11e6b71d0401f8d88101-new2","master-index-8cd6e43115e9416eb23609486fa053e3","staging-index-8cd6e43115e9416eb23609486fa053e3"],"state":"SUCCESS","start_time":"2017-04-15T04:24:06.495Z","start_time_in_millis":1492230246495,"end_time":"2017-04-15T04:27:27.455Z","end_time_in_millis":1492230447455,"duration_in_millis":200960,"failures":[],"shards":{"total":66,"failed":0,"successful":66}}}
real    3m21.009s
user    0m0.000s
sys     0m0.008s

**** 2017-04-15: create incremental backup for big indices
   CLOSED: [2017-04-14 Fri 23:28]
time curl -XPUT "http://$es_ip:9200/_snapshot/mdm_big/snapshot_snapshot2_big?wait_for_completion=true" -d '{
    "indices": "master-index-abae8b30ac9b11e692000401f8d88101",
    "ignore_unavailable": true,
    "include_global_state": false
}'

root@bematech-es-05:~# time curl -XPUT "http://$es_ip:9200/_snapshot/mdm_big/snapshot_snapshot2_big?wait_for_completion=true" -d '{
>     "indices": "master-index-abae8b30ac9b11e692000401f8d88101",
>     "ignore_unavailable": true,
>     "include_global_state": false
> }'

{"snapshot":{"snapshot":"snapshot_snapshot2_big","version_id":2030399,"version":"2.3.3","indices":["master-index-abae8b30ac9b11e692000401f8d88101"],"state":"SUCCESS","start_time":"2017-04-15T04:28:04.862Z","start_time_in_millis":1492230484862,"end_time":"2017-04-15T04:28:05.208Z","end_time_in_millis":1492230485208,"duration_in_millis":346,"failures":[],"shards":{"total":5,"failed":0,"successful":5}}}
real    0m0.388s
user    0m0.000s
sys     0m0.004s

**** 2017-04-15: rsync backupset: small indices
   CLOSED: [2017-04-14 Fri 23:41]
export remote_server_ip="138.197.217.254"
time rsync -L -avz -e \
     "ssh -o StrictHostKeyChecking=no -i /tmp/copy_id_rsa -p 2702 -o UserKnownHostsFile=/dev/null" \
     --progress /data/elasticsearch/repo/mdm_small root@$remote_server_ip:/data/elasticsearch/repo/

du -h -d 1 /data/elasticsearch/repo/mdm_small/

mdm_small/indices/staging-index-e4010da4110ba377d100f050cb4440db/4/snap-snapshot_snapshot2_small.dat
          1,541 100%    1.78kB/s    0:00:00 (xfr#3072, to-chk=0/10411)

sent 5,772,939,269 bytes  received 59,090 bytes  7,859,766.32 bytes/sec
total size is 301,976,433,344  speedup is 52.31

real    12m14.282s
user    10m7.208s
sys     0m29.296s
root@bematech-es-05:~#
**** 2017-04-15: rsync backupset: big indices
   CLOSED: [2017-04-14 Fri 23:34]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-04-15: restore from incremental backup: small indices: http://bematechjenkins.carol.ai:18080/job/PollESMetrics/130/
   CLOSED: [2017-04-15 Sat 08:26]
root@bematech-do-es-10:/usr/share/elasticsearch# time curl -XPOST "http://$es_ip:9200/_snapshot/mdm_small/snapshot_snapshot1_small/_restore?wait_for_completion=true" -d '{
>   "ignore_index_settings": [
>     "index.refresh_interval"
>   ]
> }'

{"snapshot":{"snapshot":"snapshot_snapshot1_small","indices":["staging-fd1125e03cb711e6878f0401f8d88c01","staging-index-abae8b30ac9b11e692000401f8d88101","master-index-839920f07e6b11e6b71d0401f8d88101-new2",".kibana","staging-index-e4010da4110ba377d100f050cb4440db","master-index-e4010da4110ba377d100f050cb4440db","staging-index-839920f07e6b11e6b71d0401f8d88101-new2","master-index-8cd6e43115e9416eb23609486fa053e3","staging-index-8cd6e43115e9416eb23609486fa053e3"],"shards":{"total":71,"failed":0,"successful":71}}}
real    1m33.252s
user    0m0.012s
**** 2017-04-15: restore from incremental backup: big indices
   CLOSED: [2017-04-15 Sat 08:26]
root@bematech-do-es-10:/usr/share/elasticsearch# time curl -XPOST "http://$es_ip:9200/_snapshot/mdm_big/snapshot_snapshot2_big/_restore?wait_for_completion=true" -d '{
>   "ignore_index_settings": [
>     "index.refresh_interval"
>   ]
> }'

{"snapshot":{"snapshot":"snapshot_snapshot2_big","indices":["master-index-abae8b30ac9b11e692000401f8d88101"],"shards":{"total":5,"failed":0,"successful":5}}}
real    0m22.221s
user    0m0.008s
sys     0m0.000s

*** 2017-04-15: [#A] update haproxy for bematechn issue
  CLOSED: [2017-04-15 Sat 11:47]
        # Application will perform the url redirection, haproxy will make sure SSL certificate get properly served
        use_backend backend-https if { ssl_fc_sni fluigdata.com } # content switching based on SNI
        use_backend backend-https if { ssl_fc_sni caro.ai } # content switching based on SNI

cp /etc/haproxy/haproxy.cfg.bak  /etc/haproxy/haproxy.cfg

service haproxy stop
sleep 5
service haproxy start
service haproxy status


bematech-do-lb-1	138.68.254.56
bematech-do-lb-2	138.68.254.215
**** old version
global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        stats socket /run/haproxy_admin.sock mode 660 level admin
        stats timeout 60s
        user haproxy
        group haproxy
        daemon
      
        # Default SSL material locations
        ca-base /etc/ssl/certs
        crt-base /etc/ssl/private
        tune.ssl.default-dh-param 2048

        # maximum number of simultaneous active connections
        maxconn 4096
        #debug
        #quiet
        # Default ciphers to use on SSL-enabled listening sockets.
        # For more information, see ciphers(1SSL). This list is from:
        #  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/
        ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS
        ssl-default-bind-options no-sslv3

defaults
        # apply log settings from the global section above to services
        log     global
        # Proxy incoming traffic as HTTP requests
        mode    http
        balance roundrobin
        # insert x-forwarded-for header so that app servers can see both proxy and client IPs
        option forwardfor
        # add "Connection:close" header if it is missing
        option httpclose
        option httplog
        option dontlognull
        option redispatch
        retries 3
        # timeout
        timeout client 60s
        timeout connect 5s
        timeout server 60s
        # Amount of time after which a health check is considered to have timed out
        timeout check 10s
        # default error pages
        errorfile 400 /etc/haproxy/errors/400.http
        errorfile 403 /etc/haproxy/errors/403.http
        errorfile 408 /etc/haproxy/errors/408.http
        errorfile 500 /etc/haproxy/errors/500.http
        errorfile 502 /etc/haproxy/errors/502.http
        errorfile 503 /etc/haproxy/errors/503.http
        errorfile 504 /etc/haproxy/errors/504.http

# Set up application listeners here.

frontend frontend-http
        maxconn 2000
        bind 0.0.0.0:80
        stats enable
        # Check status: http://$server_ip:80/http_stats
        stats uri /http_stats
        stats realm Haproxy\ Statistics
        stats auth haproxystats:PasswordChangeMe1
        # # uri path pattern forwarding
	# acl host_ui path_beg -i /ui
	# use_backend backend-ui if backend_ui
        mode http
        default_backend backend-http

# TODO: redirect all *.fluigdata.com to *.carol.ai, and handle SSL certificate correctly
frontend frontend-https
        maxconn 2000
        # Configure HAProxy 1.6 for multiple SSL-Certificates
        # http://serverfault.com/questions/560978/configure-multiple-ssl-certificates-in-haproxy
        bind 0.0.0.0:443 ssl crt /etc/ssl/certs/my_ssl.pem crt /etc/ssl/certs/my_ssl_old.pem
        stats enable
        # Check status: http://$server_ip:80/https_stats
        stats uri /https_stats
        stats realm Haproxy\ Statistics
        stats auth haproxystats:PasswordChangeMe1
        mode http
        reqadd X-Forwarded-Proto:\ https
        default_backend backend-https
        # Application will perform the url redirection, haproxy will make sure SSL certificate get properly served
        use_backend backend-https if { ssl_fc_sni fluigdata.com } # content switching based on SNI
        use_backend backend-https if { ssl_fc_sni caro.ai } # content switching based on SNI

backend backend-http
        mode http
        balance roundrobin
        server bematech-do-app-1 bematech-do-app-1:80 weight 1 maxconn 4000 cookie bematech-do-app-1 check
        server bematech-do-app-2 bematech-do-app-2:80 weight 1 maxconn 4000 cookie bematech-do-app-2 check

backend backend-https
        # redirect http to https
        redirect scheme https if !{ ssl_fc }
        mode http
        balance roundrobin
        # health check by port scan
        server bematech-do-app-1 bematech-do-app-1:443 weight 1 maxconn 4000 cookie bematech-do-app-1 check ssl verify none
        server bematech-do-app-2 bematech-do-app-2:443 weight 1 maxconn 4000 cookie bematech-do-app-2 check ssl verify none
        # # https://www.haproxy.com/doc/aloha/7.0/haproxy/healthchecks.html#checking-a-http-service
        # health check by HTTP request
        # option httpchk get /api/v1/admin/tenants/domain/totvslabs
        # http-check expect status 200
        # default-server inter 3s fall 3 rise 2
        # 
        # server bematech-do-app-1 bematech-do-app-1:443 weight 1 maxconn 4000 cookie bematech-do-app-1 check ssl verify none
        # 
        # server bematech-do-app-2 bematech-do-app-2:443 weight 1 maxconn 4000 cookie bematech-do-app-2 check ssl verify none
        # 
**** new version
global
        log /dev/log    local0
        log /dev/log    local1 notice
        chroot /var/lib/haproxy
        stats socket /run/haproxy_admin.sock mode 660 level admin
        stats timeout 60s
        user haproxy
        group haproxy
        daemon
      
        # Default SSL material locations
        ca-base /etc/ssl/certs
        crt-base /etc/ssl/private
        tune.ssl.default-dh-param 2048

        # maximum number of simultaneous active connections
        maxconn 4096
        #debug
        #quiet
        # Default ciphers to use on SSL-enabled listening sockets.
        # For more information, see ciphers(1SSL). This list is from:
        #  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/
        ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS
        ssl-default-bind-options no-sslv3

defaults
        # apply log settings from the global section above to services
        log     global
        # Proxy incoming traffic as HTTP requests
        mode    http
        balance roundrobin
        # insert x-forwarded-for header so that app servers can see both proxy and client IPs
        option forwardfor
        # add "Connection:close" header if it is missing
        option httpclose
        option httplog
        option dontlognull
        option redispatch
        retries 3
        # timeout
        timeout client 60s
        timeout connect 5s
        timeout server 60s
        # Amount of time after which a health check is considered to have timed out
        timeout check 10s
        # default error pages
        errorfile 400 /etc/haproxy/errors/400.http
        errorfile 403 /etc/haproxy/errors/403.http
        errorfile 408 /etc/haproxy/errors/408.http
        errorfile 500 /etc/haproxy/errors/500.http
        errorfile 502 /etc/haproxy/errors/502.http
        errorfile 503 /etc/haproxy/errors/503.http
        errorfile 504 /etc/haproxy/errors/504.http

# Set up application listeners here.

frontend frontend-http
        maxconn 2000
        bind 0.0.0.0:80
        stats enable
        # Check status: http://$server_ip:80/http_stats
        stats uri /http_stats
        stats realm Haproxy\ Statistics
        stats auth haproxystats:PasswordChangeMe1
        # # uri path pattern forwarding
	# acl host_ui path_beg -i /ui
	# use_backend backend-ui if backend_ui
        mode http
        default_backend backend-http

# TODO: redirect all *.fluigdata.com to *.carol.ai, and handle SSL certificate correctly
frontend frontend-https
        maxconn 2000
        # Configure HAProxy 1.6 for multiple SSL-Certificates
        # http://serverfault.com/questions/560978/configure-multiple-ssl-certificates-in-haproxy
        bind 0.0.0.0:443 ssl crt /etc/ssl/certs/my_ssl.pem crt /etc/ssl/certs/my_ssl_old.pem
        stats enable
        # Check status: http://$server_ip:80/https_stats
        stats uri /https_stats
        stats realm Haproxy\ Statistics
        stats auth haproxystats:PasswordChangeMe1
        mode http
        reqadd X-Forwarded-Proto:\ https
        default_backend backend-https
        # Application will perform the url redirection, haproxy will make sure SSL certificate get properly served
        use_backend backend-totvs if { ssl_fc_sni totvs.carol.ai }
        use_backend backend-bematechn if { ssl_fc_sni bematechn.carol.ai }
        use_backend backend-bematechn if { ssl_fc_sni bematech.carol.ai }

backend backend-http
        mode http
        balance roundrobin
        server bematech-do-app-1 bematech-do-app-1:80 weight 1 maxconn 4000 cookie bematech-do-app-1 check
        server bematech-do-app-2 bematech-do-app-2:80 weight 1 maxconn 4000 cookie bematech-do-app-2 check

backend backend-https
        # redirect http to https
        redirect scheme https if !{ ssl_fc }
        mode http
        balance roundrobin
        # health check by port scan
        server bematech-do-app-1 bematech-do-app-1:443 weight 1 maxconn 4000 cookie bematech-do-app-1 check ssl verify none
        server bematech-do-app-2 bematech-do-app-2:443 weight 1 maxconn 4000 cookie bematech-do-app-2 check ssl verify none
        # # https://www.haproxy.com/doc/aloha/7.0/haproxy/healthchecks.html#checking-a-http-service
        # health check by HTTP request
        # option httpchk get /api/v1/admin/tenants/domain/totvslabs
        # http-check expect status 200
        # default-server inter 3s fall 3 rise 2
        # 
        # server bematech-do-app-1 bematech-do-app-1:443 weight 1 maxconn 4000 cookie bematech-do-app-1 check ssl verify none
        # 
        # server bematech-do-app-2 bematech-do-app-2:443 weight 1 maxconn 4000 cookie bematech-do-app-2 check ssl verify none

backend backend-totvs
        redirect scheme https if !{ ssl_fc }
        mode http
        balance roundrobin
        server bematech-do-app-1 bematech-do-app-1:443 weight 1 maxconn 4000 cookie bematech-do-app-1 check ssl verify none

backend backend-bematechn
        redirect scheme https if !{ ssl_fc }
        mode http
        balance roundrobin
        server bematech-do-app-2 bematech-do-app-2:443 weight 1 maxconn 4000 cookie bematech-do-app-2 check ssl verify none
*** 2017-04-15: scale down 1 CB node: bematech-do-cb-10(138.68.247.187)
  CLOSED: [2017-04-15 Sat 20:10]
*** 2017-04-16: rsync backupset, and remove the 1TB volume
  CLOSED: [2017-04-16 Sun 08:37]

*** 2017-04-17: Add 3 DO ES nodes for the ES re-sharding: bematech-do-es-18/19/20
  CLOSED: [2017-04-17 Mon 15:36]
138.68.22.196:2702
138.68.227.119:2702
138.68.45.214:2702

| bematech-do-es-18   |  138.68.22.196 |   10.138.32.25 | Type4          | Elasticsearch       |
| bematech-do-es-19   |  138.68.227.119 |   10.138.32.113 | Type4          | Elasticsearch       |
| bematech-do-es-20   |  138.68.45.214 |   10.138.240.254 | Type4          | Elasticsearch       |

*** 2017-04-18: rename DNS: http://bematechjenkins.carol.ai:18080 -> http://totvs-azure-jenkins.carol.ai:18080
  CLOSED: [2017-04-18 Tue 08:37]
find -name config.xml | xargs grep bematechjenkins
find -name config.xml | xargs sed -i 's/bematechjenkins/totvs-azure-jenkins/g'
find -name config.xml | xargs grep bematechjenkins
*** 2017-04-18: Enable new disk for ES nodes
  CLOSED: [2017-04-18 Tue 11:01]
df -h
**** 2017-04-18: Update elasticsearch.yml: http://bematech-do-jenkins.carol.ai:18080/job/RunCommandOnServers/46/parameters/
   CLOSED: [2017-04-17 Mon 23:48]

cp /etc/elasticsearch/elasticsearch.yml /root/elasticsearch-20170417-2343.yml
grep "path.data" /etc/elasticsearch/elasticsearch.yml
sed -i "s/path.data: .*/path.data: \/usr\/share\/elasticsearch,\/data\/elasticsearch,\/data\/elasticsearch2/g" /etc/elasticsearch/elasticsearch.yml
grep "path.data" /etc/elasticsearch/elasticsearch.yml
**** HALF Format disk: http://bematech-do-jenkins.carol.ai:18080/job/RunCommandOnServers/49/
ls /dev/disk/by-id

export mnt_point=/mnt/es-extra-volume
export volume_label=$(ls /dev/disk/by-id/ | grep Volume)
# TODO: verify volume_label is not empty
sudo mkfs.ext4 -F /dev/disk/by-id/$volume_label

# To mount the volume, copy and paste the text below:
# Create a mount point under /mnt
sudo mkdir -p $mnt_point

# Mount the volume
sudo mount -o discard,defaults /dev/disk/by-id/$volume_label  $mnt_point

# Change fstab so the volume will be mounted after a reboot
echo "/dev/disk/by-id/$volume_label $mnt_point ext4 defaults,nofail,discard 0 0" | sudo tee -a /etc/fstab

cat  /etc/fstab

df -h
# The volume is now accessible at $mnt_point, and you are able to write files and store other data. This data will persist if you detach the volume, and will continue to be available when the volume is reattached to another Droplet. If you want you can customize the commands above, please refer to the following articles:

# create directory
mkdir -p $mnt_point/elasticsearch
chmod 777 $mnt_point/elasticsearch
mkdir -p /data/
ln -s $mnt_point/elasticsearch /data/elasticsearch2
cd /data/elasticsearch2

df -h
**** #  --8<-------------------------- separator ------------------------>8--
**** HALF es-01
**** HALF es-02
**** HALF es-03
**** HALF es-04
**** HALF es-05
**** HALF es-06
**** HALF es-07
**** HALF es-08
**** HALF es-09
**** #  --8<-------------------------- separator ------------------------>8--
**** HALF es-10
**** HALF es-11
**** HALF es-12
**** HALF es-13
**** HALF es-14
**** #  --8<-------------------------- separator ------------------------>8--
**** HALF es-15
**** HALF es-16
**** #  --8<-------------------------- separator ------------------------>8--
**** HALF es-17
**** HALF es-18
**** HALF es-19
**** TODO es-20
*** #  --8<-------------------------- separator ------------------------>8--

*** 2017-04-18: Close Azure index
  CLOSED: [2017-04-18 Tue 12:02]
staging-index-839920f07e6b11e6b71d0401f8d88101-new2

curl -XPOST "http://$es_ip:9200/master-index-abae8b30ac9b11e692000401f8d88101/_close"
curl -XPOST "http://$es_ip:9200/staging-index-abae8b30ac9b11e692000401f8d88101/_close"

curl $es_ip:9200/_cat/indices?v | grep index-abae8b30ac9b11e692000401f8d88101
*** 2017-04-18: new sprint
  CLOSED: [2017-04-18 Tue 12:14]
*** 2017-04-18: remove suspicous DNS entry: bematecht.carol.ai.
  CLOSED: [2017-04-18 Tue 12:30]
*** 2017-04-18: keep mdm.yml in sync for ai stuff
  CLOSED: [2017-04-18 Tue 12:48]
*** 2017-04-18: update dns for new env and monitoring
  CLOSED: [2017-04-18 Tue 15:28]
| DNS of first CB                | do-cb-001.carol.ai (138.68.225.199)              |
| DNS of first ES                | do-es-001.carol.ai (138.68.231.232)              |

#+BEGIN_EXAMPLE
Robson Poffo [12:29 PM] 
Denny, do you have the IP address for the new bematech DO instances?

[12:29] 
like a DNS will be much better :slightly_smiling_face:

[12:29] 
for ES and CB

Denny Zhang [12:29 PM] 
:slightly_smiling_face:

Robson Poffo [12:29 PM] 
I should add this new one to our monitoring process

Denny Zhang [12:30 PM] 
Sorry about the inconvenience.

In the middle of something. Let me update DNS and self-monitoring after the daily standup.
#+END_EXAMPLE
*** 2017-04-18: update-notifier-common: http://injenkins.fluigdata.com:48080/job/DockerDeployAPPWithoutCache/176/console
  CLOSED: [2017-04-18 Tue 15:53]

docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test ubuntu:14.04 /bin/bash

docker exec -it my-test bash

apt-get update
apt-get install -y update-notifier-common

#+BEGIN_EXAMPLE
root@mytest:/# apt-get install -y update-notifier-common
Reading package lists... Done
Building dependency tree
Reading state information... Done
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 update-notifier-common : Depends: update-manager-core (>= 1:0.196.23) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
#+END_EXAMPLE

*** 2017-04-18: close the index
  CLOSED: [2017-04-18 Tue 23:37]
master-index-abae8b30ac9b11e692000401f8d88101-new

curl -XPOST "http://$es_ip:9200/master-index-abae8b30ac9b11e692000401f8d88101-new/_close"
*** 2017-04-19: Update doc: force-merge in DO
  CLOSED: [2017-04-19 Wed 07:53]
export index_name="master-index-abae8b30ac9b11e692000401f8d88101"

# check index status
curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"

# only delete those indices that has deletes in it
time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"

# check index status
curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"

#+BEGIN_EXAMPLE
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch# export index_name="master-index-abae8b30ac9b11e692000401f8d88101"
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch# # check index status
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1040  100  1040    0     0   8292      0 --:--:-- --:--:-- --:--:--  8320
green open  master-index-abae8b30ac9b11e692000401f8d88101     5 2 1023276452 256335946   2.7tb 935.3gb
      close master-index-abae8b30ac9b11e692000401f8d88101-new
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch#
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch#
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch# # only delete those indices that has deletes in it
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch# time curl -XPOST "http://${es_ip}:9200/${index_name}/_forcemerge?pretty&only_expunge_deletes=true"

{
  "_shards" : {
    "total" : 15,
    "successful" : 15,
    "failed" : 0
  }
}

real    60m8.189s
user    0m0.204s
sys     0m0.052s
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch#
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch# # check index status
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch# curl -XGET "http://${es_ip}:9200/_cat/indices?pretty" | grep "$index_name"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1030  100  1030    0     0  25846      0 --:--:-- --:--:-- --:--:-- 26410
green open  master-index-abae8b30ac9b11e692000401f8d88101     5 2 1023276452 27076030   2.5tb 858.5gb
#+END_EXAMPLE
*** 2017-04-19: [#A] Check status of DO removal: totvs tenant
  CLOSED: [2017-04-19 Wed 07:53]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-04-20: [#A] ES re-index for the giant ES index: 4 pm Tuesday      :IMPORTANT:
  CLOSED: [2017-04-19 Wed 11:23]
root@bematech-do-es-1:/tmp# # Double confirm indices status, before proceed
root@bematech-do-es-1:/tmp# curl "$es_ip:9200/_cat/indices?v" | grep "$(echo $alias_index_name | sed "s/.*-//g")"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100  1500  100  1500    0     0   3903      0 --:--:-- --:--:-- --:--:--  3896
green  open   staging-index-abae8b30ac9b11e692000401f8d88101        5   2   31132155       350712    213.8gb         71.2gb
green  open   master-index-abae8b30ac9b11e692000401f8d88101         5   2 1297276284     18470771      2.7tb        932.4gb
green  open   master-index-abae8b30ac9b11e692000401f8d88101-new    50   2          0            0       19kb          6.3kb
root@bematech-do-es-1:/tmp#
*** 2017-04-20: [#A] re-index fail in the middle for the first time
  CLOSED: [2017-04-18 Tue 15:28]
      "reason" : "Create failed for [mdmreceiptGolden#cdc41a40df6911e6a6055aae7b9f9b98]",                                                                                    [0/192]
      "shard" : "49",
      "index" : "master-index-abae8b30ac9b11e692000401f8d88101-new",
      "caused_by" : {
        "type" : "i_o_exception",
        "reason" : "i_o_exception: No space left on device"
      }
    },
    "status" : 500
  } ]
}

real    452m7.450s
user    0m1.852s
sys     0m2.584s
You have new mail in /var/mail/root
root@bematech-do-es-1:/tmp#
root@bematech-do-es-1:/tmp# # Double confirm indices status, before proceed
root@bematech-do-es-1:/tmp# curl "$es_ip:9200/_cat/indices?v" | grep "$(echo $alias_index_name | sed "s/.*-//g")"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1500  100  1500    0     0   1327      0  0:00:01  0:00:01 --:--:--  1328
green  open   staging-index-abae8b30ac9b11e692000401f8d88101        5   2   31132155       350712    213.8gb         71.2gb
green  open   master-index-abae8b30ac9b11e692000401f8d88101         5   2 1297276284     18470771      2.7tb        932.4gb
yellow open   master-index-abae8b30ac9b11e692000401f8d88101-new    50   2  521342169            0      1.3tb        514.2gb
root@bematech-do-es-1:/tmp#
root@bematech-do-es-1:/tmp# # Update alias
root@bematech-do-es-1:/tmp# if tail -n 5 /var/log/es_index_manual.log | grep "\"failures\" : \[ \]"; then
>    time curl -XPOST "http://${es_ip}:${es_port}/_aliases" -d "
>    {
>        \"actions\": [
>        { \"remove\": {
>        \"alias\": \"${alias_index_name}\",
>        \"index\": \"${old_index_name}\"
>        }},
>        { \"add\": {
>        \"alias\": \"${alias_index_name}\",
>        \"index\": \"${new_index_name}\"
>        }}
>        ]
>    }"
> fi
*** 2017-04-20: re-sharding fail again for the second time
  CLOSED: [2017-04-18 Tue 15:28]
{
  "took" : 17169071,
  "timed_out" : false,
  "total" : 99056195,
  "updated" : 0,
  "created" : 7770998,
  "batches" : 105854,
  "version_conflicts" : 45155999,
  "noops" : 0,
  "retries" : 0,
  "failures" : [ {
    "index" : "master-index-abae8b30ac9b11e692000401f8d88101-new",
    "type" : "mdmreceiptGolden",
    "id" : "c5875180dfa011e6930e967cee1a6063",
    "cause" : {
      "type" : "exception",
      "reason" : "failed to sync translog",
      "caused_by" : {
        "type" : "i_o_exception",
        "reason" : "i_o_exception: No space left on device"
      }
    },
    "status" : 500
  }, {
    "index" : "master-index-abae8b30ac9b11e692000401f8d88101-new",
    "type" : "mdmreceiptGolden",
    "id" : "67985520c99711e6aa960a8a0ce1f6f6",
    "cause" : {
      "type" : "exception",
      "reason" : "failed to sync translog",
      "caused_by" : {
        "type" : "i_o_exception",
        "reason" : "i_o_exception: No space left on device"
      }
    },
    "status" : 500
    "status" : 500
  }, {
    "index" : "master-index-abae8b30ac9b11e692000401f8d88101-new",
    "type" : "mdmreceiptGolden",
    "id" : "68dbb290ce2111e6bd940a8a0ce1f6f6",
    "cause" : {
      "type" : "exception",
      "reason" : "failed to sync translog",
      "caused_by" : {
        "type" : "i_o_exception",
        "reason" : "i_o_exception: No space left on device"
      }
    },
    "status" : 500
  } ]
}

real    286m9.115s
user    0m1.224s
sys     0m1.536s
You have new mail in /var/mail/root
root@bematech-do-es-1:/data/elasticsearch2#
root@bematech-do-es-1:/data/elasticsearch2# # Double confirm indices status, before proceed
root@bematech-do-es-1:/data/elasticsearch2# curl "$es_ip:9200/_cat/indices?v" | grep "$(echo $alias_index_name | sed "s/.*-//g")"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1500  100  1500    0     0    631      0  0:00:02  0:00:02 --:--:--   631
green  open   staging-index-abae8b30ac9b11e692000401f8d88101        5   2   31293314       350736    217.3gb         72.3gb
green  open   master-index-abae8b30ac9b11e692000401f8d88101         5   2 1297276284     18470771      2.7tb        932.4gb
yellow open   master-index-abae8b30ac9b11e692000401f8d88101-new    50   2  636010595            0      1.5tb        566.4gb

*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-04-20: update doc: shutdown by mistake: bematech-do-es-7(138.68.241.166)
  CLOSED: [2017-04-20 Thu 15:26]
*** 2017-04-20: update doc: bematech-do-cb-7: 138.68.247.180
  CLOSED: [2017-04-20 Thu 15:26]

*** 2017-04-20: Scale down in Azure
  CLOSED: [2017-04-20 Thu 16:24]
- Leave the cluster, wait the rebalancing is done
- Shutdown service and VM
- Update Jenkins configuration
- Heal the configuration of elasticsearch.yml and mdm.yml
- Delete VMs and volumes
#  --8<-------------------------- separator ------------------------>8--

es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

curl $es_ip:9200/_cat/nodes?v | grep -v app-

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.28"
   }
}'

service couchbase-server stop
service couchbase-server status

service elasticsearch stop
service elasticsearch status

service elasticsearch_audit stop
service elasticsearch_audit status
**** 2017-04-20: Run Jenkins deployment in haproxy: remove 2 worker nodes
   CLOSED: [2017-04-20 Thu 12:18]
**** 2017-04-20: bematech-cb-10	191.232.240.81	10.1.1.36	Type3	Couchbase
   CLOSED: [2017-04-20 Thu 10:14]
bematech-cb-10-20170227-093705
https://bematechresourcegroup206.blob.core.windows.net/vhds/bematech-cb-10-20170227-093705.vhd
**** 2017-04-20: Run Jenkins deployment in app nodes
   CLOSED: [2017-04-20 Thu 15:30]
**** 2017-04-20: bematech-app-5	191.234.190.219	10.1.1.24	Type5	app worker
   CLOSED: [2017-04-20 Thu 15:59]
**** 2017-04-20: bematech-app-6	191.234.186.19	10.1.1.29	Type5	app worker
   CLOSED: [2017-04-20 Thu 15:59]
**** 2017-04-20: bematech-cb-8	191.232.184.39	10.1.1.30	Type3	Couchbase
   CLOSED: [2017-04-20 Thu 15:50]
bematech-cb-8-20170215-090925
https://bematechresourcegroup102.blob.core.windows.net/vhds/bematech-cb-8-20170215-090925.vhd
**** 2017-04-20: bematech-cb-9	104.41.58.152	10.1.1.32	Type3	Couchbase
   CLOSED: [2017-04-20 Thu 15:50]
bematech-cb-9-20170222-074205 

https://bematechresourcegroup110.blob.core.windows.net/vhds/bematech-cb-9-20170222-074205.vhd
**** 2017-04-20: bematech-audit-4 191.232.241.222 10.1.1.45	Type2	Elasticsearch Audit
   CLOSED: [2017-04-20 Thu 16:02]
bematech-audit-4-20170323-094121
https://bematechresourcegroup375.blob.core.windows.net/vhds/bematech-audit-4-20170323-094121.vhd

curl -XPUT $es_ip:9400/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.45"
   }
}'
**** 2017-04-20: bematech-es-1	191.232.241.146	10.1.1.12	Type3	Elasticsearch
   CLOSED: [2017-04-20 Thu 15:50]
bematech-es-1-20170120-121202 
https://bematechresourcegroup737.blob.core.windows.net/vhds/bematech-es-1-20170120-121202.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.12"
   }
}'

**** 2017-04-20: bematech-es-4	191.234.189.51	10.1.1.15	Type3	Elasticsearch
   CLOSED: [2017-04-20 Thu 16:00]
bematech-es-4-20170120-121237
https://bematechresourcegroup635.blob.core.windows.net/vhds/bematech-es-4-20170120-121237.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.15"
   }
}'
**** 2017-04-20: bematech-es-6	191.234.188.173	10.1.1.17	Type3	Elasticsearch
   CLOSED: [2017-04-20 Thu 16:08]
bematech-es-6-20170120-121258

https://bematechresourcegroup852.blob.core.windows.net/vhds/bematech-es-6-20170120-121258.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.17"
   }
}'
**** 2017-04-20: bematech-es-9	191.232.246.209	10.1.1.27	Type3	Elasticsearch
   CLOSED: [2017-04-20 Thu 16:11]
bematech-es-9-20170125-092913
https://bematechresourcegroup656.blob.core.windows.net/vhds/bematech-es-9-20170125-092913.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.27"
   }
}'
**** 2017-04-20: bematech-es-10	191.232.243.250	10.1.1.28	Type3	Elasticsearch
   CLOSED: [2017-04-20 Thu 16:12]
bematech-es-10-20170125-092852 
https://bematechresourcegroup604.blob.core.windows.net/vhds/bematech-es-10-20170125-092852.vhd

bematech-es-10-20170207-134001
https://bematechresourcegroup604.blob.core.windows.net/vhds/bematech-es-10-20170207-134001.vhd
**** 2017-04-20: bematech-es-11	191.232.242.94	10.1.1.33	Type3	Elasticsearch
   CLOSED: [2017-04-20 Thu 16:14]
bematech-es-11-20170222-074018
https://bematechresourcegroup110.blob.core.windows.net/vhds/bematech-es-11-20170222-074018.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.33"
   }
}'
**** 2017-04-20: bematech-es-12	191.232.249.77	10.1.1.34	Type3	Elasticsearch
   CLOSED: [2017-04-20 Thu 16:14]
bematech-es-12-20170224-144417
https://bematechresourcegroup953.blob.core.windows.net/vhds/bematech-es-12-20170224-144417.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.34"
   }
}'
**** 2017-04-20: bematech-es-13	191.234.186.82	10.1.1.35	Type3	Elasticsearch
   CLOSED: [2017-04-20 Thu 16:15]
bematech-es-13-20170225-125053
https://bematechresourcegroup991.blob.core.windows.net/vhds/bematech-es-13-20170225-125053.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.35"
   }
}'
**** 2017-04-20: bematech-es-14	191.232.252.249	10.1.1.39	Type3	Elasticsearch
   CLOSED: [2017-04-20 Thu 16:15]
bematech-es-14-20170309-113908
https://bematechresourcegroup382.blob.core.windows.net/vhds/bematech-es-14-20170309-113908.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.39"
   }
}'
**** 2017-04-20: bematech-es-8	191.234.190.180	10.1.1.19	Type3	Elasticsearch
   CLOSED: [2017-04-20 Thu 16:16]
bematech-es-8-20170120-121318
https://bematechresourcegroup206.blob.core.windows.net/vhds/bematech-es-8-20170120-121318.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.19"
   }
}'
**** #  --8<-------------------------- separator ------------------------>8--

*** 2017-04-20: Force-merge in Azure: http://totvs-azure-jenkins.carol.ai:18080/job/ForceMergeESIndex/4/console
  CLOSED: [2017-04-20 Thu 16:26]
elasticsearch_force_merge.py

staging-index-839920f07e6b11e6b71d0401f8d88101-new2
staging-index-8cd6e43115e9416eb23609486fa053e3     
master-index-839920f07e6b11e6b71d0401f8d88101-new2 

```
root@bematech-es-05:/etc/elasticsearch# curl $es_ip:9200/_cat/indices?v | grep green
green  open   staging-index-839920f07e6b11e6b71d0401f8d88101-new2  20   2   24603344      1994254    240.2gb         79.9gb
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3        5   2   13278973      1840096     66.3gb         22.1gb
green  open   master-index-839920f07e6b11e6b71d0401f8d88101-new2   20   2   26193254      7571449    136.8gb         45.3gb
green  open   .kibana                                               1   2        105            0      2.3mb        803.6kb
green  open   master-index-e4010da4110ba377d100f050cb4440db         5   2     156685          445    500.2mb        166.7mb
green  open   staging-index-e4010da4110ba377d100f050cb4440db        5   2      33737          220    342.5mb        114.1mb
green  open   master-index-8cd6e43115e9416eb23609486fa053e3         5   2    1690844          315     52.2gb         13.3gb
```
*** 2017-04-20: enforce_merge: only show open indices
  CLOSED: [2017-04-20 Thu 21:14]
*** 2017-04-20: Running force-merge in Azure env: http://totvs-azure-jenkins.carol.ai:18080/job/ForceMergeESIndexAzure/6/console
  CLOSED: [2017-04-20 Thu 21:24]
*** 2017-04-20: Running force-merge in Bematech DO env: http://bematech-do-jenkins.carol.ai:18080/job/ForceMergeESIndexBematechDO/1/console
  CLOSED: [2017-04-20 Thu 21:32]
*** 2017-04-20: Check status of Azure removal: bematech tenant
  CLOSED: [2017-04-20 Thu 09:41]
bematech-app-1	191.234.190.255
*** 2017-04-20: re-sharding in Bematech DO
  CLOSED: [2017-04-20 Thu 10:00]
master-index-abae8b30ac9b11e692000401f8d88101-new

curl -XDELETE "http://$es_ip:9200/master-index-abae8b30ac9b11e692000401f8d88101-new"

root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch# time curl -XPOST "http://${es_ip}:${es_port}/${new_index_name}" -d @"${tmp_dir}/create.json"



{"acknowledged":false}
real    0m31.397s
user    0m0.024s
sys     0m0.008s


> }" | tee -a /var/log/es_index_manual.log
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   492    0   211    0   281      0      0 --:--:-- 24:18:56 --:--:--     0top
{
  "took" : 87536793,
  "timed_out" : false,
  "total" : 78749834,
  "updated" : 0,
  "created" : 78749834,
  "batches" : 157500,
  "version_conflicts" : 0,
  "noops" : 0,
  "retries" : 0,
  "failures" : [ ]
}

real    1458m56.806s
user    0m7.084s
sys     0m7.372s
You have new mail in /var/mail/root
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch#
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch# # Double confirm indices status, before proceed
root@bematech-do-es-1:/mnt/es-extra-volume/elasticsearch# curl "$es_ip:9200/_cat/indices?v" | grep "$(echo $alias_index_name | sed "s/.*-//g")"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1353  100  1353    0     0  10602      0 --:--:-- --:--:-- --:--:-- 10653
green  open   staging-index-abae8b30ac9b11e692000401f8d88101      5   2   32069867       350775    226.5gb         75.5gb
green  open   master-index-abae8b30ac9b11e692000401f8d88101       5   2 1023276475     27073266      2.5tb        858.5gb
green  open   master-index-abae8b30ac9b11e692000401f8d88101-new  50   2 1023271711            0      2.5tb          868gb
*** 2017-04-21: Delete closed ES indices in Azure
  CLOSED: [2017-04-21 Fri 09:02]
curl $es_ip:9200/_cat/indices?v

curl -XDELETE "http://$es_ip:9200/messagebroker"
curl -XDELETE "http://$es_ip:9200/flex2gateway"
curl -XDELETE "http://$es_ip:9200/phppath"
curl -XDELETE "http://$es_ip:9200/webui"
curl -XDELETE "http://$es_ip:9200/perl"
curl -XDELETE "http://$es_ip:9200/sawmill6cl.exe"
curl -XDELETE "http://$es_ip:9200/scroll"
curl -XDELETE "http://$es_ip:9200/spipe"
curl -XDELETE "http://$es_ip:9200/blazeds"
curl -XDELETE "http://$es_ip:9200/lcds"
curl -XDELETE "http://$es_ip:9200/egnrgo1.html"
curl -XDELETE "http://$es_ip:9200/sawmillcl.exe"

#+BEGIN_EXAMPLE
health status index                                               pri rep docs.count docs.deleted store.size pri.store.size
       close  staging-fd1125e03cb711e6878f0401f8d88c01
       close  staging-index-8cd6e43115e9416eb23609486fa053e3-tmp
green  open   master-index-e4010da4110ba377d100f050cb4440db         5   2     156685          445    500.2mb        166.7mb
green  open   staging-index-839920f07e6b11e6b71d0401f8d88101-new2  20   2   24611513      1199035    235.3gb         78.3gb
       close  staging-index-839920f07e6b11e6b71d0401f8d88101
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3        5   2   13278973       386248       37gb         12.3gb
green  open   master-index-839920f07e6b11e6b71d0401f8d88101-new2   20   2   26196454       316482    105.3gb         35.1gb
green  open   .kibana                                               1   2        105            0      2.3mb        803.6kb
       close  staging-index-839920f07e6b11e6b71d0401f8d88101-new
green  open   staging-index-e4010da4110ba377d100f050cb4440db        5   2      33737          220    342.5mb        114.1mb
       close  master-index-839920f07e6b11e6b71d0401f8d88101-new
green  open   master-index-8cd6e43115e9416eb23609486fa053e3         5   2    1692214          229     52.2gb         13.3gb
       close  master-index-839920f07e6b11e6b71d0401f8d88101
#+END_EXAMPLE
**** staging-fd1125e03cb711e6878f0401f8d88c01
curl -XDELETE "http://$es_ip:9200/staging-fd1125e03cb711e6878f0401f8d88c01"
**** close  staging-index-8cd6e43115e9416eb23609486fa053e3-tmp
curl -XDELETE "http://$es_ip:9200/staging-index-8cd6e43115e9416eb23609486fa053e3-tmp"
**** close  staging-index-839920f07e6b11e6b71d0401f8d88101
time curl -XDELETE "http://$es_ip:9200/staging-index-839920f07e6b11e6b71d0401f8d88101"
**** close  staging-index-839920f07e6b11e6b71d0401f8d88101-new
time curl -XDELETE "http://$es_ip:9200/staging-index-839920f07e6b11e6b71d0401f8d88101-new"
**** close  master-index-839920f07e6b11e6b71d0401f8d88101-new
time curl -XDELETE "http://$es_ip:9200/master-index-839920f07e6b11e6b71d0401f8d88101-new"
**** close  master-index-839920f07e6b11e6b71d0401f8d88101
time curl -XDELETE "http://$es_ip:9200/master-index-839920f07e6b11e6b71d0401f8d88101"

*** 2017-04-21: [#A] Scale down in Azure for even more
  CLOSED: [2017-04-21 Fri 14:49]
- Leave the cluster, wait the rebalancing is done
- Shutdown service and VM
- Update Jenkins configuration
- Heal the configuration of elasticsearch.yml and mdm.yml
- Delete VMs and volumes
#  --8<-------------------------- separator ------------------------>8--

es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

curl $es_ip:9200/_cat/nodes?v | grep -v app-

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.28"
   }
}'

service couchbase-server stop
service couchbase-server status

service elasticsearch stop
service elasticsearch status
**** 2017-04-21: bematech-cb-6	191.232.242.176	10.1.1.25	Type3	Couchbase
   CLOSED: [2017-04-21 Fri 12:24]
bematech-cb-6-20170125-092821
https://bematechresourcegroup210.blob.core.windows.net/vhds/bematech-cb-6-20170125-092821.vhd

**** 2017-04-21: bematech-cb-7	191.234.182.201	10.1.1.26	Type3	Couchbase
   CLOSED: [2017-04-21 Fri 13:08]
bematech-cb-7-20170125-092839
https://bematechresourcegroup148.blob.core.windows.net/vhds/bematech-cb-7-20170125-092839.vhd
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-04-21: bematech-es-15	191.235.92.95	10.1.1.40	Type4	Elasticsearch
   CLOSED: [2017-04-21 Fri 13:47]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.40"
   }
}'
**** 2017-04-21: bematech-es-16	191.234.188.183	10.1.1.41	Type4	Elasticsearch
   CLOSED: [2017-04-21 Fri 13:47]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.41"
   }
}'

**** 2017-04-21: bematech-es-17	191.234.184.68	10.1.1.46	Type4	Elasticsearch
   CLOSED: [2017-04-21 Fri 13:08]
bematech-es-17-20170325-214610

https://bematechresourcegroup368.blob.core.windows.net/vhds/bematech-es-17-20170325-214610.vhd
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.46"
   }
}'
**** 2017-04-21: bematech-es-18	191.232.250.135	10.1.1.47	Type4	Elasticsearch
   CLOSED: [2017-04-21 Fri 13:08]
https://bematechresourcegroup127.blob.core.windows.net/vhds/bematech-es-18-20170328-114052.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.47"
   }
}'
**** 2017-04-21: bematech-es-19	191.232.251.189	10.1.1.48	Type4	Elasticsearch
   CLOSED: [2017-04-21 Fri 12:24]
bematech-es-19-20170328-114254 
https://bematechresourcegroup147.blob.core.windows.net/vhds/bematech-es-19-20170328-114254.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.48"
   }
}'
**** 2017-04-21: bematech-es-20	191.232.251.125	10.1.1.49	Type4	Elasticsearch
   CLOSED: [2017-04-21 Fri 12:26]
bematech-es-20-20170328-114406

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.49"
   }
}'
**** 2017-04-21: bematech-es-21	191.232.247.40	10.1.1.50	Type4	Elasticsearch
   CLOSED: [2017-04-21 Fri 13:08]
bematech-es-21-20170328-114700 
https://bematechresourcegroup279.blob.core.windows.net/vhds/bematech-es-21-20170328-114700.vhd

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.50"
   }
}'
**** CANCELED Don't have enough resource: bematech-cb-5	191.234.191.244	10.1.1.11	Type3	Couchbase
   CLOSED: [2017-04-21 Fri 10:06]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-04-21: bematech-app-4	191.234.190.132	10.1.1.23	Type5	app worker
   CLOSED: [2017-04-21 Fri 13:47]

*** 2017-04-21: Scale down in DO
  CLOSED: [2017-04-21 Fri 15:27]
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

curl $es_ip:9200/_cat/nodes?v | grep -v app-

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.28"
   }
}'

service couchbase-server stop
service couchbase-server status

service elasticsearch stop
service elasticsearch status
**** 2017-04-21: close ES index
   CLOSED: [2017-04-21 Fri 10:34]
curl -XPOST "http://$es_ip:9200/$old_index_name/_close"

curl $es_ip:9200/_cat/indices?v
**** close  staging-839920f07e6b11e6b71d0401f8d88101
curl -XPOST "http://$es_ip:9200/staging-839920f07e6b11e6b71d0401f8d88101/_close"
**** close  staging-fd1125e03cb711e6878f0401f8d88c01
curl -XPOST "http://$es_ip:9200/staging-fd1125e03cb711e6878f0401f8d88c01/_close"
**** close  master-index-abae8b30ac9b11e692000401f8d88101
curl -XPOST "http://$es_ip:9200/master-index-abae8b30ac9b11e692000401f8d88101/_close"
**** 2017-04-21: bematech-do-cb-9	138.68.247.186	10.138.240.73	Type3	Couchbase
   CLOSED: [2017-04-21 Fri 14:44]
**** 2017-04-21: bematech-do-es-20	138.68.45.214	10.138.240.254	Type4	Elasticsearch
   CLOSED: [2017-04-21 Fri 15:06]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.45.214"
   }
}'
**** 2017-04-21: bematech-do-es-3	138.68.247.61	10.138.72.143	Type4	Elasticsearch
   CLOSED: [2017-04-21 Fri 15:10]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.247.61"
   }
}'
**** #  --8<-------------------------- separator ------------------------>8--

*** 2017-04-21: #51 [Docker-compose] - Create demo VM config for ioT containers: 6h
  CLOSED: [2017-04-21 Fri 17:05]
*** 2017-04-19: [log] - Create ES indices: 6h
  CLOSED: [2017-04-19 Wed 16:19]
https://bitbucket.org/nubesecure/devops/issues/61/log-create-es-indices
*** 2017-04-19: [#A] #46 Move penroz/common proxy docker build to devops and prepare VM config: 4h
  CLOSED: [2017-04-19 Wed 19:09]
http://stackoverflow.com/questions/35154441/docker-compose-links-vs-external-links
*** 2017-04-21: Improve proxy healthcheck, only test if launch and piper when file exists: 1h
  CLOSED: [2017-04-21 Fri 11:08]
*** 2017-04-22: [0%-> 70%] #138 Next gen CI for penroz: 2h + 2h + 2h
  CLOSED: [2017-04-22 Sat 12:17]
https://bitbucket.org/nubesecure/penroz/issues/138/next-gen-ci-for-penroz

${IMG}:penroz_db_${IMG_TAG_POSTFIX} -> soterianetworks/penroz:db_${IMG_TAG_POSTFIX}
${IMG}:penroz_dbseed_${IMG_TAG_POSTFIX} -> soterianetworks/penroz:dbseed_${IMG_TAG_POSTFIX}
${IMG}:penroz_eureka_${IMG_TAG_POSTFIX} -> soterianetworks/penroz:eureka_${IMG_TAG_POSTFIX}
${IMG}:penroz_iamsvc_${IMG_TAG_POSTFIX} -> soterianetworks/penroz:iamsvc_${IMG_TAG_POSTFIX}

- PR in penroz
- PR in brozton
- PR in devops

docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test soterianetworks/penroz:base_images /bin/bash

docker exec -it my-test bash
**** 2017-04-22: iamsvc/eureka dockerfile
   CLOSED: [2017-04-21 Fri 18:20]
**** 2017-04-22: scripts
   CLOSED: [2017-04-21 Fri 18:20]
**** 2017-04-22: create docker hub for all images
   CLOSED: [2017-04-21 Fri 18:38]
**** 2017-04-22: devops repo use new image: image_name and service_name; create PR. update bitbucket ticket
   CLOSED: [2017-04-21 Fri 18:38]
*** 2017-04-23: iamsvc connect with elasticsearch service: 1h
  CLOSED: [2017-04-23 Sun 19:52]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-04-23: change memory threshold from 90% to 93% in CB nodes
  CLOSED: [2017-04-23 Sun 09:44]
*** 2017-04-23: add 500GB disk to app-03: /mnt/data
  CLOSED: [2017-04-23 Sun 09:02]
ls /dev/disk/by-id

export mnt_point=/mnt/data
export volume_label=$(ls /dev/disk/by-id/ | grep Volume)
# TODO: verify volume_label is not empty
sudo mkfs.ext4 -F /dev/disk/by-id/$volume_label

# To mount the volume, copy and paste the text below:
# Create a mount point under /mnt
sudo mkdir -p $mnt_point

# Mount the volume
sudo mount -o discard,defaults /dev/disk/by-id/$volume_label  $mnt_point

# Change fstab so the volume will be mounted after a reboot
echo "/dev/disk/by-id/$volume_label $mnt_point ext4 defaults,nofail,discard 0 0" | sudo tee -a /etc/fstab

cat  /etc/fstab

df -h

*** 2017-04-25: close index Bematech DO
  CLOSED: [2017-04-24 Mon 14:05]
curl -XPOST "http://$es_ip:9200/master-index-abae8b30ac9b11e692000401f8d88101/_close"

curl -XDELETE "http://$es_ip:9200/master-index-abae8b30ac9b11e692000401f8d88101"
*** 2017-04-25: Scale down in DO
  CLOSED: [2017-04-25 Tue 11:39]
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

curl $es_ip:9200/_cat/nodes?v | grep -v app-

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.28"
   }
}'

service couchbase-server stop
service couchbase-server status

service elasticsearch stop
service elasticsearch status
**** 2017-04-25: delete ES index
   CLOSED: [2017-04-24 Mon 13:46]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-04-25: update jenkins jobs
   CLOSED: [2017-04-25 Tue 11:39]
**** 2017-04-25: update wiki: cost estimation
   CLOSED: [2017-04-25 Tue 10:30]
**** 2017-04-25: remove firewall rules for existing nodes: http://bematech-do-jenkins.carol.ai:18080/job/RunCommandOnServers/65/
   CLOSED: [2017-04-25 Tue 10:30]
*** 2017-04-25: [#A] ad wiki: Lessons Learned For Cost Saving In Public Cloud
  CLOSED: [2017-04-25 Tue 13:14]
https://github.com/TOTVS/mdmdevops/wiki/Lessons-Learned-For-Cost-Saving-In-Public-Cloud

*** 2017-04-25: Scale down in Bematech DO
  CLOSED: [2017-04-25 Tue 16:58]
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

curl $es_ip:9200/_cat/nodes?v | grep -v app-

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.28"
   }
}'

service couchbase-server stop
service couchbase-server status

service elasticsearch stop
service elasticsearch status

ufw delete allow from 
**** 2017-04-25: update nagios
   CLOSED: [2017-04-25 Tue 13:56]
**** 2017-04-25: update wiki: cost estimation
   CLOSED: [2017-04-25 Tue 13:56]
*** #  --8<-------------------------- separator ------------------------>8--

*** 2017-04-25: (1) [Bematech DO] Change audit nodes from 3 to 1, and enable weekly ES backup.
  CLOSED: [2017-04-25 Tue 17:11]
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
es_port=9400

curl $es_ip:$es_port/_cat/indices?v

watch "curl $es_ip:$es_port/_cat/shards?v | grep -v STARTED"

service elasticsearch_audit status
service elasticsearch_audit stop

shutdown -h now
**** 2017-04-25: Change all replica to 0
   CLOSED: [2017-04-25 Tue 10:39]
for index in $(curl $es_ip:$es_port/_cat/indices?v | grep open | awk -F' ' '{print $3}'); do
     echo "Enable replica as 0 for $index"
     curl -XPUT "$es_ip:$es_port/$index/_settings" -d '
     {
         "index" : {
             "number_of_replicas": 0
         }
     }'
done

curl $es_ip:$es_port/_cat/indices?v
**** 2017-04-25: update nagios
   CLOSED: [2017-04-25 Tue 11:02]
**** 2017-04-25: bematech-do-audit-2	138.68.5.87	10.138.176.252	Type2	Elasticsearch Audit
   CLOSED: [2017-04-25 Tue 17:10]
curl -XPUT $es_ip:$es_port/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.5.87"
   }
}'

**** 2017-04-25: bematech-do-audit-3	138.197.209.157	10.138.112.170	Type2	Elasticsearch Audit
   CLOSED: [2017-04-25 Tue 17:10]
curl -XPUT $es_ip:$es_port/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.197.209.157"
   }
}'
*** 2017-04-25: Change CB to full eviction
  CLOSED: [2017-04-25 Tue 21:25]
#+BEGIN_EXAMPLE
Bruno Volpato 
[1:25 PM] 
it seems that if we change the CB to full eviction, we'll be able to use less servers.

Bruno Volpato 
[1:25 PM] 
uploaded this image: Pasted image at 2017-04-25, 11:25 AM
Add Comment

Denny Zhang
[1:26 PM] 
Nice. Let me track this, and do some research.

If it looks good, let’s give a try.
#+END_EXAMPLE
*** 2017-04-25: Change fielddata cache from 20% to 15%, in order to save cost. 
  CLOSED: [2017-04-25 Tue 23:44]
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

# disable allocation: none
curl -XPUT "http://$es_ip:9200/_cluster/settings" -d '
{
 "persistent": {
   "cluster.routing.allocation.enable": "none"
 }
}
'

# Perform a synced flush
Shard recovery will be much faster if you stop indexing and issue a synced-flush request:

curl -XPOST "http://$es_ip:9200/_flush/synced"

# restart es
service elasticsearch stop
service elasticsearch start

service elasticsearch status

# check es status
curl $es_ip:9200/_cat/nodes?v | grep -v app-

# enable allocation: all
curl -XPUT "http://$es_ip:9200/_cluster/settings" -d '
{
 "persistent": {
   "cluster.routing.allocation.enable": "all"
 }
}
'

watch "curl $es_ip:9200/_cat/shards?v | grep -v STARTED"
**** 2017-04-25: bematech-do-es-1	138.68.231.232	10.138.136.243	Type4	Elasticsearch
   CLOSED: [2017-04-25 Tue 21:28]
**** 2017-04-25: bematech-do-es-2	138.68.247.36	10.138.72.124	Type4	Elasticsearch
   CLOSED: [2017-04-25 Tue 21:58]
**** 2017-04-25: bematech-do-es-4	138.68.239.172	10.138.136.96	Type4	Elasticsearch
   CLOSED: [2017-04-25 Tue 21:59]
**** 2017-04-25: bematech-do-es-5	138.68.224.86	10.138.224.113	Type4	Elasticsearch
   CLOSED: [2017-04-25 Tue 22:00]
**** 2017-04-25: bematech-do-es-6	138.197.217.22	10.138.144.225	Type4	Elasticsearch
   CLOSED: [2017-04-25 Tue 22:16]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-04-25: bematech-do-es-8	138.197.194.166	10.138.64.112	Type4	Elasticsearch
   CLOSED: [2017-04-25 Tue 22:59]
**** 2017-04-25: bematech-do-es-9	138.197.199.94	10.138.32.40	Type4	Elasticsearch
   CLOSED: [2017-04-25 Tue 23:01]
**** 2017-04-25: bematech-do-es-10	138.197.217.254	10.138.176.45	Type4	Elasticsearch
   CLOSED: [2017-04-25 Tue 23:23]
**** HALF bematech-do-es-11	138.68.48.175	10.138.112.98	Type4	Elasticsearch
*** 2017-04-26: (1) When VM restart, DO will change back /etc/hosts to original content.
  CLOSED: [2017-04-26 Wed 17:05]
https://trello.com/c/t9YTBT9X

cd /etc/cloud/templates

vim hosts.debian.tmpl

*** 2017-04-27: [#A] Re-index: Sat 15:00. Need to close old index
  CLOSED: [2017-04-21 Fri 20:39]
export es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
export es_port=9200
export tmp_dir="/tmp/"
export REINDEX_BATCH_SIZE="500"
export replica_count=2

# Note: we use 5 for indices less than 20 GB. If over 20GB, we use 20 shards
export shard_count=50
export old_index_name="master-index-abae8b30ac9b11e692000401f8d88101"
export new_index_name="master-index-abae8b30ac9b11e692000401f8d88101-new"
export alias_index_name="master-abae8b30ac9b11e692000401f8d88101"

>     }
> }" | tee -a /var/log/es_index_manual.log
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100   281    0     0    0   281      0      0 --:--:--  0:15:09 --:--:--     0

100   493    0   212    0   281      0      0 --:--:--  4:51:59 --:--:--     0
{
  "took" : 17519260,
  "timed_out" : false,
  "total" : 78749857,
  "updated" : 0,
  "created" : 29,
  "batches" : 157500,
  "version_conflicts" : 78749828,
  "noops" : 0,
  "retries" : 0,
  "failures" : [ ]
}

real    291m59.272s
user    0m1.188s
sys     0m1.672s
You have new mail in /var/mail/root

*** 2017-04-29: shutdown Azure env
  CLOSED: [2017-04-29 Sat 12:32]
*** 2017-04-29: update wiki for old Azure env
  CLOSED: [2017-04-29 Sat 12:49]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-04-26: In development cycle, build idp docker image locally: 1h
  CLOSED: [2017-04-26 Wed 09:16]
https://bitbucket.org/nubesecure/brozton/pull-requests/32/in-development-cycle-build-idp-docker/diff
*** 2017-04-24: healthcheck for proxy service improvement: 1h
  CLOSED: [2017-04-24 Mon 11:02]
*** 2017-04-25: base docker image install netcat: 1h
  CLOSED: [2017-04-25 Tue 08:51]
docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test --entrypoint=/bin/sh openjdk:alpine

docker exec -it my-test sh
which nc
*** 2017-04-30: setup new VM: iot.shibgeek.com: 1h
  CLOSED: [2017-04-30 Sun 16:31]
**** 2017-04-30: install docker daemon
   CLOSED: [2017-04-30 Sun 16:10]
ssh -p 2702 root@iot.shibgeek.com
**** setup iot
**** setup checkout code
**** setup jenkins env
**** update dns

*** 2017-04-30: #67 [logging] - Upgrade ES and Logstash to latest possible alpine version: 4h
  CLOSED: [2017-04-30 Sun 16:40]
https://bitbucket.org/nubesecure/devops/issues/67/logging-upgrade-es-and-logstash-to-latest
*** 2017-04-30: #65 [proxy] - Achieve proxy_pass independence: 3h
  CLOSED: [2017-04-30 Sun 17:05]
https://bitbucket.org/nubesecure/devops/issues/65/proxy-achieve-proxy_pass-independence

curl -I http://localhost:80/idp/
curl -I http://idp/idp/

curl -I http://localhost:80/oauth/
curl -I http://oauth2/oauth/

docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test  --entrypoint=/bin/bash soterianetworks/devops:proxy_${IMG_TAG_POSTFIX} 

docker exec -it my-test bash
export PROXY_SERVICES="gateway"
/docker-entrypoint.sh

cat /etc/nginx/conf.d/default.conf
** 2017-05
*** 2017-05-02: Scale down in Prod DO env
  CLOSED: [2017-05-02 Tue 11:25]
es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

curl $es_ip:9200/_cat/nodes?v | grep -v app-

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "10.1.1.28"
   }
}'

service couchbase-server stop
service couchbase-server status

service elasticsearch stop
service elasticsearch status

service elasticsearch_audit stop
service elasticsearch_audit status

ufw delete allow from 
**** 2017-05-02: change all replica to 0
   CLOSED: [2017-05-01 Mon 17:22]
for index in $(curl $es_ip:$es_port/_cat/indices?v | grep open | awk -F' ' '{print $3}'); do
     echo "Enable replica as 0 for $index"
     curl -XPUT "$es_ip:$es_port/$index/_settings" -d '
     {
         "index" : {
             "number_of_replicas": 0
         }
     }'
done

curl $es_ip:$es_port/_cat/indices?v
curl $es_ip:$es_port/_cat/shards?v | grep -v STARTED
**** 2017-05-02: update jenkins jobs
   CLOSED: [2017-05-01 Mon 17:35]
*** 2017-05-03: For Prod DO env, delete the old and closed ES indices, from the previous re-sharding
  CLOSED: [2017-05-03 Wed 00:35]
#+BEGIN_EXAMPLE
root@prod-es-01:/var/log/elasticsearch# curl $es_ip:9200/_cat/indices?v  | grep close
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  7375  100  7375    0     0   7692      0 --:--:-- --:--:-- --:--:--  7690
       close  staging-index-799e458055c611e6bb000401f8d88101
       close  master-index-799e458055c611e6bb000401f8d88101
       close  staging-index-8cd6e43115e9416eb23609486fa053e3
       close  staging-index-56b33120ef1311e6a81da2f42be00f79-new
       close  master-index-839920f07e6b11e6b71d0401f8d88101-new
       close  staging-index-3366bf206f2a11e694f80401f8d88c01
       close  master-index-3366bf206f2a11e694f80401f8d88c01-new
       close  staging-fd1125e03cb711e6878f0401f8d88c01
       close  master-index-56b33120ef1311e6a81da2f42be00f79
       close  staging-index-3366bf206f2a11e694f80401f8d88c01-new
       close  master-index-860245c0841e11e6a8260401f8d88101
       close  staging-index-860245c0841e11e6a8260401f8d88101
       close  staging-index-860245c0841e11e6a8260401f8d88101-new
       close  staging-860245c0841e11e6a8260401f8d88101
       close  staging-index-3fa847206b9911e6b61d0401f8d88101-new
       close  master-index-3fa847206b9911e6b61d0401f8d88101
       close  staging-56b33120ef1311e6a81da2f42be00f79
       close  staging-index-799e458055c611e6bb000401f8d88101-new
       close  master-index-860245c0841e11e6a8260401f8d88101-new
       close  staging-index-8cd6e43115e9416eb23609486fa053e3-new
       close  master-index-3fa847206b9911e6b61d0401f8d88101-new
       close  master-index-3366bf206f2a11e694f80401f8d88c01
       close  staging-index-3fa847206b9911e6b61d0401f8d88101
       close  master-index-56b33120ef1311e6a81da2f42be00f79-new
       close  master-index-799e458055c611e6bb000401f8d88101-new
#+END_EXAMPLE

#+BEGIN_EXAMPLE
curl $es_ip:9200/_cat/indices?v | grep staging-index-799e458055c611e6bb000401f8d88101
curl -XDELETE "http://$es_ip:9200/staging-index-799e458055c611e6bb000401f8d88101"

curl -XDELETE "http://$es_ip:9200/master-index-799e458055c611e6bb000401f8d88101"
curl -XDELETE "http://$es_ip:9200/staging-index-8cd6e43115e9416eb23609486fa053e3"

curl $es_ip:9200/_cat/indices?v | grep staging-index-56b33120ef1311e6a81da2f42be00f79-new
curl -XDELETE "http://$es_ip:9200/staging-index-56b33120ef1311e6a81da2f42be00f79-new"

curl $es_ip:9200/_cat/indices?v | grep master-index-839920f07e6b11e6b71d0401f8d88101-new
curl -XDELETE "http://$es_ip:9200/master-index-839920f07e6b11e6b71d0401f8d88101-new"

curl $es_ip:9200/_cat/indices?v | grep staging-index-3366bf206f2a11e694f80401f8d88c01
curl -XDELETE "http://$es_ip:9200/staging-index-3366bf206f2a11e694f80401f8d88c01"

curl $es_ip:9200/_cat/indices?v | grep master-index-3366bf206f2a11e694f80401f8d88c01-new
curl -XDELETE "http://$es_ip:9200/master-index-3366bf206f2a11e694f80401f8d88c01-new"

curl $es_ip:9200/_cat/indices?v | grep staging-fd1125e03cb711e6878f0401f8d88c01
curl -XDELETE "http://$es_ip:9200/staging-fd1125e03cb711e6878f0401f8d88c01"

curl $es_ip:9200/_cat/indices?v | grep master-index-56b33120ef1311e6a81da2f42be00f79
curl -XDELETE "http://$es_ip:9200/master-index-56b33120ef1311e6a81da2f42be00f79"

curl $es_ip:9200/_cat/indices?v | grep staging-index-3366bf206f2a11e694f80401f8d88c01-new
curl -XDELETE "http://$es_ip:9200/staging-index-3366bf206f2a11e694f80401f8d88c01-new"

curl $es_ip:9200/_cat/indices?v | grep master-index-860245c0841e11e6a8260401f8d88101
curl -XDELETE "http://$es_ip:9200/master-index-860245c0841e11e6a8260401f8d88101"

curl $es_ip:9200/_cat/indices?v | grep staging-index-860245c0841e11e6a8260401f8d88101
curl -XDELETE "http://$es_ip:9200/staging-index-860245c0841e11e6a8260401f8d88101"

curl $es_ip:9200/_cat/indices?v | grep staging-index-860245c0841e11e6a8260401f8d88101-new
curl -XDELETE "http://$es_ip:9200/staging-index-860245c0841e11e6a8260401f8d88101-new"

curl $es_ip:9200/_cat/indices?v | grep staging-860245c0841e11e6a8260401f8d88101
curl -XDELETE "http://$es_ip:9200/staging-860245c0841e11e6a8260401f8d88101"

curl $es_ip:9200/_cat/indices?v | grep staging-index-3fa847206b9911e6b61d0401f8d88101-new
curl -XDELETE "http://$es_ip:9200/staging-index-3fa847206b9911e6b61d0401f8d88101-new"

curl $es_ip:9200/_cat/indices?v | grep master-index-3fa847206b9911e6b61d0401f8d88101
curl -XDELETE "http://$es_ip:9200/master-index-3fa847206b9911e6b61d0401f8d88101"

curl $es_ip:9200/_cat/indices?v | grep staging-56b33120ef1311e6a81da2f42be00f79
curl -XDELETE "http://$es_ip:9200/staging-56b33120ef1311e6a81da2f42be00f79"

curl $es_ip:9200/_cat/indices?v | grep staging-index-799e458055c611e6bb000401f8d88101-new
curl -XDELETE "http://$es_ip:9200/staging-index-799e458055c611e6bb000401f8d88101-new"

curl $es_ip:9200/_cat/indices?v | grep master-index-860245c0841e11e6a8260401f8d88101-new
curl -XDELETE "http://$es_ip:9200/master-index-860245c0841e11e6a8260401f8d88101-new"

curl $es_ip:9200/_cat/indices?v | grep staging-index-8cd6e43115e9416eb23609486fa053e3-new
curl -XDELETE "http://$es_ip:9200/staging-index-8cd6e43115e9416eb23609486fa053e3-new"

curl $es_ip:9200/_cat/indices?v | grep master-index-3fa847206b9911e6b61d0401f8d88101-new
curl -XDELETE "http://$es_ip:9200/master-index-3fa847206b9911e6b61d0401f8d88101-new"

curl $es_ip:9200/_cat/indices?v | grep master-index-3366bf206f2a11e694f80401f8d88c01
curl -XDELETE "http://$es_ip:9200/master-index-3366bf206f2a11e694f80401f8d88c01"

curl $es_ip:9200/_cat/indices?v | grep staging-index-3fa847206b9911e6b61d0401f8d88101
curl -XDELETE "http://$es_ip:9200/staging-index-3fa847206b9911e6b61d0401f8d88101"

curl $es_ip:9200/_cat/indices?v | grep master-index-56b33120ef1311e6a81da2f42be00f79-new
curl -XDELETE "http://$es_ip:9200/master-index-56b33120ef1311e6a81da2f42be00f79-new"

curl $es_ip:9200/_cat/indices?v | grep master-index-799e458055c611e6bb000401f8d88101-new
curl -XDELETE "http://$es_ip:9200/master-index-799e458055c611e6bb000401f8d88101-new"
#+END_EXAMPLE

*** 2017-05-03: #27 [proxy] - Achieve proxy_pass independence: 2h
  CLOSED: [2017-05-03 Wed 11:55]
https://bitbucket.org/nubesecure/devops/issues/65/proxy-achieve-proxy_pass-independence

*** 2017-05-04: Scale up Bematech env and support for performance issues
  CLOSED: [2017-05-04 Thu 08:47]
**** add one CB nodes and one ES nodes
138.197.200.188:2702

138.197.208.58:2702

**** add 2 workers
138.68.251.197:2702
138.68.251.220:2702

138.68.251.197    bematech-do-app-5
138.68.251.220    bematech-do-app-6
**** add 3 more ES nodes
| bematech-do-es-10   |                 |                | Type4          | Elasticsearch       |
| bematech-do-es-11   |   138.68.48.175 |  10.138.112.98 | Type4          | Elasticsearch       |
| bematech-do-es-12   |                 |                | Type4          | Elasticsearch       |
| bematech-do-es-13   |                 |                | Type4          | Elasticsearch       |

138.68.10.163:2702
138.68.48.175:2702
138.197.213.8:2702
138.68.8.192:2702

| bematech-do-es-10   |   138.68.10.163 | 10.138.208.221 | Type4          | Elasticsearch       |
| bematech-do-es-11   |   138.68.48.175 |  10.138.112.98 | Type4          | Elasticsearch       |
| bematech-do-es-12   |   138.197.213.8 | 10.138.176.184 | Type4          | Elasticsearch       |
| bematech-do-es-13   |    138.68.8.192 |  10.138.136.59 | Type4          | Elasticsearch       |
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-05-04: Register and update all accounts(bitbucket, slack, docker) using company emails: 3h
  CLOSED: [2017-05-04 Thu 11:37]
*** 2017-05-05: Build and test latest docker image: http://injenkins.carol.ai:48080/job/BuidAllDockerImages/130/console
  CLOSED: [2017-05-05 Fri 10:03]
*** 2017-05-05: #70 [proxy] - /gateway proxy_pass config is broken: 1h
  CLOSED: [2017-05-05 Fri 16:21]
https://bitbucket.org/nubesecure/devops/issues/70/proxy-gateway-proxy_pass-config-is-broken
*** 2017-05-06: Workaround dofacdenny oversize issue: 2h
  CLOSED: [2017-05-06 Sat 12:55]

*** 2017-05-09: add wiki: Doc Template For Requests Of Asking A Public VM
  CLOSED: [2017-05-09 Tue 12:00]
https://github.com/TOTVS/mdmdevops/wiki/Doc-Template-For-Requests-Of-Asking-A-Public-VM
*** 2017-05-10: (2) Track system performance using internal Metric system
  CLOSED: [2017-05-10 Wed 14:42]
https://trello.com/c/qrUOgxy8

curl http://localhost:8081/admin/metrics?pretty=true

mdm_host="localhost"
mdm_port="8081"
curl http://${mdm_host}:${mdm_port}/admin/metrics?pretty=true | jq ".io.dropwizard.jetty.MutableServletContextHandler.5xx-responses"

curl http://localhost:8081/admin/metrics?pretty=true | grep -C 10 ".io.dropwizard.jetty.MutableServletContextHandler.5xx-responses"
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-05-10: [#A] #74 [elasticsearch] - Create schema for log storage: 3h
  CLOSED: [2017-05-10 Wed 10:21]
https://bitbucket.org/nubesecure/devops/issues/74/elasticsearch-create-schema-for-log

*** 2017-05-11: #72: Automate GUI login check via docker+seleinum: 2h
  CLOSED: [2017-05-11 Thu 05:08]
https://bitbucket.org/nubesecure/devops/issues/72/automate-gui-login-check-via-docker

*** 2017-05-11: #47 Setup Jenkins via docker-compose from bitbucket: 1h + 4h
  CLOSED: [2017-05-11 Thu 23:02]
https://bitbucket.org/nubesecure/brozton/issues/47/setup-jenkins-via-docker-compose-from

http://injenkins.carol.ai:8080

*** 2017-05-11: #34: Enforce shellcheck for all bash scripts in all critical repos: 2h
  CLOSED: [2017-05-11 Thu 23:46]
https://bitbucket.org/nubesecure/devops/issues/34/enforce-shellcheck-for-all-bash-scripts-in
*** 2017-05-11: #49: Enforce pylint for all python scripts in all critical repos: 2h
  CLOSED: [2017-05-11 Thu 23:46]
https://bitbucket.org/nubesecure/devops/issues/49/enforce-pylint-for-all-python-scripts-in
*** 2017-05-12: Enable pylint and shellcheck for both penroz and devops repo: 3h
  CLOSED: [2017-05-12 Fri 17:17]
*** 2017-05-12: reduce code duplication for jenkins pylint and shellcheck jobs: 2h
  CLOSED: [2017-05-12 Fri 23:22]
ignore_file=".${JOB_NAME}_pylint_ignore"
container_name="pylint_${JOB_NAME}"
code_dir="/var/lib/docker/volumes/jenkins_volume_workspace/_data/$JOB_NAME"

docker push denny/pylint:1.0
docker push denny/shellcheck:1.0

docker pull denny/pylint:1.0
docker pull denny/shellcheck:1.0
*** 2017-05-13: Integrate with GUI test: show images from GUI: 1h
  CLOSED: [2017-05-13 Sat 08:23]

*** 2017-05-14: #75 Fix all shellcheck issues for penroz/brozton/devops repos: 4h
  CLOSED: [2017-05-14 Sun 16:37]
https://bitbucket.org/nubesecure/devops/issues/75/fix-all-shellcheck-issues-for-penroz
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-05-15: #60 Enforce daily backup for named volumes in demo env: 3h
  CLOSED: [2017-05-15 Mon 15:09]
https://bitbucket.org/nubesecure/devops/issues/60/enforce-daily-backup-for-named-volumes-in

python /usr/sbin/cleanup_old_files.py --working_dir "/data/backup" --filename_pattern "cijenkins_volume_workspace-.*" --cleanup_type directory --min_copies 2

#!/bin/bash -e
for folder in $folder_to_delete; do
    dir_name=$(dirname $folder)
    file_pattern=$(basename $folder)
    echo "run cleanup in $dir_name for $file_pattern"
    $ssh_command "python /usr/sbin/cleanup_old_files.py --working_dir $dir_name --filename_pattern $file_pattern --cleanup_type directory --min_copies $keep_copies_count"
done

for file in $files_to_delete; do
    dir_name=$(dirname $file)
    file_pattern=$(basename $file)
    echo "run cleanup in $dir_name for $file_pattern"
    $ssh_command "python /usr/sbin/cleanup_old_files.py --working_dir $dir_name --filename_pattern $file_pattern --cleanup_type directory --min_copies $keep_copies_count"
done

*** 2017-05-15: #78 CleanupOldFiles to remove old backup in a safe and oragnized way: 3h
  CLOSED: [2017-05-15 Mon 15:11]
https://bitbucket.org/nubesecure/devops/issues/78/cleanupoldfiles-to-remove-old-backup-in-a

*** 2017-05-16: Jenkins jobs implementation and improvement: 2h
  CLOSED: [2017-05-16 Tue 11:39]
*** 2017-05-16: DevOps PPT for SoteriaNetworks: 4h
  CLOSED: [2017-05-16 Tue 19:22]
https://drive.google.com/file/d/0B4rL_DRfW3UUNEU3ZTVsWWVodUU/view
*** 2017-05-17: Enable slack notification in Jenkins: 2h
  CLOSED: [2017-05-17 Wed 09:11]
#internal-ci
#soteria-all-alerts
*** 2017-05-17: #60: Docker hub kumku build causing error: 3h
  CLOSED: [2017-05-17 Wed 14:40]
https://bitbucket.org/nubesecure/kumku-u/issues/60/docker-hub-kumku-build-causing-error
*** 2017-05-18: #49 [Docker] - Cannot debug java app in container: 2h
  CLOSED: [2017-05-18 Thu 11:59]
https://bitbucket.org/nubesecure/brozton/issues/49/docker-cannot-debug-java-app-in-container

ozgur.v.amac [11:27 AM] 
@dennyzhang Read up https://docs.oracle.com/cd/E13150_01/jrockit_jvm/jrockit/jrdocs/refman/optionX.html Especially -Xdebug that way you can test it out yourself.
*** 2017-05-19: #81 [Brozton] idp remote debugging port is not listening: 2h
  CLOSED: [2017-05-19 Fri 09:06]
https://bitbucket.org/nubesecure/devops/issues/81/brozton-idp-remote-debugging-port-is-not
*** 2017-05-19: [#A] #42 DeployAIOTest Jenkins job: Run all-in-one deployment and integration tests: 3h
  CLOSED: [2017-05-19 Fri 09:17]
https://bitbucket.org/nubesecure/devops/issues/42/deployaiotest-jenkins-job-run-all-in-one
*** 2017-05-19: Start a bigger disk for CB backup: 3TB
  CLOSED: [2017-05-19 Fri 14:02]
*** 2017-05-19: remove volume of Kung's mdm app node: https://cloud.digitalocean.com/droplets/45799278/graphs?i=4fab62
  CLOSED: [2017-05-19 Fri 17:18]

*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-05-22: add 2 workers to bematech do
  CLOSED: [2017-05-22 Mon 13:58]
*** 2017-05-23: [#A] Remove existing 9 CB nodes                            :IMPORTANT:
  CLOSED: [2017-05-23 Tue 16:12]
service couchbase-server stop

shutdown -h now
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-05-23: bematech-do-cb-2	138.68.229.74
  CLOSED: [2017-05-21 Sun 17:33]
**** 2017-05-23: bematech-do-cb-3	138.68.14.103
  CLOSED: [2017-05-21 Sun 18:42]
**** 2017-05-23: bematech-do-cb-4	138.68.56.31 <2017-05-21 20:01 UTC +8> - <2017-05-21 20:36 UTC +8> - 
  CLOSED: [2017-05-21 Sun 20:38]
**** 2017-05-23: bematech-do-cb-5	138.68.247.158
  CLOSED: [2017-05-22 Mon 08:25]
**** 2017-05-23: bematech-do-cb-6	138.68.247.166
  CLOSED: [2017-05-22 Mon 12:09]
**** 2017-05-23: bematech-do-cb-7	138.197.200.120
  CLOSED: [2017-05-22 Mon 19:09]
**** 2017-05-23: bematech-do-cb-8	138.68.247.181
  CLOSED: [2017-05-22 Mon 23:44]
**** 2017-05-23: bematech-do-cb-9	138.197.200.188
  CLOSED: [2017-05-23 Tue 08:03]
**** 2017-05-23: bematech-do-cb-1	138.68.225.199
  CLOSED: [2017-05-23 Tue 11:46]*** #  --8<-------------------------- separator ------------------------>8--
**** #  --8<-------------------------- separator ------------------------>8--

*** 2017-05-23: retire bematech-do-es-12(138.197.213.8:2702)
  CLOSED: [2017-05-23 Tue 16:20]
**** update Jenkins jobs
**** Update firewall

*** 2017-05-23: Add wiki: Scale in: Remove one node out of existing cluster
   CLOSED: [2017-05-23 Tue 16:46]
https://github.com/TOTVS/mdmdevops/wiki/Remove-one-node-out-of-existing-cluster
*** 2017-05-25: add wiki: Query Cluster Node List Information From Loadbalancer
  CLOSED: [2017-05-25 Thu 11:59]
https://github.com/TOTVS/mdmdevops/wiki/Query-Cluster-Node-List-Information-From-Loadbalancer
*** 2017-05-25: (3) DevOps provides a API to return ip/hostname list for prod envs, considering to constant scale down/up: git repo?
  CLOSED: [2017-05-25 Thu 23:53]
https://trello.com/c/owSdA5Zj

http://injenkins.carol.ai:48080/view/Basic/job/DockerDeployFeatureCookbooks/619/console

ssh -N -p 22 -i /home/denny/denny_id_rsa -f root@172.17.0.4 -L *:8084:localhost:80 -n /bin/bash

curl -I http://45.33.87.74:8084

telnet 45.33.87.74 8084
*** #  --8<-------------------------- separator ------------------------>8--
**** 2017-05-25: get cb list and es list: ip address only
   CLOSED: [2017-05-24 Wed 19:30]
**** 2017-05-25: haproxy redirect based on uri
   CLOSED: [2017-05-25 Thu 11:59]
curl http://localhost:8000/couchbase

**** 2017-05-25: healthcheck
   CLOSED: [2017-05-25 Thu 11:59]

*** 2017-05-26: audit issue in prod env
  CLOSED: [2017-05-26 Fri 11:07]
*** #  --8<-------------------------- separator ------------------------>8--
*** Fix Jenkins failures: http://jenkins.shibgeek.com:48084/view/All/job/DeployAIOSandboxTest/3/console
*** Prepare demo env
*** setup sandbox.shibgeek.com
ozgur.v.amac
[10:58 AM] 
Ok. Great. Thank you.

dennyzhang 
[10:59 AM] 
BTW, I’m working on sandbox story.

I will prepare docker-compose.yml in devops repo.

But do we have to setup that docker compose env in the server? Or once the test is good, we destroy it? (edited)

ozgur.v.amac
[11:14 AM] 
No, we will deploy and run just like the others. I think we should deploy them to second Shibgeek VM (point a subdomain e.g. sandbox.shibgeek.com from google domains) because proof of concept is to be able to protect odyssey and sahara with our services on shibgeek Demo VM

*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-05-24: #83: In CI Jenkins, enforce regular cleanup: 3h
  CLOSED: [2017-05-24 Wed 10:23]
https://bitbucket.org/nubesecure/devops/issues/83/in-ci-jenkins-enforce-regular-cleanup
*** 2017-05-24: #80 Jenkins run docker image build daily: 3h
  CLOSED: [2017-05-24 Wed 09:12]
https://bitbucket.org/nubesecure/devops/issues/80/jenkins-run-docker-image-build-daily
*** 2017-05-24: #71: CreateMonthlyGitTag: Create monthly tag automatically for the default branch of all critical repos: 2h
  CLOSED: [2017-05-24 Wed 09:13]
https://bitbucket.org/nubesecure/devops/issues/71/createmonthlygittag-create-monthly-tag
*** 2017-05-24: #82: BackupDBDemoEnv and PollDemoDBMetric: when wrong db credential, the scripts give false postive: 2h
  CLOSED: [2017-05-24 Wed 09:13]
https://bitbucket.org/nubesecure/devops/issues/82/backupdbdemoenv-and-polldemodbmetric-when
*** 2017-05-26: #89 Run hourly GUI login test for demo env: change back repo setting: 3h
  CLOSED: [2017-05-26 Fri 16:46]
https://bitbucket.org/nubesecure/devops/issues/89/run-hourly-gui-login-test-for-demo-env
*** 2017-05-26: #4 [STORY] - Prepare deployment scheme for Odyssey and Sahara onto one of our Demo VMs: 4h + 2h
  CLOSED: [2017-05-26 Fri 16:46]
https://bitbucket.org/nubesecure/sandbox/issues/4/story-prepare-deployment-scheme-for

ssh -p 2702 root@sandbox.shibgeek.com

/Users/mac/backup/soteria_code/sandbox-soterianetworks/scripts/docker-entrypoint.sh

/Users/mac/backup/soteria_code/sandbox-soterianetworks/scripts/docker-healthcheck.sh

docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test --entrypoint=/bin/bash soterianetworks/sandbox:base_images

docker exec -it my-test bash
**** 2017-05-26: create all jenkins jobs for each project: code, deployment
   CLOSED: [2017-05-25 Thu 10:01]
*** 2017-05-26: #90 Configure Demo VM to support unicode character set: 3h
  CLOSED: [2017-05-26 Fri 19:16]
https://bitbucket.org/nubesecure/devops/issues/90/configure-demo-vm-to-support-unicode

*** 2017-05-27: #4 Prepare deployment scheme for Odyssey and Sahara onto one of our Demo VMs: 3h
  CLOSED: [2017-05-27 Sat 10:58]
https://bitbucket.org/nubesecure/sandbox/issues/4/story-prepare-deployment-scheme-for

ssh -p 2702 root@sandbox.shibgeek.com

mkdir -p /opt/soteria/sandbox
cd /opt/soteria/sandbox
git clone git@bitbucket.org:nubesecure/devops.git
cd devops/deployment/env_conf/sandbox_env
docker-compose up -d
*** #  --8<-------------------------- separator ------------------------>8--

** 2017-06
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-06-01: mdm.yml update for UI node switch into worker nodes
   CLOSED: [2017-06-01 Thu 18:58]
*** 2017-06-01: #59 ShellCheckBrozton: fix issues found by shellcheck: 3h: change back repo: http://jenkins.shibgeek.com:48084/job/ShellCheckBrozton/5/console
  CLOSED: [2017-06-01 Thu 17:13]
https://bitbucket.org/nubesecure/brozton/issues/59/shellcheckbrozton-fix-issues-found-by
*** 2017-06-01: #92 Fix all code static check issue and enable the jenkins jobs: 2h
  CLOSED: [2017-06-01 Thu 22:57]
https://bitbucket.org/nubesecure/devops/issues/92/fix-all-code-static-check-issue-and-enable
*** 2017-06-01: #76 RefreshSelectiveContainers: Update selective docker containers From Jenkins GUI: 3h
  CLOSED: [2017-06-01 Thu 23:00]
https://bitbucket.org/nubesecure/devops/issues/76/refreshselectivecontainers-update
*** 2017-06-01: #85 RefreshDemoEnv job: run whole demo env upgrade from Jenkins: 3h
  CLOSED: [2017-06-01 Thu 23:03]
https://bitbucket.org/nubesecure/devops/issues/85/refreshdemoenv-job-run-whole-demo-env

*** 2017-06-01: #26 [STORY] - Refactor docker hub setup and CI process: 2h
  CLOSED: [2017-06-01 Thu 23:04]
https://bitbucket.org/nubesecure/devops/issues/26/story-refactor-docker-hub-setup-and-ci
*** 2017-06-02: #77 CheckDockerImageSize: detect any critical docker images are too big: 1h
  CLOSED: [2017-06-02 Fri 08:26]
https://bitbucket.org/nubesecure/devops/issues/77/checkdockerimagesize-detect-any-critical

*** 2017-06-03: enforce nightly deployment for sandbox env: sahara and odyssey: 1h
  CLOSED: [2017-06-03 Sat 07:06]
*** 2017-06-03: #97: [STORY] - Finalize ioT portal deployment to second VM: setup jenkins jobs: 3h
  CLOSED: [2017-06-03 Sat 10:24]
https://bitbucket.org/nubesecure/devops/issues/97/story-finalize-iot-portal-deployment-to

cd /tmp
git clone git@bitbucket.org:dennyzhang001/devops-soterianetworks.git
cd devops-soterianetworks
git checkout denny-iot-demo
cd deployment/env_conf/iot_env
*** 2017-06-04: digitlaocean has rebooted our vm, fix epxlore env issue
  CLOSED: [2017-06-04 Sun 07:52]
Uptime Robot APP [1:30 AM] 
Monitor is DOWN: totvs_explore ( https://explore.carol.ai/mdm-ui/ )

[1:31] 
Monitor is DOWN: totvs_explore_ux ( https://ux.carol.ai/mdm-ui/ )

Denny Zhang
[6:48 AM] 
digitalocean has restarted explore env. #devop-digitalocean

Checking.

*** 2017-06-04: retire app-05, app-06, app-07 and app-08
  CLOSED: [2017-06-04 Sun 07:52]

*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-06-04: Issue #106 Nightly deployment process should pull latest git code for docker-compose envs: 3h
  CLOSED: [2017-06-04 Sun 17:11]
https://bitbucket.org/nubesecure/devops/issues/106/nightly-deployment-process-should-pull
*** 2017-06-05: setup 3 VM: move jenkins and sanbodx there, and leave 2nd VM for IOT only: 3h
  CLOSED: [2017-06-05 Mon 10:06]
https://bitbucket.org/nubesecure/devops/wiki/Critical%20Server%20List
*** 2017-06-05: Wrap up jenkins jobs as pipelines: 3h
  CLOSED: [2017-06-05 Mon 11:02]

*** 2017-06-05: Setup weekly demo meeting notification: 1h
  CLOSED: [2017-06-05 Mon 13:43]
*** 2017-06-06: retire more nodes: leave CB node for future
  CLOSED: [2017-06-06 Tue 10:48]
| ~~bematech-do-app-4~~   | ~~138.68.241.121~~  | ~~10.138.240.218~~ | ~~Type2~~      | ~~app worker            | ~~2017/06/05~~    |
| ~~bematech-do-es-16~~   | ~~138.68.243.14~~   |                    | ~~Type4~~      | ~~Elasticsearch~~       | ~~2017/06/05~~    |
| ~~bematech-do-es-17~~   | ~~138.197.199.33~~  |                    | ~~Type4~~      | ~~Elasticsearch~~       | ~~2017/06/05~~    |
| ~~bematech-do-cb-09~~   | ~~165.227.8.68~~    | ~~10.138.168.106~~ | ~~Type3~~      | ~~Couchbase~~           | ~~2017/06/05~~    |

| ~~bematech-do-es-14~~   | ~~138.68.235.207~~  |                    | ~~Type4~~      | ~~Elasticsearch~~       | ~~2017/06/05~~    |
| ~~bematech-do-es-15~~   | ~~138.68.239.246~~  |                    | ~~Type4~~      | ~~Elasticsearch~~       | ~~2017/06/05~~    |
**** 2017-06-06: es-16
   CLOSED: [2017-06-05 Mon 15:23]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.243.14"
   }
}'
**** 2017-06-06: es-17
   CLOSED: [2017-06-05 Mon 17:17]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.197.199.33"
   }
}'
**** CANCELED bematech-do-cb-09
    CLOSED: [2017-06-06 Tue 10:48]

*** 2017-06-06: retire one more ES node
  CLOSED: [2017-06-06 Tue 13:51]
bematech-do-es-13	138.68.8.192	10.138.136.59	Type4	Elasticsearch
**** 2017-06-06: bematech-do-es-13
   CLOSED: [2017-06-06 Tue 13:51]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.8.192"
   }
}'
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-06-06: node list response, role, hostname
  CLOSED: [2017-06-06 Tue 14:25]

*** 2017-06-06: #108 Capature the error message of docker container logs and send alerts: 2h
  CLOSED: [2017-06-06 Tue 21:17]
https://bitbucket.org/nubesecure/devops/issues/108/capature-the-error-message-of-docker
*** 2017-06-07: update mdm cluster env: DNS issue
  CLOSED: [2017-06-07 Wed 23:27]
Kung Wang [8:19 PM] 
@denny.zhang

for this ticket:
https://trello.com/c/xel5NcRR/2629-2-add-new-tenant-in-bematech-cluster-need-to-change-dns-to-correct-ip

I have verified that BE already working fine if we have correct settings in this place in mdm.yml:

       # default A record when creating new company CNAME records
        defaultCNameARecord: "bematech.carol.ai."

so, for each cluster, we need to create one A record that indicate this cluster, then we put that name in mdm.yml for that cluster. Then, for all tenants created in that cluster, will use this settings and create CNAME record for it.

So, please make changes to your deployment, so we can have correct DNS settings for new tenants created.

Denny Zhang [8:20 PM] 
Let me read it

[8:21] 
“for each cluster, we need to create one A record that indicate this cluster”

This is a manual step. Right?

[8:21] 
Or the application will call AWS Route53 API to do that?

Kung Wang
[8:22 PM] 
it's a manual step, as we don't create cluster all day long, so when we deploy a new cluster, we need to do this

Denny Zhang
[8:22 PM] 
OK.

And for testing env, should we leave defaultCNameARecord as empty?

Kung Wang
[8:23 PM] 
for testing environment, if not testing DNS, then we should set this to false:

374     route53Configuration:
375         # enable DNS service
376         enabled: false

Denny Zhang [8:23 PM] 
Got it. It makes sense.

[8:24] 
For testing envs, we will that enabled as false and leave defaultCNameARecord as empty.

[8:24] 
Sounds good?

Kung Wang
[8:24 PM] 
that is fine

Denny Zhang
[8:24 PM] 
Nice. Will handle it late today or early tomorrow.

Kung Wang
[8:24 PM] 
thank you Denny

Denny Zhang
[8:25 PM] 
Also update related wiki. :slightly_smiling_face:
*** 2017-06-07: #102: Automate daily deployment test for penroz project: 3h + 2h + 3h + 4h: http://injenkins.carol.ai:48080/job/DigitalOceanDeployCookbooks/92/console
  CLOSED: [2017-06-07 Wed 23:43]
*** 2017-06-08: #94 Kumkus docker image build failed: cp error: 2h
  CLOSED: [2017-06-08 Thu 13:26]
https://bitbucket.org/nubesecure/kumku-u/issues/94/kumkus-docker-image-build-failed-cp-error
*** 2017-06-08: (2) For all Bematech Jenkins jobs, avoid configure server list parameter manually
  CLOSED: [2017-06-08 Thu 15:37]
http://bematech-do-jenkins.carol.ai:18080/job/dennyExamineHostsFilesBematechDO/3/console

https://trello.com/c/jfsESGVA/2654-2-for-all-bematech-jenkins-jobs-avoid-configure-server-list-parameter-manually

ssh -p 2702 root@bematech-do-jenkins.carol.ai

> /tmp/test.py && vim /tmp/test.py
*** 2017-06-08: api to combine mdmui and mdmworker: http://injenkins.carol.ai:48080/job/DockerDeployFeatureCookbooks/673/console
  CLOSED: [2017-06-08 Thu 18:17]
*** 2017-06-08: api to get a nagios server
  CLOSED: [2017-06-08 Thu 18:17]
*** 2017-06-09: #107 In demo-env, we have dangling volumes: 2h
  CLOSED: [2017-06-09 Fri 10:06]
https://bitbucket.org/nubesecure/devops/issues/107/in-demo-env-we-have-dangling-volumes
**** demo-env
root@shibgeek-demo-2722:/var/lib/docker/volumes# ls -lth
total 128K
drwxr-xr-x 3 root root 4.0K Jun  9 07:55 demoproxy_piper_home
-rw------- 1 root root  64K Jun  9 07:55 metadata.db
drwxr-xr-x 3 root root 4.0K Jun  9 07:55 demoproxy_launch_home
drwxr-xr-x 3 root root 4.0K Jun  4 02:38 demoapplication_idp_home
drwxr-xr-x 3 root root 4.0K May 27 00:10 demoapplication_db_home
drwxr-xr-x 3 root root 4.0K May 26 23:46 636c87e5d97056515352dece8c7614776d04b4b98fe03b540507c3ea7a00d945
drwxr-xr-x 3 root root 4.0K May 26 23:46 bf8cad87fe3d702b6c0abed140d2a2698779c0197e73791216875826088adf2e
drwxr-xr-x 3 root root 4.0K May 26 23:46 9c3a9c3bc516a632d8278cdfa52c8162257e061ee599e1afe974460f3d8f0f2f
drwxr-xr-x 3 root root 4.0K May 26 23:46 ee466773343c903b1c0bcbdba0cd711f75170071eaf24b65e0588bb8dde90d17
drwxr-xr-x 3 root root 4.0K May 26 23:46 c69d393348aa8406f6ad910923e03703bab3f86c0b4ce747a7d94423651b1e3e
drwxr-xr-x 3 root root 4.0K May 26 23:46 2e2e8d673bdaef752e223debd09b4947104a390808f76f0c1a063e3c93b82e7f
drwxr-xr-x 3 root root 4.0K May 17 17:55 demoapplication_mail_home
drwxr-xr-x 3 root root 4.0K May 17 17:46 demologging_elasticsearch_repo_volume
drwxr-xr-x 3 root root 4.0K May 17 17:46 demologging_elasticsearch_data_volume
drwxr-xr-x 3 root root 4.0K May 16 15:14 demoapplication_cache_home
drwxr-xr-x 3 root root 4.0K Apr 27 03:37 1e19035ecaf3aa741e8d0bbd3e11fe0c25ec033d4e3a3e5bfd03066cf4f89c6b
drwxr-xr-x 3 root root 4.0K Apr 27 03:37 474d7e887e9f1b0445a306b6619e34097eabdc91df821b18442f41d8ec68f521
drwxr-xr-x 3 root root 4.0K Apr 27 03:37 2607fcd3407ff55fde850ba28f46bc2c879e46083a49bc2a6bda2b7360c0cd29
drwxr-xr-x 3 root root 4.0K Apr 27 03:37 3d03dbc4f98a6246779cab6cc5bc5e0b22c9a9b972888103ea519ff3ccebd551
drwxr-xr-x 3 root root 4.0K Apr 27 03:37 4a40877332d1c04ff7b211a1b67bca277a5481233e7d956426ad63b8787d1e76
drwxr-xr-x 3 root root 4.0K Apr 27 03:37 63aba0fa677833200270b76274688657eac9b514ef70f41e7a6d0b9d6c194e2f

root@shibgeek-demo-2722:/var/lib/docker/volumes# docker volume ls
DRIVER              VOLUME NAME
local               1e19035ecaf3aa741e8d0bbd3e11fe0c25ec033d4e3a3e5bfd03066cf4f89c6b
local               2607fcd3407ff55fde850ba28f46bc2c879e46083a49bc2a6bda2b7360c0cd29
local               2e2e8d673bdaef752e223debd09b4947104a390808f76f0c1a063e3c93b82e7f
local               3d03dbc4f98a6246779cab6cc5bc5e0b22c9a9b972888103ea519ff3ccebd551
local               474d7e887e9f1b0445a306b6619e34097eabdc91df821b18442f41d8ec68f521
local               4a40877332d1c04ff7b211a1b67bca277a5481233e7d956426ad63b8787d1e76
local               636c87e5d97056515352dece8c7614776d04b4b98fe03b540507c3ea7a00d945
local               63aba0fa677833200270b76274688657eac9b514ef70f41e7a6d0b9d6c194e2f
local               9c3a9c3bc516a632d8278cdfa52c8162257e061ee599e1afe974460f3d8f0f2f
local               bf8cad87fe3d702b6c0abed140d2a2698779c0197e73791216875826088adf2e
local               c69d393348aa8406f6ad910923e03703bab3f86c0b4ce747a7d94423651b1e3e
local               demoapplication_cache_home
local               demoapplication_db_home
local               demoapplication_idp_home
local               demoapplication_mail_home
local               demologging_elasticsearch_data_volume
local               demologging_elasticsearch_repo_volume
local               ee466773343c903b1c0bcbdba0cd711f75170071eaf24b65e0588bb8dde90d17

*** 2017-06-09: #95: When refreshing ui image in demo env, we don't seem to see the latest code change: 1h
  CLOSED: [2017-06-09 Fri 12:16]
https://bitbucket.org/nubesecure/devops/issues/95/when-refreshing-ui-image-in-demo-env-we

http://jenkins.shibgeek.com:48084/job/CodeBuildKumkus/10/console

cd kumkus
# 1. make some dummy change in source code
sed -i 's/Workbench/Workbench123/g' launch/src/index.html

# 2. build docker build to simulate docker hub repo locally
time docker build --no-cache -t soterianetworks/kumku-u:base_images --rm=true .

# 3. Start a container from the image
docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test --entrypoint=/bin/bash soterianetworks/kumku-u:base_images

# 4. Confirm the change is already there: we should be seeing: Workbench123
docker exec my-test grep Workbench /usr/share/nginx/html/index.html
*** 2017-06-09: monitor the change of previousacitvesprint
  CLOSED: [2017-06-09 Fri 12:17]
*** 2017-06-10: #96: A proxy container restart is required when we update a downstream server, i.e. idp: 3h
  CLOSED: [2017-06-10 Sat 16:29]
https://bitbucket.org/nubesecure/devops/issues/96/a-proxy-container-restart-is-required-when

1. Brozton dev repo, start docker-compose
2. Manually stop idp
```
docker-compose stop idp
docker-compose start idp
```
3. Wait for 10 minutes, and verify all docker healthcheck has passed
```
docker-compose ps
docker ps
```
4. Verify GUI login


# verify docker healthcheck
docker ps

domain_name="dev.penroz.com"
bash /usr/sbin/selenium_gui_test.sh "http://$domain_name" \
"/root/deployment_test/soteria/devops/test/gui_test/scripts/selenium_load_page.py" "30" \
"seleinum" "$domain_name:172.17.0.1"

#+BEGIN_EXAMPLE
bash-4.3# curl http://localhost:80/idp/
<html>
<head><title>502 Bad Gateway</title></head>
<body bgcolor="white">
<center><h1>502 Bad Gateway</h1></center>
<hr><center>nginx/1.13.1</center>
</body>
</html>
#+END_EXAMPLE

ping idp
# 172.18.0.7

curl http://idp/idp/
curl http://172.18.0.7/idp/
curl -v http://localhost:80/idp/

docker exec idp ifconfig | grep inet
docker stop idp
docker start idp

- oauth:
curl -v http://localhost:80/oauth/

- gateway
curl -v http://localhost:80/gateway/

cat /etc/nginx/conf.d/default.conf
vi  /etc/nginx/conf.d/default.conf

*** 2017-06-11: Updated jenkins and 3 VMs timezone to PDT: 1h
  CLOSED: [2017-06-11 Sun 21:20]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-06-12: #93: In daily deployment test, verify not only docker healthcheck, but also GUI simulation login: 2h
  CLOSED: [2017-06-12 Mon 09:32]
https://bitbucket.org/nubesecure/devops/issues/93/in-daily-deployment-test-verify-not-only
*** 2017-06-13: SoteriaNetworks: all-in-one deployment test: 2h + 3h
  CLOSED: [2017-06-13 Tue 19:42]
*** 2017-06-16: add a new ES node to Bematech Env
  CLOSED: [2017-06-16 Fri 10:41]
bematech-do-es-12   |  138.68.234.181
*** 2017-06-16: run ES migration for Explore env
  CLOSED: [2017-06-15 Thu 14:51]
172.17.0.3
ssh -i /var/lib/jenkins/.ssh/ci_id_rsa -p 6022 root@165.227.15.216
**** ES first round of re-indexing 
**** first run with all indices: http://prodjenkins.carol.ai:18080/view/All/job/ESIndexMigration/6/console
staging-index-98b28be03c1411e783570242ac110002
master-index-5b010c60f21111e6a49e0242ac110003
staging-index-5b010c60f21111e6a49e0242ac110003
staging-index-ce7aa4a0208611e7808d0242ac110003
master-index-98b28be03c1411e783570242ac110002
staging-index-e4010da4110ba377d100f050cb4440db
master-index-e4010da4110ba377d100f050cb4440db
staging-index-b5c8d1f04a2211e78a470242ac110003
master-index-8cd6e43115e9416eb23609486fa053e3
staging-index-8cd6e43115e9416eb23609486fa053e3
master-index-b5c8d1f04a2211e78a470242ac110003
master-index-ce7aa4a0208611e7808d0242ac110003
**** some indices were good: staging-index-98b28be03c1411e783570242ac110002, master-index-5b010c60f21111e6a49e0242ac110003
**** [#A] staging-index-5b010c60f21111e6a49e0242ac110003 has issues
#+BEGIN_EXAMPLE
Denny Zhang
[1:46 PM] 
ES cluster is yellow.

[1:46] 
Checking, should be fine.

[1:51] 
 ```2017-06-15 14:42:02 + jq --sort-keys .
2017-06-15 14:42:02 + index_type=staging
2017-06-15 14:42:02 + java -jar /root/fix-mappings-reindex-1.0.jar staging staging-index-5b010c60f21111e6a49e0242ac110003 ./mapping_sorted.json ./settings.json
2017-06-15 14:42:02 Getting Staging Mapping
2017-06-15 14:42:03 Fixing Staging Mapping
2017-06-15 14:42:03 java.lang.NullPointerException
2017-06-15 14:42:03     at com.totvslabs.mdm.tools.mappings.fix.FixMappings.fixStagingMappings(FixMappings.java:319)
2017-06-15 14:42:03 ERROR: updated mappings is empty
2017-06-15 14:42:03     at com.totvslabs.mdm.tools.mappings.fix.FixMappings.main(FixMappings.java:64)
2017-06-15 14:42:03 + log 'create new index with settings and mappings'
2017-06-15 14:42:03 + local 'msg=create new index with settings and mappings'
```

[1:52] 
Got issue to run the jar program in explore env.

It fails at staging-index-5b010c60f21111e6a49e0242ac110003.
The json files are placed in /tmp/staging-index-5b010c60f21111e6a49e0242ac110003/

@bruno & @kungwang , culd you take a look?

[1:53] 
How to login

```ssh -p 2702  root@explore.carol.ai
docker exec -it mdm-all-in-one bash
cd /tmp/staging-index-5b010c60f21111e6a49e0242ac110003/```
#+END_EXAMPLE
**** keep going with the following indices: http://prodjenkins.carol.ai:18080/view/All/job/ESIndexMigration/7/console
staging-index-ce7aa4a0208611e7808d0242ac110003
master-index-e4010da4110ba377d100f050cb4440db
master-index-b5c8d1f04a2211e78a470242ac110003
staging-index-8cd6e43115e9416eb23609486fa053e3
master-index-98b28be03c1411e783570242ac110002
staging-index-e4010da4110ba377d100f050cb4440db
staging-index-b5c8d1f04a2211e78a470242ac110003
master-index-8cd6e43115e9416eb23609486fa053e3
master-index-ce7aa4a0208611e7808d0242ac110003
**** [#A] Index has issues: staging-index-ce7aa4a0208611e7808d0242ac110003
**** keep going with following indices: http://prodjenkins.carol.ai:18080/view/All/job/ESIndexMigration/8/console
master-index-e4010da4110ba377d100f050cb4440db
master-index-b5c8d1f04a2211e78a470242ac110003
staging-index-8cd6e43115e9416eb23609486fa053e3
master-index-98b28be03c1411e783570242ac110002
staging-index-e4010da4110ba377d100f050cb4440db
staging-index-b5c8d1f04a2211e78a470242ac110003
master-index-8cd6e43115e9416eb23609486fa053e3
master-index-ce7aa4a0208611e7808d0242ac110003
**** delete staging-index-5b010c60f21111e6a49e0242ac110003-new3 and staging-index-ce7aa4a0208611e7808d0242ac110003-new3
curl -XDELETE "http://$es_ip:9200/staging-index-5b010c60f21111e6a49e0242ac110003-new3"
curl -XDELETE "http://$es_ip:9200/staging-index-ce7aa4a0208611e7808d0242ac110003-new3"
**** try again with those 2 indices: http://prodjenkins.carol.ai:18080/view/All/job/ESIndexMigration/9/console
**** stop mdm
**** run es re-indexing again: http://prodjenkins.carol.ai:18080/view/All/job/ESIndexMigration/10/console
staging-index-98b28be03c1411e783570242ac110002
master-index-5b010c60f21111e6a49e0242ac110003
staging-index-5b010c60f21111e6a49e0242ac110003
staging-index-ce7aa4a0208611e7808d0242ac110003
master-index-98b28be03c1411e783570242ac110002
staging-index-e4010da4110ba377d100f050cb4440db
master-index-e4010da4110ba377d100f050cb4440db
staging-index-b5c8d1f04a2211e78a470242ac110003
master-index-8cd6e43115e9416eb23609486fa053e3
staging-index-8cd6e43115e9416eb23609486fa053e3
master-index-b5c8d1f04a2211e78a470242ac110003
master-index-ce7aa4a0208611e7808d0242ac110003
**** CANCELED es audit: 
   CLOSED: [2017-06-15 Thu 13:37]
**** TODO Running deployment for 1.67 with staging_datawatch as true: http://explorejenkins.carol.ai:18080/job/AutoDailyUpdateSandboxMDM/41/console
*** 2017-06-16: test in 3 nodes docker cluster: http://injenkins.carol.ai:48080/job/DockerDeployFeatureCookbooks/715/
  CLOSED: [2017-06-16 Fri 10:46]
cd /var/lib/jenkins/code/DockerDeployFeatureCookbooks/1.66/mdmdevops/cookbooks/mdm-cluster
export INSTANCE_NAME=mdm-cluster-DockerDeployFeatureCookbooks-715
export KITCHEN_YAML=.kitchen.yml
export branch_name=1.67
export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true

export TEST_KITCHEN_YAML=".kitchen.yml"
# export DOCKER_PORT_FORWARD_PREFIX=31
export PACKAGE_URL='http://172.17.0.1:18000'
export APP_BRANCH_NAME=1.67
export FRAMEWORK_BRANCH_NAME=1.67
export CLUSTER_ID=kitchen-mdm-feature
export WHETHER_CHECK_LOG=false
**** disable mdm flush
sed -i 's/flushDatabaseOnConnect: true/flushDatabaseOnConnect: false/g' /opt/mdm/config/mdm.yml

sed -i 's/whether_flushdatabaseonconnect: \"true\",/whether_flushdatabaseonconnect: \"false\",/g' .kitchen.yml

grep whether_flushdatabaseonconnect .kitchen.yml
**** es cluster
http://injenkins.carol.ai:48080/job/ESIndexMigration/35/console

master-index-098f6bcd4621d373cade4e832627b4f6
staging-index-13a1f8adbec032ed68f3d035449ef48d
master-index-13a1f8adbec032ed68f3d035449ef48d
master-index-46078234297e400a1648d9c427dc8c4b
staging-index-098f6bcd4621d373cade4e832627b4f6
staging-index-e4010da4110ba377d100f050cb4440db
master-index-e4010da4110ba377d100f050cb4440db
staging-index-46078234297e400a1648d9c427dc8c4b
master-index-8cd6e43115e9416eb23609486fa053e3
staging-index-8cd6e43115e9416eb23609486fa053e3

- Before
#+BEGIN_EXAMPLE
0000-00-00 00:00:00 health status index                                          pri rep docs.count docs.deleted store.size pri.store.size 
0000-00-00 00:00:00 green  open   master-index-098f6bcd4621d373cade4e832627b4f6    2   1          1            0     21.4kb          8.3kb 
0000-00-00 00:00:00 green  open   staging-index-13a1f8adbec032ed68f3d035449ef48d   2   1          0            0       578b           289b 
0000-00-00 00:00:00 green  open   master-index-13a1f8adbec032ed68f3d035449ef48d    2   1          1            0       24kb          9.5kb 
0000-00-00 00:00:00 green  open   master-index-46078234297e400a1648d9c427dc8c4b    2   1          3            0     50.2kb         25.5kb 
0000-00-00 00:00:00 green  open   staging-index-098f6bcd4621d373cade4e832627b4f6   2   1          0            0       578b           289b 
0000-00-00 00:00:00 green  open   staging-index-e4010da4110ba377d100f050cb4440db   2   1     112903            4    302.8mb        150.2mb 
0000-00-00 00:00:00 green  open   master-index-e4010da4110ba377d100f050cb4440db    2   1        450           19      2.7mb          1.2mb 
0000-00-00 00:00:00 green  open   staging-index-46078234297e400a1648d9c427dc8c4b   2   1      50663            3    136.6mb         67.3mb 
0000-00-00 00:00:00 green  open   master-index-8cd6e43115e9416eb23609486fa053e3    2   1       2398           30      5.9mb            3mb 
0000-00-00 00:00:00 green  open   staging-index-8cd6e43115e9416eb23609486fa053e3   2   1          1            0    310.6kb         84.5kb 
#+END_EXAMPLE

- After
#+BEGIN_EXAMPLE
green  open   staging-index-46078234297e400a1648d9c427dc8c4b-new3   4   1      50663            0    119.9mb         59.9mb
green  open   staging-index-13a1f8adbec032ed68f3d035449ef48d-new3   4   1          0            0      1.2kb           636b
green  open   staging-index-e4010da4110ba377d100f050cb4440db-new3   4   1     112903            0    268.4mb        134.1mb
green  open   master-index-e4010da4110ba377d100f050cb4440db-new3    4   1        384            0      2.4mb          1.2mb
green  open   staging-index-098f6bcd4621d373cade4e832627b4f6-new3   4   1          0            0      1.2kb           636b
green  open   master-index-098f6bcd4621d373cade4e832627b4f6-new3    4   1          1            0     16.1kb            8kb
green  open   master-index-13a1f8adbec032ed68f3d035449ef48d-new3    4   1          1            0     18.5kb          9.2kb
green  open   master-index-8cd6e43115e9416eb23609486fa053e3-new3    4   1       2098            0      5.1mb          2.6mb
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3-new3   4   1          1            0    167.2kb         83.6kb
green  open   master-index-46078234297e400a1648d9c427dc8c4b-new3    4   1          3            0     48.4kb         24.2kb
#+END_EXAMPLE
**** es audit
http://injenkins.carol.ai:48080/job/ESIndexMigration/36/console

audit-index-e4010da4110ba377d100f050cb4440db
audit-index-46078234297e400a1648d9c427dc8c4b
audit-index-13a1f8adbec032ed68f3d035449ef48d
audit-index-8cd6e43115e9416eb23609486fa053e3
audit-index-098f6bcd4621d373cade4e832627b4f6

- Before
#+BEGIN_EXAMPLE
health status index                                        pri rep docs.count docs.deleted store.size pri.store.size
green  open   audit-index-e4010da4110ba377d100f050cb4440db   2   1          6            0     41.7kb         20.8kb
green  open   audit-index-46078234297e400a1648d9c427dc8c4b   2   1          0            0       636b           318b
green  open   audit-index-13a1f8adbec032ed68f3d035449ef48d   2   1          0            0       636b           318b
green  open   audit-index-8cd6e43115e9416eb23609486fa053e3   2   1         36            0    143.5kb         68.2kb
green  open   audit-index-098f6bcd4621d373cade4e832627b4f6   2   1          0            0       636b           318b
#+END_EXAMPLE

- After
#+BEGIN_EXAMPLE
green  open   audit-index-8cd6e43115e9416eb23609486fa053e3-new3   4   1         18            0     71.6kb         35.8kb
green  open   audit-index-098f6bcd4621d373cade4e832627b4f6-new3   4   1          0            0      1.2kb           636b
green  open   audit-index-13a1f8adbec032ed68f3d035449ef48d-new3   4   1          0            0      1.2kb           636b
green  open   audit-index-46078234297e400a1648d9c427dc8c4b-new3   4   1          0            0      1.2kb           636b
green  open   audit-index-e4010da4110ba377d100f050cb4440db-new3   4   1          2            0     39.4kb         19.7kb
#+END_EXAMPLE
**** stop services in app02 and app03
service mdm stop
**** Reindex ES and ES audit
http://injenkins.carol.ai:48080/job/ESIndexMigration/37/console
http://injenkins.carol.ai:48080/job/ESIndexMigration/38/console
**** upgrade mdm from jenkins
git config remote.origin.url git@github.com:TOTVS/mdmdevops.git
git branch 1.67
git checkout 1.67
git pull origin 1.67

kitchen converge

curl http://127.0.0.1:8081/admin/healthcheck
*** 2017-06-16: test in 3 nodes digitalocean cluster: http://injenkins.carol.ai:48080/job/DigitalOceanDeployCookbooks/96/console
  CLOSED: [2017-06-16 Fri 10:46]
cd /var/lib/jenkins/code/DigitalOceanDeployCookbooks/1.66/mdmdevops/cookbooks/mdm-cluster
export INSTANCE_NAME=mdm-cluster-DigitalOceanDeployCookbooks-96
export KITCHEN_YAML=.kitchen.digitalocean_3nodes.yml

export branch_name=1.67
export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true

export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true
export CLEAN_START=true

export PACKAGE_URL='http://injenkins.carol.ai:18000'
export TEST_KITCHEN_YAML=".kitchen.digitalocean.yml,.kitchen.digitalocean_3nodes.yml"
export DIGITALOCEAN_SSH_KEY_IDS="1968722,979830,812123"
export DIGITALOCEAN_FLAVOR=8gb
export APP_BRANCH_NAME=1.67
export FRAMEWORK_BRANCH_NAME=1.67
export CURRENT_ACTIVE_SPRINT=1.67

export DIGITALOCEAN_ACCESS_TOKEN="d535bf5bf949b8c785d2f4d54aa5e0397967f675b766ff944b3e4ed673a5148a"

ssh -p 22 root@138.197.209.164

138.197.209.164
**** preparation
mkdir /home/denny/
cd /home/denny/
git clone git@github.com:TOTVS/mdmdevops.git

git checkout 1.68-deploy

wget -O /root/fix-mappings-reindex-1.0.jar http://injenkins.carol.ai:18000/fix-mappings-reindex-1.0.jar
**** disable mdm flush
sed -i 's/flushDatabaseOnConnect: true/flushDatabaseOnConnect: false/g' /opt/mdm/config/mdm.yml
**** es cluster
http://injenkins.carol.ai:48080/job/ESIndexMigration/26/console

master-index-098f6bcd4621d373cade4e832627b4f6
staging-index-13a1f8adbec032ed68f3d035449ef48d
master-index-13a1f8adbec032ed68f3d035449ef48d
master-index-46078234297e400a1648d9c427dc8c4b
staging-index-098f6bcd4621d373cade4e832627b4f6
staging-index-e4010da4110ba377d100f050cb4440db
staging-index-46078234297e400a1648d9c427dc8c4b
master-index-e4010da4110ba377d100f050cb4440db
master-index-8cd6e43115e9416eb23609486fa053e3
staging-index-8cd6e43115e9416eb23609486fa053e3
**** es audit
http://injenkins.carol.ai:48080/view/All/job/ESIndexMigration/27/

audit-index-e4010da4110ba377d100f050cb4440db
audit-index-46078234297e400a1648d9c427dc8c4b
audit-index-13a1f8adbec032ed68f3d035449ef48d
audit-index-8cd6e43115e9416eb23609486fa053e3
audit-index-098f6bcd4621d373cade4e832627b4f6
**** stop services in app02 and app03
service mdm stop
**** Reindex ES and ES audit
http://injenkins.carol.ai:48080/job/ESIndexMigration/28/
http://injenkins.carol.ai:48080/job/ESIndexMigration/29
**** upgrade mdm from jenkins
git config remote.origin.url git@github.com:TOTVS/mdmdevops.git
git branch 1.67
git checkout 1.67
git pull origin 1.67

kitchen converge
*** 2017-06-16: [#A] Trouble shooting Proxy service issue: 3h
  CLOSED: [2017-06-16 Fri 14:21]
*** 2017-06-16: build twice to verify the size
  CLOSED: [2017-06-16 Fri 16:08]
#+BEGIN_EXAMPLE
Denny Zhang [2:30 PM] 
Hey @kungwang @bruno

About the size issue of mdm jar, I’ve finished run 2 code build against 1.68.

#1429: app-1.68.0-SNAPSHOT-1429.jar (219M 229566671 bytes). Revision: 928edc2f3ff9b3610cf89c1b06249c49063bfa9d
Jenkins job: http://injenkins.carol.ai:48080/job/BuildMDMRepoCodeActiveSprint/1429/
Download link: http://injenkins.carol.ai:18000/app-1.68.0-SNAPSHOT-1429.jar

#1430: app-1.68.0-SNAPSHOT-1430.jar (220M 230624214 bytes). Revision: 928edc2f3ff9b3610cf89c1b06249c49063bfa9d
Jenkins job: http://injenkins.carol.ai:48080/job/BuildMDMRepoCodeActiveSprint/1430/
Download link: http://injenkins.carol.ai:18000/app-1.68.0-SNAPSHOT-1430.jar
#+END_EXAMPLE
*** 2017-06-17: [#A] verify Mitu's ES migration procedure: https://github.com/TOTVS/mdm/wiki/Fixing-Mappings-and-Reindexing
  CLOSED: [2017-06-17 Sat 22:49]
scp -P 2702 fix-mappings-reindex-1.0.jar  root@injenkins.carol.ai:/home/denny/
docker cp /home/denny/fix-mappings-reindex-1.0.jar docker-jenkins:/var/www/repo/

curl -I  http://injenkins.carol.ai:18000/fix-mappings-reindex-1.0.jar
wget -O /root/fix-mappings-reindex-1.0.jar http://injenkins.carol.ai:18000/fix-mappings-reindex-1.0.jar

export old_index_name="

export old_index_name="master-index-098f6bcd4621d373cade4e832627b4f6"
export old_index_name="staging-index-13a1f8adbec032ed68f3d035449ef48d"
export old_index_name="master-index-13a1f8adbec032ed68f3d035449ef48d"
export old_index_name="master-index-46078234297e400a1648d9c427dc8c4b"
export old_index_name="staging-index-098f6bcd4621d373cade4e832627b4f6"
export old_index_name="staging-index-e4010da4110ba377d100f050cb4440db"
export old_index_name="staging-index-46078234297e400a1648d9c427dc8c4b"
export old_index_name="master-index-e4010da4110ba377d100f050cb4440db"
export old_index_name="master-index-8cd6e43115e9416eb23609486fa053e3"
export old_index_name="staging-index-8cd6e43115e9416eb23609486fa053e3"
**** before
root@kitchen-mdm-feature-node1:/# curl $es_ip:9200/_cat/indices?v
health status index                                          pri rep docs.count docs.deleted store.size pri.store.size
green  open   master-index-098f6bcd4621d373cade4e832627b4f6    2   1          1            0     27.6kb          8.3kb
green  open   staging-index-13a1f8adbec032ed68f3d035449ef48d   2   1          0            0       578b           289b
green  open   master-index-13a1f8adbec032ed68f3d035449ef48d    2   1          1            0     23.9kb          9.5kb
green  open   master-index-46078234297e400a1648d9c427dc8c4b    2   1          3            0     50.2kb         25.5kb
green  open   staging-index-098f6bcd4621d373cade4e832627b4f6   2   1          0            0       578b           289b
green  open   staging-index-e4010da4110ba377d100f050cb4440db   2   1     112903            4    300.1mb        151.2mb
green  open   staging-index-46078234297e400a1648d9c427dc8c4b   2   1      50663            4      136mb         67.3mb
green  open   master-index-e4010da4110ba377d100f050cb4440db    2   1        450           18      2.7mb          1.1mb
green  open   master-index-8cd6e43115e9416eb23609486fa053e3    2   1       2398           25      4.8mb          2.4mb
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3   2   1          1            0    310.3kb         84.5kb* #  --8<-------------------------- separator ------------------------>8--
**** copy files
docker cp kitchen-mdm-feature-node1:/tmp/master-index-098f6bcd4621d373cade4e832627b4f6_MappingsCurrent_sorted.json /tmp/
docker cp /tmp/master-index-098f6bcd4621d373cade4e832627b4f6_MappingsCurrent_sorted.json docker-jenkins:/var/www/repo/
curl -I http://injenkins.carol.ai:18000/master-index-098f6bcd4621d373cade4e832627b4f6_MappingsCurrent_sorted.json

docker cp kitchen-mdm-feature-node1:/tmp/master-index-098f6bcd4621d373cade4e832627b4f6_MappingsCurrent.json /tmp/
docker cp /tmp/master-index-098f6bcd4621d373cade4e832627b4f6_MappingsCurrent.json docker-jenkins:/var/www/repo/
curl -I http://injenkins.carol.ai:18000/master-index-098f6bcd4621d373cade4e832627b4f6_MappingsCurrent.json

docker cp kitchen-mdm-feature-node1:/tmp/master-index-098f6bcd4621d373cade4e832627b4f6_SettingsCurrent.json /tmp/
docker cp /tmp/master-index-098f6bcd4621d373cade4e832627b4f6_SettingsCurrent.json docker-jenkins:/var/www/repo/
curl -I http://injenkins.carol.ai:18000/master-index-098f6bcd4621d373cade4e832627b4f6_SettingsCurrent.json
*** 2017-06-17: Full cb backup: http://prodjenkins.carol.ai:18080/job/BackupSystemDOProd/82/console
  CLOSED: [2017-06-17 Sat 22:49]
*** 2017-06-17: Full cb backup: http://prodjenkins.carol.ai:18080/job/BackupSystemDOProd/82/console
  CLOSED: [2017-06-17 Sat 22:49]

*** 2017-06-18: improvement for the build process: http://injenkins.carol.ai:48080/job/DockerDeployAllInOne/714/console
  CLOSED: [2017-06-18 Sun 00:15]
file size

Denny Zhang 
[3 hours ago] 
@kungwang

With above check-in, we now know:

- The git revision number for not only mdm repo, but also mdm framework
- Track the cksum of generated artifacts. For each deployment, we will have a updated file of /opt/mdm/config/build_version.

Sample output:
```Build From 1.66-be-2382-annotations-and-reindex branch of mdm repo.
Revision: ff37211dd9976c1eae057bdea8772cc7c6a3113c
Build From 1.66-be-2382-annotations-and-reindex branch of totvslabs-framework repo.
Revision: cb36bd31b9b98e19c0d7fd5100656453badff0cc
Build Time: 2017-06-16 05:46:58
Jenkins Job: OfficialBuildMDMRepoReindexBranch:#17
```

Kung Wang [7 minutes ago] 
@denny.zhang , this is good, and if it has the last commit id in the build, that would be even better.

Kung Wang 
[6 minutes ago] 
but, still, from above info, we can not tell why the file size mismatch. So, maybe, we should also include file size generated, so we can compare

Denny Zhang 
[< 1 minute ago] 
Revision number is the commit id.

Right now,  the file size and cksum are displayed in the Jenkins console oputput.

Not in artifact repo server or target deployed env.

That’s a good suggestion. Let me pass the information to the deployment target machines.

*** 2017-06-18: 1.67-devops-build-cleanup: compare size: #29, #30, #31, #32: http://injenkins.carol.ai:48080/job/BuildMDMRepoCodeNoAlert/
  CLOSED: [2017-06-18 Sun 00:16]
*** 2017-06-21: Retire DO ES cluster <2017-06-21 11:54 UTC +8>
138.68.250.138
138.68.5.55
138.68.41.211
138.68.3.169

| ~~prod-es-01~~     | ~~138.68.250.138/10.138.192.138~~  | ~~Type4~~                                      | ~~Elasticsearch~~    | Jun 20th, 2017 |
| ~~prod-es-02~~     | ~~138.68.5.55/10.138.72.13~~       | ~~Type4~~                                      | ~~Elasticsearch~~    | Jun 20th, 2017 |
| ~~prod-es-03~~     | ~~138.68.41.211/10.138.72.62~~     | ~~Type4~~                                      | ~~Elasticsearch~~    | Jun 20th, 2017 |
| ~~prod-es-16~~     | ~~138.68.3.169~~                   | ~~Type4~~                                      | ~~Elasticsearch~~    | Jun 20th, 2017 |

es_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')

service elasticsearch status

curl -XPOST "http://$es_ip:9200/_flush/synced"

service elasticsearch stop

service elasticsearch start

service elasticsearch status

tail -f /var/log/elasticsearch/mdm.log
ls -lth /data/elasticsearch2
**** 2017-06-21: Stop nagios monitoring new es nodes
   CLOSED: [2017-06-21 Wed 10:37]
rm -rf 138.197.219.98 138.197.215.132 138.68.239.152 138.68.236.191
**** 2017-06-21: restart prod-es-1
   CLOSED: [2017-06-20 Tue 22:08]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.197.219.98"
   }
}'
**** 2017-06-21: restart prod-es-2
   CLOSED: [2017-06-21 Wed 08:04]
138.197.215.132

curl -XGET $es_ip:9200/_cluster/settings

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.197.215.132"
   }
}'

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : ""
   }
}'
**** 2017-06-21: restart prod-es-4
   CLOSED: [2017-06-21 Wed 08:04]
138.68.236.191

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.236.191"
   }
}'

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.include._ip" : "138.68.236.191"
   }
}'
**** 2017-06-21: restart prod-es-3
   CLOSED: [2017-06-21 Wed 08:04]
138.68.239.152

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.239.152"
   }
}'

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.include._ip" : "138.68.239.152"
   }
}'
**** 2017-06-21: Change back nagios monitoring: add new 4 ES nodes, and remove old 4 ES nodes
   CLOSED: [2017-06-21 Wed 10:37]
**** 2017-06-21: Restart existing ES nodes to use /data/elasticsearch2
   CLOSED: [2017-06-20 Tue 22:53]
**** 2017-06-21: fix elasticsearch.yml
   CLOSED: [2017-06-21 Wed 10:37]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-06-21: retire prod-es-01
   CLOSED: [2017-06-21 Wed 11:51]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.250.138"
   }
}'
**** 2017-06-21: retire prod-es-02
   CLOSED: [2017-06-21 Wed 11:51]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.5.55"
   }
}'
**** 2017-06-21: retire prod-es-03
   CLOSED: [2017-06-21 Wed 11:52]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.41.211"
   }
}'
**** 2017-06-21: retire prod-es-16
   CLOSED: [2017-06-21 Wed 11:52]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.3.169"
   }
}'
**** 2017-06-21: update firewall
   CLOSED: [2017-06-21 Wed 11:51]
**** #  --8<-------------------------- separator ------------------------>8--
**** 2017-06-21: remove nodes and volumes
   CLOSED: [2017-06-21 Wed 11:53]
**** [#A] remove indices: verify the closed indices: http://prodjenkins.carol.ai:18080/job/RunCommandOnServers/399/console
**** [#A] remove ES audit indices
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-06-22: emails issues: disable email sending in daily deployment test
  CLOSED: [2017-06-22 Thu 11:45]
*** 2017-06-23: Recreate Bematech Jenkins by docker-compose
  CLOSED: [2017-06-23 Fri 10:19]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-06-19: #60 [docker] - metadata folder is being created recursively: 3h: http://jenkins.shibgeek.com:48084/job/DeployAIOBroztonTest/14/console
  CLOSED: [2017-06-19 Mon 20:02]
https://bitbucket.org/nubesecure/brozton/issues/60/docker-metadata-folder-is-being-created

http://jenkins.shibgeek.com:48084/view/2_IntegrationTest/job/DeployAIOBroztonTest/build?delay=0sec

http://jenkins.shibgeek.com:48084/view/2_IntegrationTest/job/DeployAIOBroztonTest/22/console

ssh -i /var/jenkins_home/.ssh/deployment_test_id_rsa -p 22 root@45.79.111.239
denny-shib-image

export code_dir="/root/deployment_test/soteria/devops/test/daily_deployment_test"
export test_scenario="brozton"

cd $code_dir
bash -e ./deployment_test_${test_scenario}.sh prepare_test
bash -e ./deployment_test_${test_scenario}.sh build_image
bash -e ./deployment_test_${test_scenario}.sh deploy
bash -e ./deployment_test_${test_scenario}.sh status

bash -e ./deployment_test_${test_scenario}.sh destroy

cd /root/deployment_test/code/soteria/brozton

export docker_compose_yml="docker-compose-testing.yml"
docker-compose -f $docker_compose_yml ps

export branch_name="denny-shib-image"
docker-compose -f $docker_compose_yml logs shib

docker-compose -f $docker_compose_yml down && docker-compose -f $docker_compose_yml up -d && \
sleep 5 && \
docker-compose -f $docker_compose_yml down && docker-compose -f $docker_compose_yml up -d && \
sleep 5 && \
docker-compose -f $docker_compose_yml down && docker-compose -f $docker_compose_yml up -d

tree /var/lib/docker/volumes/*_idp_home/_data/metadata

tree /opt/brozton-idp/metadata

rm -rf /opt/brozton-idp/metadata/metadata

cp -r /opt/brozton-idp/backup/idp_home/20170619_172527/metadata/ /opt/brozton-idp/metadata

cp -r /opt/brozton-idp/backup/idp_home/20170619_172527/metadata/* /opt/brozton-idp/metadata

#+BEGIN_EXAMPLE
shib       | BUILD SUCCESSFUL
shib       | Total time: 11 seconds
shib       | + cd /opt/brozton-idp
shib       | + [ -d /opt/brozton-idp/backup/idp_home/20170619_172527/conf ]
shib       | + echo Copy back old unrecoverable files
shib       | + mkdir -p conf
shib       | Copy back old unrecoverable files
shib       | + cp /opt/brozton-idp/backup/idp_home/20170619_172527/conf/metadata-providers.xml conf/
shib       | + mkdir -p metadata
shib       | + cp -r /opt/brozton-idp/backup/idp_home/20170619_172527/metadata/ metadata/
shib       | + [ -d /opt/brozton-idp/backup/idp_home ]
shib       | + days_to_retention=30
shib       | + echo Remove backup older than 30 days
shib       | + cd /opt/brozton-idp/backup/idp_home
shib       | + find . -mtime +30 -exec rm {} ;
shib       | Remove backup older than 30 days
shib       | + cd /
shib       | + echo Keep container running
shib       | + true
shib       | + sleep 1000
shib       | Keep container running
#+END_EXAMPLE
*** 2017-06-19: Create PipelineRefreshDemoEnvAll job: integrate test into deployment pipeline: 1h
  CLOSED: [2017-06-19 Mon 11:26]
*** 2017-06-20: nightly deployment test implementation and improvement: 3h 
  CLOSED: [2017-06-20 Tue 00:10]
**** 2017-06-20: Issue: Create index has failed acknowledge: false: timeout
**** 2017-06-20: Issue: Create index has failed acknowledge: false: timeout
*** 2017-06-23: deployment and env support: 2h
  CLOSED: [2017-06-23 Fri 23:10]
*** 2017-06-22: Learn gradle: 2h
  CLOSED: [2017-06-22 Thu 16:45]
*** 2017-06-24: all-in-one deployment test improvement: when it fails, show docker logs: 1h
  CLOSED: [2017-06-24 Sat 21:19]

*** 2017-06-25: full cb backup for Prod env: http://prodjenkins.carol.ai:18080/job/BackupSystemDOProd/86/console
  CLOSED: [2017-06-25 Sun 13:21]
*** 2017-06-25: full CB backup: http://bematech-do-jenkins.carol.ai:18080/job/BackupCouchbaseDOBematech/17/
  CLOSED: [2017-06-25 Sun 13:21]
*** 2017-06-28: shutdown and remove bematech-do-es-19(165.227.14.204)
  CLOSED: [2017-06-28 Wed 10:55]
*** 2017-06-28: ES audit index issue(audit-index-40e0def05c1f11e7a9954a8136534b63) in prod DO env
  CLOSED: [2017-06-28 Wed 11:52]
Nagios APP [11:36 AM] 
prod-audit-01/check_elasticsearch_audit_health is WARNING:
WARN: elasticsearch cluster status is yellow, by checking 138.68.248.23:9400/_cluster/health

Denny Zhang
[11:39 AM] 
I’m checking above audit warning.

hmm, we have a new index created today, and the replica count is configured to 2.
```yellow open   audit-index-40e0def05c1f11e7a9954a8136534b63        5   2          0            0       795b           795b
```

Probably it’s from auto-creating. (edited)

[11:40] 
However, I’ve confirmed we have below setting in elasticsearch.yml

```index.number_of_replicas: 0
action.auto_create_index: false
```

Not sure which part creates this index. (edited)

[11:42] 
@bruno & @kungwang

Any idea?

Kung Wang [11:43 AM] 
let me check

[11:43] 
@denny.zhang , also create is turned off right?

Denny Zhang
[11:43 AM] 
We have this configured, before ES audit restart.

So I think, yes, it’s turned off.

```action.auto_create_index: false
```

Bruno Volpato
[11:44 AM] 
was the cluster restarted after changing the elasticsearch.yml file?

[11:45] 
otherwise for it to take place it should be sent through PUT _settings

Denny Zhang
[11:48 AM] 
yes, ES audit started with that configuration of elasticsearch.yml

[11:49] 
I remember it clearly.

Also we have solid evidence.

The start time of es audit is Jun20, which is the same modified time of elasticsearch.yml

[11:49] 
For now, should I close this index(audit-index-40e0def05c1f11e7a9954a8136534b63) immediately? (edited)

Kung Wang
[11:50 AM] 
yes, it's a bug, we will fix it in 1.67

Bruno Volpato
[11:50 AM] 
are you sure? we talked about this `action.auto_create_index` solution not long ago, and I don't remember last time we rebooted this cluster (edited)

[11:50] 
but it seems that this is not the issue, according to @kungwang

Denny Zhang [11:51 AM] 
@bruno

I changed the configuration manually late last week. Then I rebooted ES audit on purpose. (edited)

Bruno Volpato
[11:51 AM] 
got it

[11:51] 
thanks

Denny Zhang
[11:51 AM] 
Let me close that index now?

[11:52] 
Closed. Will remove next week.
*** 2017-06-28: Change totvslabs email password
  CLOSED: [2017-06-28 Wed 14:41]
Change.This@that

Change.That123@this

Change.This@that123
Change.That@this123
*** 2017-06-28: ToggleESShardAllocation: Create a Jenkins job to disable and enable elasticsearch shard allocation
  CLOSED: [2017-06-28 Wed 16:03]
ssh -p 2702 root@bematech-do-jenkins.carol.ai
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-06-28: #79: [Security] source code obfuscation for services: 3h
  CLOSED: [2017-06-28 Wed 16:45]
https://bitbucket.org/nubesecure/devops/issues/79/security-source-code-obfuscation-for

http://jenkins.shibgeek.com:48084/view/Pipeline/job/PipelineDeployTest/15/console

denny-code-obfuscation

./gradlew clean build -x test

> proguard/proguard.txt && vim proguard/proguard.txt

./gradlew getProguard

git pull && ./gradlew clean proguard -x test && ls -lth ./gateway/build/libs

scp -P 2702 -r root@injenkins.carol.ai:/home/denny/brozton-soterianetworks/totp/build /tmp/

ls -lth /tmp/test

./gradlew clean proguard build -x test

cd /root/deployment_test/code/soteria/brozton

export branch_name="denny-code-obfuscation"
export IMG_TAG_TEST="JENKINS_TEST_${branch_name}"

docker build --no-cache -t soterianetworks/brozton:base_${IMG_TAG_TEST} --rm=true . && \
cd idp && docker build --no-cache -f Dockerfile-jetty -t soterianetworks/brozton:idp_${IMG_TAG_TEST} --rm=true . && cd .. && \
echo $?

docker-compose down -v && \
docker-compose -f docker-compose-testing.yml up -d && \

docker stop seleinum; docker rm seleinum && \
docker run -t -d --privileged -h selenium --add-host=www.brozton.com:172.17.0.1 -v /root/deployment_test/soteria/devops/test/gui_test/scripts/selenium_load_page.py:/home/seluser/scripts/selenium_load_page.py --name seleinum denny/selenium:v1

docker exec seleinum python /home/seluser/scripts/selenium_load_page.py --page_url http://www.brozton.com --max_load_seconds 30

*** 2017-06-30: [#A] Enable shard allocation for both envs
  CLOSED: [2017-06-30 Fri 09:39]
*** 2017-06-30: heal the audit index
  CLOSED: [2017-06-30 Fri 22:31]
root@prod-audit-01:/etc/elasticsearch_audit# curl $es_ip:9400/_cat/indices?v | grep open
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  6273  100  6273    0     0   273k      0 --:--:-- --:--:-- --:--:--  278k
green  open   audit-40e0def05c1f11e7a9954a8136534b63              5   0         44            0    244.5kb        244.5kb



curl $es_ip:9400/_cat/indices?v | grep audit-40e0def05c1f11e7a9954a8136534b63
curl -XPOST "http://$es_ip:9400/audit-40e0def05c1f11e7a9954a8136534b63/_close"

curl $es_ip:9400/_cat/indices?v | grep audit-40e0def05c1f11e7a9954a8136534b63
curl -XDELETE "http://$es_ip:9400/audit-40e0def05c1f11e7a9954a8136534b63"

curl -XPOST "http://$es_ip:9400/_aliases" -d '{
"actions" : [
{"add":{"index":"audit-index-40e0def05c1f11e7a9954a8136534b63","alias":"audit-40e0def05c1f11e7a9954a8136534b63"}}]}'

curl -XGET "http://$es_ip:9400/_aliases?pretty" | grep -C 5 40e0def05c1f11e7a9954a8136534b63

** 2017-07-05: Remove bematech es-01 out of the cluster: http://bematech-do-jenkins.carol.ai:18080/job/RunCommandOnServers/11/console
  CLOSED: [2017-07-05 Wed 14:25]
# TODO: change to correct ip
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.231.232"
   }
}'
** 2017-07-05: Stop VM Retire es-01: update ufw rules; remove vm and volumes
  CLOSED: [2017-07-05 Wed 15:02]
** 2017-07-06: Explore env can't login: we have to delete the closed indices
  CLOSED: [2017-07-06 Thu 12:09]
*** es_indices
root@aio:/opt/mdm# curl $es_ip:9200/_cat/indices?v | grep 8cd6e43115e9416eb23609486fa053e3
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2125  100  2125    0     0  19775      0 --:--:-- --:--:-- --:--:-- 19859
       close  staging-index-8cd6e43115e9416eb23609486fa053e3
green  open   master-index-8cd6e43115e9416eb23609486fa053e3-new3    1   0       2365            7      2.4mb          2.4mb
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3-new3   1   0          0            0       159b           159b
       close  master-index-8cd6e43115e9416eb23609486fa053e3
*** es_alias_status
root@aio:/opt/mdm# curl -XGET "http://$es_ip:9200/_aliases?pretty" | grep -C 3 8cd6e43115e9416eb23609486fa053e3
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1719  100  1719    0     0   404k      0 --:--:-- --:--:-- --:--:--  559k
      "staging-98b28be03c1411e783570242ac110002" : { }
    }
  },
  "master-index-8cd6e43115e9416eb23609486fa053e3-new3" : {
    "aliases" : {
      "master-8cd6e43115e9416eb23609486fa053e3" : { }
    }
  },
  "staging-index-e4010da4110ba377d100f050cb4440db-new3" : {
--
      "master-e4010da4110ba377d100f050cb4440db" : { }
    }
  },
  "staging-index-8cd6e43115e9416eb23609486fa053e3-new3" : {
    "aliases" : {
      "staging-8cd6e43115e9416eb23609486fa053e3" : { }
    }
  },
  "staging-index-ce7aa4a0208611e7808d0242ac110003-new3" : {

*** mdm log
[06 Jul 2017;15:51:37.274]-[WARN ][AwsRoute53ServiceImpl - ensurePersistentRecords:374 - main - T_mdm] - {"errorCode":500,"errorMessage":"com.amazonaws.services.route53.model.InvalidCha
ngeBatchException: Tried to create resource record set [name='inkibana.carol.ai.', type='A'] but it already exists (Service: AmazonRoute53; Status Code: 400; Error Code: InvalidChangeBa
tch; Request ID: fe3a73ab-6262-11e7-8689-61caaa25b681)","responsibleField":""}
[06 Jul 2017;15:51:37.434]-[WARN ][TenantServiceImpl - upgradeTenants:836 - main - T_mdm] - Error upgrading tenants. ! org.elasticsearch.indices.IndexClosedException: closed
! at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:172)
! at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:113)
! at org.elasticsearch.action.search.AbstractSearchAsyncAction.<init>(AbstractSearchAsyncAction.java:99)
! at org.elasticsearch.action.search.SearchQueryThenFetchAsyncAction.<init>(SearchQueryThenFetchAsyncAction.java:53)
! at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:93)
! at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:47)
! at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:149)
! at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:137)
! at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:85)
! at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
! at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
! at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:86)
! at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:56)
! at com.totvslabs.framework.elasticsearch.dao.impl.ElasticsearchSyncDaoImpl.search(ElasticsearchSyncDaoImpl.java:588)
! at com.totvslabs.framework.elasticsearch.dao.impl.ElasticsearchSyncSearcherImpl.search(ElasticsearchSyncSearcherImpl.java:530)
! at com.totvslabs.framework.dao.impl.GenericSyncSearcherImpl.search_aroundBody0(GenericSyncSearcherImpl.java:352)
! at com.totvslabs.framework.dao.impl.GenericSyncSearcherImpl$AjcClosure1.run(GenericSyncSearcherImpl.java:1)
! at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149)
! at com.totvslabs.framework.dao.transaction.interceptor.TransactionManagementInterceptor.interceptSearcher(TransactionManagementInterceptor.java:69)
! at com.totvslabs.framework.dao.impl.GenericSyncSearcherImpl.search(GenericSyncSearcherImpl.java:344)
! at com.totvslabs.mdm.app.core.services.impl.TenantServiceImpl.getTenants(TenantServiceImpl.java:589)
! at com.totvslabs.mdm.app.core.services.impl.TenantServiceImpl.upgradeTenants(TenantServiceImpl.java:822)
! at com.totvslabs.mdm.app.server.MdmApplication.upgradeDatabase(MdmApplication.java:696)
! at com.totvslabs.mdm.app.server.MdmApplication.run(MdmApplication.java:490)
! at com.totvslabs.mdm.app.server.MdmApplication.run(MdmApplication.java:1)
! at io.dropwizard.cli.EnvironmentCommand.run(EnvironmentCommand.java:43)
! at io.dropwizard.cli.ConfiguredCommand.run(ConfiguredCommand.java:85)
! at io.dropwizard.cli.Cli.run(Cli.java:75)
! at io.dropwizard.Application.run(Application.java:79)
! at com.totvslabs.mdm.app.server.MdmApplication.main(MdmApplication.java:883)

Exception in thread "main" {"errorCode":500,"errorMessage":"closed","responsibleField":""}
        at com.totvslabs.framework.core.common.exceptions.ServiceExceptionHelper.get(ServiceExceptionHelper.java:66)
        at com.totvslabs.framework.core.common.exceptions.ServiceExceptionHelper.get(ServiceExceptionHelper.java:47)
        at com.totvslabs.framework.services.validation.impl.ValidationServiceImpl.validateOnCreate(ValidationServiceImpl.java:124)
        at com.totvslabs.framework.services.validation.impl.ValidationServiceImpl.validateOnCreate(ValidationServiceImpl.java:100)
        at com.totvslabs.mdm.app.dao.base.AbstractDao.insert(AbstractDao.java:172)
        at com.totvslabs.mdm.app.core.services.impl.EntityTemplateTypeServiceImpl.createEntityTemplateType(EntityTemplateTypeServiceImpl.java:71)
        at com.totvslabs.mdm.app.server.DefaultDataLoader.createGlobalEntityTemplateTypes(DefaultDataLoader.java:294)
        at com.totvslabs.mdm.app.server.DefaultDataLoader.invokeDefaultEntitiesLoading(DefaultDataLoader.java:220)
        at com.totvslabs.mdm.app.server.MdmApplication.upgradeDatabase(MdmApplication.java:704)
        at com.totvslabs.mdm.app.server.MdmApplication.run(MdmApplication.java:490)
        at com.totvslabs.mdm.app.server.MdmApplication.run(MdmApplication.java:1)
        at io.dropwizard.cli.EnvironmentCommand.run(EnvironmentCommand.java:43)
        at io.dropwizard.cli.ConfiguredCommand.run(ConfiguredCommand.java:85)
        at io.dropwizard.cli.Cli.run(Cli.java:75)
        at io.dropwizard.Application.run(Application.java:79)
        at com.totvslabs.mdm.app.server.MdmApplication.main(MdmApplication.java:883)
Caused by: [master-index-8cd6e43115e9416eb23609486fa053e3] IndexClosedException[closed]
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:172)
        at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:113)
        at org.elasticsearch.action.search.AbstractSearchAsyncAction.<init>(AbstractSearchAsyncAction.java:99)
        at org.elasticsearch.action.search.SearchQueryThenFetchAsyncAction.<init>(SearchQueryThenFetchAsyncAction.java:53)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:93)
        at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:47)
        at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:149)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:137)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:85)
        at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:58)
        at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:359)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:86)
        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:56)
*** remove closed indices
master-index-8cd6e43115e9416eb23609486fa053e3
master-index-e4010da4110ba377d100f050cb4440db
staging-index-8cd6e43115e9416eb23609486fa053e3
staging-index-e4010da4110ba377d100f050cb4440db

for index in $(curl $es_ip:9200/_cat/indices?v | grep "close" | awk -F" " '{print $2}'); do
     echo "curl -XDELETE http://$es_ip:9200/$index"
     curl -XDELETE "http://$es_ip:9200/$index"
done


ssh root@45.79.80.81
** 2017-07-06: (2) To save cost, migrate non-critical envs from DO to Linode
  CLOSED: [2017-07-06 Thu 12:12]
https://github.com/TOTVS/mdmdevops/wiki/MDM-Server-List

49160906         explore-aio-sandbox  165.227.15.216       160.0
18479539         mdm-dev-jenkins-ci   159.203.196.76       160.0
4992032          mdm-workstation      104.236.159.226      40.0
18340442         robson1              159.203.202.212      80.0
25677927         wilson-sandbox-qa    138.68.14.146        160.0
*** Manual steps: change hostname
*** Configure /root/.ssh/authorized_keys for VM
*** #  --8<-------------------------- separator ------------------------>8--
*** TODO explore-aio-sandbox
http://45.79.80.81:18080
*** #  --8<-------------------------- separator ------------------------>8--
** #  --8<-------------------------- separator ------------------------>8--
** 2017-07-05: Fix jenkins issue of CreateMonthlyGitTag: 1h
  CLOSED: [2017-07-05 Wed 19:51]
** 2017-07-07: [#A] #114 nginx proxy research: make proxy service more tolerant for the issues of upstream services: 6h
  CLOSED: [2017-07-07 Fri 17:53]
https://bitbucket.org/nubesecure/devops/issues/114/nginx-proxy-research-make-proxy-service

** 2017-07-11: [#B] Blog: Use Docker to start Jenkins env
  CLOSED: [2017-06-11 Sun 21:20]
*** start jenkins via docker
docker run -p 8080:8080 -p 50000:50000 denny/jenkins:1.0

http://injenkins.carol.ai:8080/
*** start jenkins via docker-compose
docker-compose up -d

cd /var/lib/docker/volumes/
chmod 777 -R *_volume_backup *_volume_jobs *_volume_workspace
**** > ./docker-compose.yml && vim docker-compose.yml
#+BEGIN_EXAMPLE
version: '2'
services:
  my_jenkins:
    container_name: my_jenkins
    hostname: my_jenkins
    # Base Docker image: https://github.com/DennyZhang/devops_docker_image/blob/tag_v6/jenkins/Dockerfile_1_0
    image: denny/jenkins:1.0
    ports:
      - "8080:8080/tcp"
    environment:
      JENKINS_TIMEZONE: "America/New_York"
      JAVA_OPTS: -Djenkins.install.runSetupWizard=false
    volumes:
      - volume_jobs:/var/jenkins_home/jobs
      - volume_workspace:/var/jenkins_home/workspace
      - volume_backup:/var/jenkins_home/backup

volumes:
  volume_jobs:
  volume_backup:
  volume_workspace:
#+END_EXAMPLE

** 2017-07-11: Add 2 workers to bematech cluster: https://cloud.digitalocean.com/droplets?i=4fab62
  CLOSED: [2017-07-11 Tue 16:22]
bematech-do-app-4	138.68.21.61		Type2	app worker
bematech-do-app-5	138.68.63.208		Type2	app worker

ufw allow from 138.68.21.61
ufw allow from 138.68.63.208

** 2017-07-12: 1h: support demo env issues; disable and reconfigure for nightly deployment jobs
  CLOSED: [2017-07-12 Wed 12:18]
** 2017-07-12: ChatOps Improvement: 1h
  CLOSED: [2017-07-12 Wed 15:24]
** 2017-07-15: Bematech CB cluster run into low resource: add one more CB node: bematech-do-cb-10
  CLOSED: [2017-07-15 Sat 17:29]
bematech-do-cb-10 138.68.232.241
** #  --8<-------------------------- separator ------------------------>8--
** 2017-07-17: Setup VPN and wiki: 1h
  CLOSED: [2017-07-17 Mon 09:43]
jenkins.shibgeek.com
6187
Soteria123
** 2017-07-17: #129 For demo deployment, only persist db volumes: 3h
  CLOSED: [2017-07-17 Mon 11:42]
https://bitbucket.org/nubesecure/devops/issues/129/for-demo-deployment-only-persist-db
** 2017-07-17: Enable people to provison Linode VM in Singapore region, and automate the deployment: 3h
  CLOSED: [2017-07-17 Mon 12:31]
** 2017-07-19: #50: Create a common basic images shared across repos: 2h
  CLOSED: [2017-07-19 Wed 13:58]
https://bitbucket.org/nubesecure/devops/issues/50/create-a-common-basic-images-shared-across
** 2017-07-20: #36: Don't use root user to run java programs in docker containers: 8h
  CLOSED: [2017-07-20 Thu 09:20]
** 2017-07-21: create 2 disk for both cluster
  CLOSED: [2017-07-21 Fri 13:39]
- prod-app-03	138.197.199.169 (/data2)
- bematech-do-app-5	138.68.63.208 (/data2)
** 2017-07-21: #130 Upgrade nginx from 1.13.1 to 1.13.3 for security issue: 1h
  CLOSED: [2017-07-21 Fri 21:47]
https://bitbucket.org/nubesecure/devops/issues/130/upgrade-nginx-from-1131-to-1133-for
** 2017-07-22: Create golden image for jdk, nodes: 2h
  CLOSED: [2017-07-22 Sat 13:31]

** 2017-07-26: (1) When mdm has run into Full GC frequently, raise slack notification.
  CLOSED: [2017-07-26 Wed 09:26]
https://trello.com/c/PFPrqjr8

Update nagios:

1. Check latest file of mdm-gc.log for every 5 minutes.
2. Tail last 100 lines, and count “Full GC” entries
3. If more than 5 matches(we can customize this threshold), raise warning. And send slack notification.
4. If more than 10 matches, raise errors. And send slack notification.

> /opt/devops/bin/log_pattern_frequency.py && vim /opt/devops/bin/log_pattern_frequency.py

python /opt/devops/bin/log_pattern_frequency.py --logfile /opt/mdm/logs/mdm-gc.log --warning_count 5 --critical_count 10  --check_pattern "Full GC"
python /opt/devops/bin/log_pattern_frequency.py --logfile /opt/mdm/logs/mdm-gc.log --warning_count 5 --critical_count 10  --check_pattern "ParNew"
** 2017-07-26: Add one more CB node to prod DO
  CLOSED: [2017-07-26 Wed 09:26]
prod-cb-05(165.227.18.18)
** 2017-07-27: monitor the disk usage for staging and stagingbackup folder
  CLOSED: [2017-07-27 Thu 11:37]
#+BEGIN_EXAMPLE
root@prod-app-01:/data# df -h
Filesystem      Size  Used Avail Use% Mounted on
udev            7.9G  4.0K  7.9G   1% /dev
tmpfs           1.6G  408K  1.6G   1% /run
/dev/vda1       158G  129G   23G  86% /
none            4.0K     0  4.0K   0% /sys/fs/cgroup
none            5.0M     0  5.0M   0% /run/lock
none            7.9G     0  7.9G   0% /run/shm
none            100M     0  100M   0% /run/user
#+END_EXAMPLE
** 2017-07-27: When jenkins pipeline job send slack notification, display the username of initiator
  CLOSED: [2017-07-27 Thu 18:40]
dennyzhang
[9:40 AM] 
Right now, when we run pipeline job(e.g. PipelineRefreshDemoEnvAll), we will get slack notification.

But the problem is we don’t know who trigger the job.
It just says “Started by upstream project”

I will improve this part, so that we know who has triggered the deployment. (edited)
** 2017-07-28: (1) Load balance staging data API to workers: http://injenkins.carol.ai:48080/view/Pipeline/job/PipelineActiveSprint/29/console
  CLOSED: [2017-07-28 Fri 00:13]
https://trello.com/c/1c6FZU85

curl -k -I https://localhost:443/api/v1/staging/entities/types/test

ssh -N -p 22 -i /root/.ssh/id_rsa -f root@172.17.0.2 -L *:80:localhost:80 -n /bin/bash
curl -I http://injenkins.carol.ai:80/http_stats

ssh -N -p 22 -i /root/.ssh/id_rsa -f root@172.17.0.2 -L *:443:localhost:443 -n /bin/bash

for((i=0; i< 100; i++)); do { curl -k -I https://localhost:443/api/v1/staging/entities/types/test ;}; done
** 2017-07-28: #138 Jenkins pipeline: differentiate who has triggered the pipeline job: 1h
  CLOSED: [2017-07-28 Fri 12:08]
** 2017-07-28: update yml for customize staging threads at node level: http://injenkins.carol.ai:48080/job/DockerDeployFeatureCookbooks/59/console
  CLOSED: [2017-07-28 Fri 16:26]

** 2017-07-29: update ES bulk insert: master: http://injenkins.carol.ai:48080/job/DockerDeployFeatureCookbooks/60/console
  CLOSED: [2017-07-28 Fri 18:49]
400

threadpool.bulk.queue_size: 400

```
curl -XGET "$es_ip:9200/_cluster/settings"
curl -XPUT "$es_ip:9200/_cluster/settings" -d '{"persistent":{"threadpool.bulk.queue_size": 400}}'
curl -XGET "$es_ip:9200/_cluster/settings"
```

curl -XPUT "$es_ip:9200/_cluster/settings" -d '{"persistent":{"threadpool.index.queue_size": 1000}}'
curl -XGET "$es_ip:9200/_cluster/settings"

#+BEGIN_EXAMPLE
Denny Zhang
[12:52 PM]
I noticed it has happened in bematech-do-lb-2 sometimes. It’s fine, if it just lasts for a very short period.

Bruno Volpato
[12:55 PM]
@denny.zhang, remember the index.queue_size increase?


[12:55]
we need to increase bulk.queue_size as well


[12:55]
 ```[20 Jul 2017;17:48:21.132]-[WARN ][ElasticsearchSyncDaoImpl - bulkCreateCommon:911 - pool-13-thread-4 - T_staging-dir] - Error occurred when doing bulk create: {
  "headers": {},
  "detailMessage": "Error happened in bulk operation: RemoteTransportException[[bematech-do-es-7][138.197.208.58:9300][indices:data/write/bulk[s]]]; nested: RemoteTransportException[[bematech-do-es-7][bematech-do-es-7/138.197.208.58:9300][indices:data/write/bulk[s][p]]]; nested: EsRejectedExecutionException[rejected execution of org.elasticsearch.transport.TransportService$4@54cc9af2 on EsThreadPoolExecutor[bulk, queue capacity = 50, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@3f147fb7[Running, pool size = 12, active threads = 12, queued tasks = 61, completed tasks = 11248775]]];",```


Denny Zhang
[12:56 PM]
Got it. Any recommendation for how large it should be?


new messages
Bruno Volpato
[12:56 PM]
it is 50 right now, we need to increase it in a considerate amount. I think 400 will work fine

in Elasticsearch 5.X the default is 200. I think they realized that 50 is very small

[12:57]
bematech-do-app-3/check_mdm_access_port is CRITICAL:
Connection refused


Denny Zhang
[12:57 PM]
Okay, let’s change it to 400.

Checking official documentation and doing local test now.
#+END_EXAMPLE
#+BEGIN_EXAMPLE
Bruno Volpato [6:26 PM]
maybe, let's keep watching this. I increased the thread count, let's wait a little bit and see how it goes

[6:26]
I'll also profile the app later today

[6:27]
it seems that there's a lot of things being done by the app servers, maybe CPU has idle because it has several moments that wait for database (edited)

[6:29]
sometimes I also see that ES isn't enqueueing the request, when it has more than 200 on queue.

[6:29]
 ```[24 May 2017;23:27:33.100]-[WARN ][GenericSyncDaoImpl - updateInternalWithOriginalCas_aroundBody6:1170 - mdm-executor-56 - T_chunk-notafiscal-26bb7d5040d611e78c67e2cf834ccc6c] - ElasticSearch failed in updateInternalWithOriginalCas for type mdmreceiptGolden and id 2c635e900b9211e793a6000d3ac08a5f ! com.totvslabs.framework.core.common.exceptions.ApplicationException: {"errorCode":500,"errorMessage":"rejected execution of org.elasticsearch.transport.TransportService$4@7282c7c9 on EsThreadPoolExecutor[index, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@25535dc3[Running, pool size = 12, active threads = 12, queued tasks = 200, completed tasks = 21619960]]","responsibleField":""}```

[6:29]
this seems to be configuration on the thread_pool params for ES

[6:29]
https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html

[6:30]
we may need to increase this queue

Denny Zhang
[6:30 PM]
Do we see any negative impact, if we change it to larger?

Say from 200 to 400 (edited)

Bruno Volpato
[6:31 PM]
it may hold more memory on ES

[6:31]
but it's better than rejecting the requests.

[6:31]
the error is caused because of the `index` pool is reached

[6:31]
 ```index
For index/delete operations. Thread pool type is fixed with a size of # of available processors, queue_size of 200. The maximum size for this pool is 1 + # of available processors.
```

[6:32]
we should change it, this is one of the reasons why we are not improving the speed even more

[6:32]
@kungwang any thoughts?

Denny Zhang
[6:32 PM]
Note, we’re using ES 2.3

So it’s better based on this link: https://www.elastic.co/guide/en/elasticsearch/reference/2.3/modules-threadpool.html

new messages
Bruno Volpato
[6:33 PM]
I'll even stop one server now (app-8) to prevent any data loss related to the queue (we haven't tested this scenario yet)
#+END_EXAMPLE

#+BEGIN_EXAMPLE
Bruno Volpato
[6:43 PM]
so let me do that for this index, and then we add in the default process after verified?


Nagios APP [6:43 PM]
bematech-do-app-7/check_mdm_mem is CRITICAL:
ERROR: no related process is found

Kung Wang
[6:43 PM]
here is the command I found, but can we get the existing size so we can compare later on?

```threadpool.search.queue_size: <new queue size> .

However this would also require a cluster restart.

Alternativly you could update via the cluster-setting api and this would not require a cluster restart:

curl -XPUT  _cluster/settings -d '{
    "persistent" : {
        "threadpool.search.queue_size" : <new_size>
    }
}'
```


Nagios APP [6:44 PM]
bematech-do-app-7/check_mdm_threadcount is CRITICAL:
ERROR: no related process is found

Kung Wang
[6:45 PM]
added this Plain Text snippet
root@bematech-do-app-1:~# curl bematech-do-es-10:9200/_cat/thread_pool?v&h=search.queueSize
[1] 9775
root@bematech-do-app-1:~# host      ip       bulk.active bulk.queue bulk.rejected index.active index.queue index.rejected search.active search.queue search.rejected
138.68.235.207 138.68.235.207      0     0       0      3      0      4220      10      1        0
138.197.217.22 138.197.217.22      1     0       0      3      0      1481       8      2       13
138.68.10.163  138.68.10.163       0     0       0      1      0      1978       9      0        0
138.197.199.33 138.197.199.33      0     0       0      8      0     11982      12      3        0
138.68.48.175  138.68.48.175       0     0       0      3      0       0       3      1       12
138.68.243.14  138.68.243.14       0     0       0      1      0     30455       7      1        0
138.197.194.166 138.197.194.166      0     0       0      4      0      2197       2      0        0
138.68.241.116 138.68.241.116      0     0       0      0      0       0       0      0        0
138.68.8.192  138.68.8.192       0     0       0      12     20     11102       5      0        0
138.197.208.58 138.197.208.58      1     0       0      8      0      4807       7      1       74
138.68.251.197 138.68.251.197      0     0       0      0      0       0       0      0        0
138.68.239.246 138.68.239.246      0     0       0      12     17      3148       8      6        0
138.68.41.110  138.68.41.110       0     0       0      0      0       0       0      0        0
138.68.239.172 138.68.239.172      0     0       0      2      0      1666       8      2       170
138.68.224.86  138.68.224.86       0     0       0      6      1      5801      18      19       408
138.197.199.94 138.197.199.94      0     0       0      0      0      551       2      0        0
138.68.61.4   138.68.61.4        0     0       0      8      0     11078      18      1       178
138.68.231.232 138.68.231.232      0     0       0      7      0      2469       8      1       267
138.68.247.36  138.68.247.36       1     0       0      6      1      2139       2      2       63
138.68.241.113 138.68.241.113      0     0       0      0      0       0       0      0        0
138.68.241.121 138.68.241.121      0     0       0      0      0       0       0      0        0
Add Comment Collapse

Bruno Volpato
[6:46 PM]
just tried and this command locally works:

```curl -XPUT "localhost:9200/_cluster/settings" -d '{"persistent":{"threadpool.index.queue_size": 400}}'```

[6:47]
let me do that in the server.

Kung Wang
[6:47 PM]
# Index pool
threadpool.index.type: fixed
threadpool.index.size: 20
threadpool.index.queue_size: 100

Bruno Volpato
[6:47 PM]
done

[6:49]
let me put the servers back and see if we still get this error
#+END_EXAMPLE
** 2017-07-31: add one more ES node to the cluster
  CLOSED: [2017-07-31 Mon 13:56]
Add one more ES node:
| bematech-do-es-13 | 138.197.221.94 |
** #  --8<-------------------------- separator ------------------------>8--
** 2017-07-31: copy files to remote servers
  CLOSED: [2017-07-31 Mon 20:24]
*** HALF move millions of files
**** staging
tmp_folder="staging2"
mkdir -p "/data/$tmp_folder"

cd /data/staging
# copy 5000 files
date; cnt=0; for x in *; do let cnt=cnt+1; mv $x "../$tmp_folder"/; if [ $cnt -ge 5000 ]; then break; fi; done; date

du -h -d 1  "/data/$tmp_folder"

cd /data
time tar -czvf "${tmp_folder}.tar.gz" "$tmp_folder"

# ls -lth "/data/$tmp_folder"
# rm -rf "/data//$tmp_folder"
*** HALF manually copy files of /data/staging across nodes
In app-03

# copy app-01
scp -i /root/.ssh/ci_id_rsa -r -P 2702 root@138.197.193.53:/data/staging_tmp/  /data/staging_tmp/prod-app-01/

# copy app-02
scp -i /root/.ssh/ci_id_rsa -r -P 2702 root@138.197.193.53:/data/staging_tmp/  /data/staging_tmp/prod-app-01/

**** stagingbackup
tmp_folder="stagingbackup56"
mkdir -p "/data/$tmp_folder"

cd /data/stagingbackup
# copy 5000 files
date; cnt=0; for x in *; do let cnt=cnt+1; mv $x "../$tmp_folder"/; if [ $cnt -ge 5000 ]; then break; fi; done; date

du -h -d 1  "/data/$tmp_folder"

cd /data
time tar -czvf "${tmp_folder}.tar.gz" "$tmp_folder"

# ls -lth "/data/$tmp_folder"
# rm -rf "/data//$tmp_folder"
*** HALF copy folder to backup folder
# prod-audit-01:
ssh -p 2702 root@138.68.248.23

mkdir -p  /opt/couchbase/mdm_stagingbackup
ln -s /opt/couchbase/mdm_stagingbackup /data/backup/mdm_stagingbackup

# app-01
time scp -r -P 2702 -i /root/.ssh/ci_id_rsa root@138.197.193.53:/data/backup/ /data/backup/mdm_stagingbackup/prod-app-01/

# app-02
time scp -r -P 2702 -i /root/.ssh/ci_id_rsa root@138.197.199.11:/data/backup/ /data/backup/mdm_stagingbackup/prod-app-02/
** #  --8<-------------------------- separator ------------------------>8--
** 2017-07-31: 与Alex深入互动, 关于DevOps。见识到山外有山，人外有人。完成了与国际接轨。

** 2017-08-03: add a new node for es
  CLOSED: [2017-08-03 Thu 17:04]
Add bematech-do-es-14(138.197.202.203)
** 2017-08-03: exclude es-12 from the allocation list
  CLOSED: [2017-08-03 Thu 17:56]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.234.181"
   }
}'


curl -XGET $es_ip:9200/_cluster/settings
** 2017-08-03: runcommand failure in prod do jenkins
  CLOSED: [2017-08-03 Thu 18:41]
export server_list="#https://totvslabs.carol.ai/querycluster/nagios
# https://totvslabs.carol.ai/querycluster/mdmui
# https://totvslabs.carol.ai/querycluster/mdmworker
# https://totvslabs.carol.ai/querycluster/mdm_ui_and_worker
 https://totvslabs.carol.ai/querycluster/all
# https://totvslabs.carol.ai/querycluster/haproxy
# https://totvslabs.carol.ai/querycluster/elasticsearch
# https://totvslabs.carol.ai/querycluster/elasticsearch_audit"

echo $(echo "$server_list" | python ./strip_comments.py)

wget -O /usr/sbin/jenkins/scripts/strip_comments.py https://github.com/DennyZhang/devops_public/raw/tag_v6/python/parameters_tool/strip_comments.py 

/usr/sbin/jenkins/scripts/strip_comments.py

server_list=$(echo "$server_list" | grep -v "^#")
server_list=$(echo "$server_list" | python /usr/sbin/jenkins/scripts/strip_comments.py)

su jenkins
cd ~/jobs
find -name config.xml | xargs grep "echo.*server_list"

** 2017-08-05: Add one more ES to bematech DO
  CLOSED: [2017-08-05 Sat 11:31]
| bematech-do-es-16 | 165.227.25.58 |
** 2017-08-05: Add one more ES to bematech DO
  CLOSED: [2017-08-05 Sat 15:23]
Add | bematech-do-es-17   |  165.227.21.253 |
** 2017-08-07: add one more CB node
  CLOSED: [2017-08-07 Mon 07:36]
| bematech-do-cb-11 | 138.68.41.24 |
** 2017-08-07: Add a new node
  CLOSED: [2017-08-07 Mon 10:13]
Add | bematech-do-es-18   |  165.227.25.149
** 2017-08-07: add a new ES node: bematech-do-es-19
  CLOSED: [2017-08-07 Mon 10:32]
| bematech-do-es-19   |  138.197.205.85
** 2017-08-07: remove es-12 ip from the /etc/hosts
  CLOSED: [2017-08-07 Mon 10:39]
** 2017-08-07: Retire ES-12
  CLOSED: [2017-08-07 Mon 10:49]

** #  --8<-------------------------- separator ------------------------>8--
** 2017-08-08: pipeline: Run Deploy Rehearsal for new node
  CLOSED: [2017-08-08 Tue 16:10]
** 2017-08-08: [#A] conardo: docs.carol.ai
  CLOSED: [2017-08-08 Tue 16:57]
http://docs.carol.ai/5-querying-data/
https://github.com/TOTVS/documentation

cd /data/git_repo/documentation

git checkout 94b786acfdb504cf4a5359d3ec596fed202e51a5

bash /opt/mdmdevops/env_setup/repo.carol.ai/doc.carol.ai/crontab_git_pull.sh
** 2017-08-08: elasticsearch error: /data/elasticsearch/repo/mdm_big, remove ES repository: mdm_small, mdm_big
  CLOSED: [2017-08-08 Tue 17:54]
curl -XGET "http://$es_ip:9200/_snapshot/"

curl -XDELETE "$es_ip:9200/_snapshot/mdm_backup/snapshot_1"

curl -XDELETE "$es_ip:9200/_snapshot/mdm_small"
curl -XDELETE "$es_ip:9200/_snapshot/mdm_big"
curl -XDELETE "$es_ip:9200/_snapshot/my_backup"

#+BEGIN_EXAMPLE
Caused by: RepositoryException[[mdm_big] location [/data/elasticsearch/repo/mdm_big] doesn't match any of the locations specified by path.repo]
        at org.elasticsearch.repositories.fs.FsRepository.<init>(FsRepository.java:81)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:50)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:104)
        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:54)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:886)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:46)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:201)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:879)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)

#+END_EXAMPLE
** 2017-08-08: Change URI: mdm_ui_and_worker
  CLOSED: [2017-08-08 Tue 18:06]
*** 2017-08-08: update wiki
   CLOSED: [2017-08-08 Tue 16:12]

mdm_worker_and_ui

mdm_ui_and_worker

mdm_worker_ui_ip_list

mdm_ui_worker_ip_list
*** HALF remove old file in server
cd /opt/devops/node_cfg
ls -lth 
rm  mdm_ui_and_worker
*** HALF update jenkins
su jenkins

cd ~/jobs
find -name config.xml | xargs grep "mdm_ui_and_worker"

find -name config.xml | xargs sed -i "s/mdm_ui_and_worker/mdm_worker_and_ui/g"
*** #  --8<-------------------------- separator ------------------------>8--
*** HALF update both env
*** HALF verify sequence https://bematech.carol.ai/querycluster/mdm_worker_and_ui
https://bematech.carol.ai/querycluster/mdm_ui_and_worker

https://totvslabs.carol.ai/querycluster/mdm_ui_and_worker
https://totvslabs.carol.ai/querycluster/mdm_worker_and_ui
** #  --8<-------------------------- separator ------------------------>8--
** 2017-08-10: Add: wiki about ES allocation learning: https://github.com/TOTVS/mdmdevops/wiki/Lessons-Learned-For-Cost-Saving-Of-Elasticsearch-Cluster
  CLOSED: [2017-08-10 Thu 16:03]
** 2017-08-10: Add a new node to bematech-do-es-15
  CLOSED: [2017-08-10 Thu 16:59]
Add | bematech-do-es-15   |  138.197.218.77
** 2017-08-10: can't select multiple, and can't delete from filesystem directly
  CLOSED: [2017-03-15 Wed 18:06]
** #  --8<-------------------------- separator ------------------------>8--
** 2017-08-10: (2) Avoid spaming #DevOps channel for check_elasticsearch_replica/shard: http://injenkins.carol.ai:48080/job/DockerDeployAllInOne/75/console
  CLOSED: [2017-08-10 Thu 16:51]
https://trello.com/c/x4iti89b
** 2017-08-10: (0.5) When people migrate their queries, remove hardcode "nested" handling for QueryHelper and AggregationHelper
  CLOSED: [2017-08-10 Thu 16:53]
https://trello.com/c/NRrTdsXr

** 2017-08-11: Decommission Prod DO env
  CLOSED: [2017-08-11 Fri 12:56]
*** 2017-08-11: shudown non-critical VMs
   CLOSED: [2017-08-10 Thu 20:21]
*** 2017-08-11: shudown CB and ES cluster
   CLOSED: [2017-08-11 Fri 12:56]
*** 2017-08-11: Remove DNS settings
   CLOSED: [2017-08-11 Fri 12:56]
*** 2017-08-11: Remove VMs and Volumes
   CLOSED: [2017-08-11 Fri 12:56]

** 2017-08-12: Add 4 ES nodes and replace 4 ES nodes
  CLOSED: [2017-08-12 Sat 19:17]
165.227.5.192:2702
165.227.17.43:2702

curl $es_ip:9200/_cat/shards?v | grep -v STARTED
*** 2017-08-12: add 2 new nodes
   CLOSED: [2017-08-11 Fri 14:43]
| bematech-do-es-01    |   138.197.215.132 | Type4          | Elasticsearch       |
| bematech-do-es-02    |   138.68.50.117 | Type4          | Elasticsearch       |
*** 2017-08-12: add 2 new nodes
   CLOSED: [2017-08-11 Fri 16:14]
| bematech-do-es-03 | 165.227.5.192 |
| bematech-do-es-04 | 165.227.17.43 |
*** 2017-08-12: exclude es-2 and es-3
   CLOSED: [2017-08-11 Fri 23:58]
curl -XGET $es_ip:9200/_cluster/settings

curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.247.36,138.68.61.4"
   }
}'
*** 2017-08-12: update dns
   CLOSED: [2017-08-11 Fri 23:58]
*** 2017-08-12: remove existing node
   CLOSED: [2017-08-12 Sat 00:02]
| bematech-do-es-2 |  138.68.247.36 | Type4 | Elasticsearch |
| bematech-do-es-3 |    138.68.61.4 | Type4 | Elasticsearch |
*** 2017-08-12: update nagios3
   CLOSED: [2017-08-12 Sat 00:03]

** 2017-08-13: app-01 disk issue
  CLOSED: [2017-08-13 Sun 09:27]
tmp_folder="stagingbackup01"
mkdir -p "/data/$tmp_folder"

cd /data/stagingbackup
# copy 5000 files
date; cnt=0; for x in *.json; do let cnt=cnt+1; mv $x "../$tmp_folder"/; if [ $cnt -ge 5000 ]; then break; fi; done; date

du -h -d 1  "/data/$tmp_folder"

cd /data
time tar -czvf "${tmp_folder}.tar.gz" "$tmp_folder"

# ls -lth "/data/$tmp_folder"
# rm -rf "/data//$tmp_folder"
** 2017-08-13: add cb to the cluster
  CLOSED: [2017-08-13 Sun 09:28]
165.227.19.92:2702
| bematech-do-cb-12 | 165.227.19.92 |
** 2017-08-13: Add 4 ES nodes and replace 4 existing ES nodes
  CLOSED: [2017-08-13 Sun 18:59]
- Old
| bematech-do-es-6    |  138.197.217.22 | Type4          | Elasticsearch       |
| bematech-do-es-7    |  138.197.208.58 | Type4          | Elasticsearch       |
| bematech-do-es-8    | 138.197.194.166 | Type4          | Elasticsearch       |
| bematech-do-es-9    |  138.197.199.94 | Type4          | Elasticsearch       |

- New
| bematech-do-es-05 |  138.68.46.170 |
| bematech-do-es-06 |  165.227.21.77 |
| bematech-do-es-07 | 165.227.21.128 |
| bematech-do-es-08 |   138.68.30.61 |

138.68.46.170:2702
165.227.21.77:2702
165.227.21.128:2702
138.68.30.61:2702
*** 2017-08-13: update jenkins jobs configuration and run pipeline jobs
   CLOSED: [2017-08-12 Sat 19:36]
*** 2017-08-13: update nagios
   CLOSED: [2017-08-12 Sat 19:37]
*** 2017-08-13: Deploy 4 new nodes
   CLOSED: [2017-08-12 Sat 20:06]
*** 2017-08-13: exclude 4 nodes
   CLOSED: [2017-08-12 Sat 20:06]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.197.217.22,138.197.208.58,138.197.194.166,138.197.199.94"
   }
}'

curl -XGET $es_ip:9200/_cluster/settings
*** 2017-08-13: check and retire old VMs
   CLOSED: [2017-08-13 Sun 18:59]

** 2017-08-14: (1) Because of IT policy in Brazil, change nexus3.carol.ai:8081 to nexus3.carol.ai:8080, and update all git repos
  CLOSED: [2017-08-14 Mon 17:18]
https://trello.com/c/CdK7GTsx

change nexus3.carol.ai:8081 to nexus3.carol.ai:8080

** 2017-08-14: [#A] cherry pick replica/shard changes, and update nagios: https://github.com/TOTVS/mdmdevops/pull/501
  CLOSED: [2017-08-14 Mon 20:51]
https://github.com/TOTVS/mdmdevops/pull/498/files
** 2017-08-16: add 2 nodes and remove 3 nodes
  CLOSED: [2017-08-16 Wed 09:03]
138.197.200.214:2702
138.197.194.166:2702

- add
| bematech-do-es-09 | 138.197.200.214 |
| bematech-do-es-12 | 138.197.194.166 |

- Remove
| bematech-do-es-10 |  138.68.10.163 |
| bematech-do-es-11 |  138.68.48.175 |
| bematech-do-es-13 | 138.197.221.94 |
*** 2017-08-16: update nagios
  CLOSED: [2017-08-15 Tue 13:27]
*** 2017-08-16: Deploy 4 new nodes
   CLOSED: [2017-08-15 Tue 13:26]
*** 2017-08-16: exclude 3 nodes
    CLOSED: [2017-08-15 Tue 13:26]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.68.10.163,138.68.48.175,138.197.221.94"
   }
}'

curl -XGET $es_ip:9200/_cluster/settings
** #  --8<-------------------------- separator ------------------------>8--
*** 2017-08-16: check and retire old VMs
   CLOSED: [2017-08-16 Wed 09:02]
** 2017-08-16: add a new worker
  CLOSED: [2017-08-16 Wed 09:03]
138.68.19.130:2702
| bematech-do-app-6 | 138.68.19.130 |
** 2017-08-16: add 2 worker nodes
  CLOSED: [2017-08-16 Wed 15:25]
138.68.240.209:2702
165.227.20.234:2702

| bematech-do-app-7 | 138.68.240.209 |
| bematech-do-app-8 | 165.227.20.234 |

** 2017-08-17: (3) When retiring nodes, cleanup old entries of /etc/hosts and ufw rules in existing nodes
  CLOSED: [2017-08-17 Thu 16:14]
https://trello.com/c/BQQdcaYO
cat > /tmp/test_hosts <<EOF
104.236.158.226 repo.dennytest.com
EOF

python ./python-hosts-tool.py --action add --add_hosts_file /tmp/test_hosts
python ./python-hosts-tool.py --action remove --remove_hosts_file /tmp/test_hosts
python ./python-hosts-tool.py --action examine --examine_hosts_file /tmp/test_hosts
** 2017-08-18: retire app7 and app8: http://bematech-do-jenkins.carol.ai:18080/view/Pipeline/job/PipeLineRemoveNodes/8/console
  CLOSED: [2017-08-18 Fri 10:28]
138.68.240.209 bematech-do-app-7
165.227.20.234 bematech-do-app-8
| bematech-do-app-7 | 138.68.240.209 |
| bematech-do-app-8 | 165.227.20.234 |
*** remove VMs
** 2017-08-18: Add 3 ES nodes and remove all existing 4 nodes
  CLOSED: [2017-08-18 Fri 15:20]
138.197.221.191:2702
138.197.221.161:2702
165.227.27.184:2702

- Add
| bematech-do-es-10 | 138.197.221.191 |
| bematech-do-es-11 | 138.197.221.161 |
| bematech-do-es-13 |  165.227.27.184 |

- Remove
| bematech-do-es-14 | 138.197.202.203 |
| bematech-do-es-15 |  138.197.218.77 |
| bematech-do-es-16 |   165.227.25.58 |
| bematech-do-es-17 |  165.227.21.253 |
*** 2017-08-18: Add new nodes, update jenkins jobs, and deploy it
   CLOSED: [2017-08-17 Thu 11:27]
*** 2017-08-18: exclude old nodes
   CLOSED: [2017-08-17 Thu 11:28]
curl -XPUT $es_ip:9200/_cluster/settings -d '{
  "transient" :{
      "cluster.routing.allocation.exclude._ip" : "138.197.202.203,138.197.218.77,165.227.25.58,165.227.21.253"
   }
}'

curl -XGET $es_ip:9200/_cluster/settings

** 2017-08-18: Add | bematech-do-es-17 | 165.227.31.102 |
  CLOSED: [2017-08-18 Fri 22:16]
** 2017-08-18: Add | bematech-do-es-15 | 138.68.230.220 |
  CLOSED: [2017-08-18 Fri 22:16]
** 2017-08-18: add 4 elasticsearch nodes
  CLOSED: [2017-08-18 Fri 22:16]
138.68.23.132  bematech-do-es-14 
138.68.230.220  bematech-do-es-15 
165.227.13.42  bematech-do-es-16 
165.227.31.102  bematech-do-es-17 

| bematech-do-es-14 |  138.68.23.132 |
| bematech-do-es-15 | 138.68.230.220 |
| bematech-do-es-16 |  165.227.13.42 |
| bematech-do-es-17 | 165.227.31.102 |
** 2017-08-18: Add | bematech-do-cb-13   |    138.68.31.123
  CLOSED: [2017-08-18 Fri 23:03]
138.68.31.123 bematech-do-cb-13
** 2017-08-19: nagios configure check interval
  CLOSED: [2017-08-19 Sat 11:57]
** 2017-08-19: Check ES check from Jenkins
  CLOSED: [2017-08-19 Sat 11:59]

** 2017-08-22: change email password
  CLOSED: [2017-08-22 Tue 11:33]
Change.This@that

Change.That123@that
Change.That123@this
Change.This@that
** CANCELED (2) Run procedure to fix Audit mapping for Bematech/Prod/Explore env, rename the index by re-indexing: http://injenkins.carol.ai:48080/job/DockerDeployFeatureCookbooks/115/console
  CLOSED: [2017-08-24 Thu 17:12]
https://trello.com/c/w7h7LbST

curl $es_ip:9400/_cat/indices?v
curl -XGET "http://$es_ip:9400/_aliases?pretty"
**** 2017-08-24: remove closed indices
   CLOSED: [2017-08-10 Thu 18:48]
       close  audit-abae8b30ac9b11e692000401f8d88101-new
       close  audit-1604b7201f9711e7af0b000d3ac08037-new
       close  audit-index-dbb9e10026ce11e7aa220e4789ade3a3
       close  audit-8cd6e43115e9416eb23609486fa053e3-new
       close  audit-e4010da4110ba377d100f050cb4440db-new
       close  audit-839920f07e6b11e6b71d0401f8d88101-new

my_index="audit-839920f07e6b11e6b71d0401f8d88101-new"
curl $es_ip:$es_port/_cat/indices?v | grep $my_index

curl -XPOST "http://$es_ip:$es_port/$my_index/_close"
curl -XDELETE "http://$es_ip:$es_port/$my_index"
curl $es_ip:$es_port/_cat/indices?v | grep $my_index

curl $es_ip:$es_port/_cat/indices?v | grep close
*** HALF Update indices
es_ip="XXX"
es_port="9400"
old_index_name="audit-index-8a18aa800e5911e785f24a8136534b63"
new_index_name="audit-index-8a18aa800e5911e785f24a8136534b63-new2"
index_alias_name="audit-8a18aa800e5911e785f24a8136534b63"

bash ./create_index_from_old_201708.sh "$old_index_name" "$new_index_name"
bash ./es_reindex.sh "$old_index_name" "$new_index_name" "$index_alias_name" "$es_port" "yes" "$es_ip"

root@bematech-do-audit-1:/var/log/elasticsearch# curl $es_ip:$es_port/_cat/indices?v | grep index
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  3936  100  3936    0     0  18034      0 --:--:-- --:--:-- --:--:-- 18055
health status index                                             pri rep docs.count docs.deleted store.size pri.store.size
green  open   audit-index-8a18aa800e5911e785f24a8136534b63        5   0       3847            0      1.7mb          1.7mb
green  open   audit-index-8e2d11502c5511e79481a2f42be00f79        5   0     185247            0    118.5mb        118.5mb
green  open   audit-index-91af8e607b9311e7aa950e4789ade3a3        5   0         96            0    318.4kb        318.4kb
green  open   audit-index-ef5d1930786711e7bd7edec7170128a9        5   0         44            0      429kb          429kb
green  open   audit-index-fc595c406e6011e7b9770e4789ade3a3        5   0      21581            0      7.6mb          7.6mb
green  open   audit-index-88eee7107e2411e786390e4789ade3a3        5   0        102            0    383.4kb        383.4kb
green  open   audit-index-d3ae95b0d12811e69edf0401f8d88501        5   0        666            0    459.1kb        459.1kb
green  open   audit-index-0acedf50628d11e7b815a2f42be00f79        5   0      16996            0       12mb           12mb
green  open   audit-index-40e0def05c1f11e7a9954a8136534b63        5   0        474            0    389.5kb        389.5kb
green  open   audit-index-f36f9840666011e7b588dec7170128a9        5   0      13692            0      5.6mb          5.6mb
green  open   audit-index-1154b0b041d211e7b7c04a8136534b63        5   0      50454            0     36.7mb         36.7mb
green  open   audit-index-685778901e1c11e79595a2f42be00f79        5   0       2017            0      1.1mb          1.1mb
green  open   audit-index-b74f5a703d5e11e68ac00401f8d88501        5   0          0            0       795b           795b
green  open   audit-index-321bb9606b2111e7b579a2f42be00f79        5   0      80588            0     56.4mb         56.4mb
green  open   audit-index-1fcd49c06e6111e7a62cdec7170128a9        5   0        650            0      936kb          936kb
green  open   audit-index-88e3be30786b11e7bd7edec7170128a9        5   0         19            0    218.7kb        218.7kb
green  open   audit-index-d5381d00202611e792b4a2f42be00f79        5   0         40            0    327.2kb        327.2kb
green  open   audit-index-03b541d0208111e7bbb0a2f42be00f79        5   0         40            0    236.9kb        236.9kb
green  open   audit-index-71dd4700628c11e7b2fa4a8136534b63        5   0      10032            0      4.5mb          4.5mb
green  open   audit-index-96a19880fa7111e6bdb0a2f42be00f79        5   0        439            0      364kb          364kb
green  open   audit-index-ef2a88c0786d11e79dae0e4789ade3a3        5   0         52            0    384.2kb        384.2kb
green  open   audit-index-dbb9e10026ce11e7aa220e4789ade3a3-new3   5   0     147178            0     38.9mb         38.9mb
green  open   audit-index-799e458055c611e6bb000401f8d88101        5   0        147            0    316.9kb        316.9kb
green  open   audit-index-cf5e90403d5e11e68ac00401f8d88501        5   0         39            0    198.8kb        198.8kb
green  open   audit-index-da1c1280ac9b11e68e250401f8d88501        5   0      55818            0     17.4mb         17.4mb
green  open   audit-index-0f65ca607d5911e7b17a0e4789ade3a3        5   0         20            0    192.8kb        192.8kb
*** TODO Jenkins job: elasticsearch/es_audit_rename_201708.sh
./es_audit_rename_201708.sh "$es_ip" "$es_port" "new2" "audit-index-098f6bcd4621d373cade4e832627b4f6"
** 2017-08-23: Retire app-5 and app-6
  CLOSED: [2017-08-23 Wed 22:42]
 138.68.63.208  bematech-do-app-5 
 138.68.19.130  bematech-do-app-6 

| bematech-do-app-5   |   138.68.63.208 | Type2          | app worker          | 2017/08/23        |
| bematech-do-app-6   |   138.68.19.130 | Type2          | app worker          | 2017/08/23        |
*** move /data/staging and /data/stagingbackup
tar -zcvf ./stagingbackup_app6_20170823.tar.gz stagingbackup
** 2017-08-24: configure monitoring
  CLOSED: [2017-08-24 Thu 17:11]
curl http://dev.carol.ai/index.html

** 2017-08-28: remove es-09
  CLOSED: [2017-08-28 Mon 21:04]
Denny Zhang
[8:20 AM]
From above DO email, I will replace es-09 with a new one.

@bruno & @kungwang

| bematech-do-es-09 | 138.197.194.166 | Type6 | Elasticsearch | 2017/08/27 |
** #  --8<-------------------------- separator ------------------------>8--
** 2017-08-30: [#A] Restart all Elasticsearch                             :IMPORTANT:
  CLOSED: [2017-08-30 Wed 09:45]
- Chef deployment
- restart
- check status

curl $es_ip:9200/_cat/shards?v | grep -v STARTED
curl $es_ip:9200/_cluster/health?pretty
cat /etc/default/elasticsearch
grep -C 3 moni /etc/elasticsearch/elasticsearch.yml
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-08-30: update jenkins
   CLOSED: [2017-08-20 Sun 12:18]
138.68.50.117:2702
165.227.5.192:2702
165.227.17.43:2702
138.68.46.170:2702
*** 2017-08-30: es-01
   CLOSED: [2017-08-20 Sun 09:30]
*** 2017-08-30: bematech-do-es-02   |   138.68.50.117 |
   CLOSED: [2017-08-20 Sun 14:35]
*** 2017-08-30: bematech-do-es-03   |   165.227.5.192 |
   CLOSED: [2017-08-20 Sun 15:10]
*** 2017-08-30: bematech-do-es-04      165.227.17.43
   CLOSED: [2017-08-20 Sun 17:38]
*** 2017-08-30: bematech-do-es-05      138.68.46.170
   CLOSED: [2017-08-20 Sun 20:35]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-08-30: update jenkins
   CLOSED: [2017-08-20 Sun 17:42]
165.227.21.77:2702
165.227.21.128:2702
138.68.30.61:2702
138.197.194.166:2702
138.197.221.191:2702
*** 2017-08-30: bematech-do-es-06      165.227.21.77
   CLOSED: [2017-08-21 Mon 00:35]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-08-30: [#A] bematech-do-es-07      165.227.21.128: reconfigured -XX:NewRatio=3
   CLOSED: [2017-08-21 Mon 14:15]

-XX:NewRatio=3
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-08-30: bematech-do-es-08      138.68.30.61
   CLOSED: [2017-08-21 Mon 23:23]
*** 2017-08-30: bematech-do-es-09      138.197.194.166
   CLOSED: [2017-08-22 Tue 06:54]
*** 2017-08-30: bematech-do-es-10      138.197.221.191
   CLOSED: [2017-08-23 Wed 08:08]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-08-30: update jenkins
   CLOSED: [2017-08-23 Wed 08:08]
138.197.221.161:2702
138.197.200.214:2702
165.227.27.184:2702
138.68.23.132:2702
138.68.230.220:2702
*** 2017-08-30: bematech-do-es-11      138.197.221.161
   CLOSED: [2017-08-23 Wed 13:26]
*** 2017-08-30: bematech-do-es-12      138.197.200.214
   CLOSED: [2017-08-23 Wed 19:05]
*** 2017-08-30: bematech-do-es-13      165.227.27.184
   CLOSED: [2017-08-24 Thu 14:32]
*** 2017-08-30: bematech-do-es-14      138.68.23.132
   CLOSED: [2017-08-25 Fri 22:47]
*** 2017-08-30: bematech-do-es-15      138.68.230.220
   CLOSED: [2017-08-25 Fri 23:25]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-08-30: update jenkins
   CLOSED: [2017-08-24 Thu 15:52]
165.227.13.42:2702
165.227.31.102:2702
165.227.25.149:2702
138.197.205.85:2702
*** 2017-08-30: bematech-do-es-16      165.227.13.42
   CLOSED: [2017-08-25 Fri 23:58]
*** 2017-08-30: bematech-do-es-17      165.227.31.102
   CLOSED: [2017-08-26 Sat 00:21]
*** 2017-08-30: bematech-do-es-18     165.227.25.149
   CLOSED: [2017-08-26 Sat 13:21]
*** 2017-08-30: bematech-do-es-19     138.197.205.85
   CLOSED: [2017-08-27 Sun 10:53]
*** #  --8<-------------------------- separator ------------------------>8--
*** TODO bematech-do-es-01  138.197.215.132
*** TODO bematech-do-es-02    138.68.50.117
*** TODO bematech-do-es-03    165.227.5.192
*** TODO bematech-do-es-04    165.227.17.43
*** TODO bematech-do-es-05    138.68.46.170
*** TODO bematech-do-es-06    165.227.21.77
** 2017-08-29: add 4 ES nodes for rebalancing: http://bematech-do-jenkins.carol.ai:18080/job/DeploySystemDOBematech/91/console
  CLOSED: [2017-08-29 Tue 17:52]
165.227.24.213:2702
138.68.246.50:2702
165.227.22.228:2702
138.68.21.77:2702


 165.227.24.213  bematech-do-es-1 
  138.68.246.50  bematech-do-es-2 
 165.227.22.228  bematech-do-es-3 
   138.68.21.77  bematech-do-es-4 

| bematech-do-es-1 | 165.227.24.213 | Type7 | Elasticsearch |
| bematech-do-es-2 |  138.68.246.50 | Type7 | Elasticsearch |
| bematech-do-es-3 | 165.227.22.228 | Type7 | Elasticsearch |
| bematech-do-es-4 |   138.68.21.77 | Type7 | Elasticsearch |
** 2017-08-29: update mdm.yml for 1.73
  CLOSED: [2017-08-29 Tue 18:42]
Bruno Volpato 
[5:50 PM] 
@denny.zhang beware of mdm.yml changes to add a "config" bucket and index to each tenant


[5:51] 
migration script will take care of moving the documents, but we need to include those configs in our deployment mdm.yml for 1.73


[5:51] 
https://github.com/TOTVS/mdm/compare/70c0d0f063ce...ee419994d5c4


Denny Zhang
[5:51 PM] 
Thanks, will update soon.
** [#A] 2017-08-29: Re-index explore env: http://bematech-do-jenkins.carol.ai:18080/job/ESReIndex/44/consoleFull
  CLOSED: [2017-08-29 Tue 20:49]
http://45.79.80.81:9200

*** download facilities
*** deploy to 1.71
*** es indices before
green  open   staging-index-b5c8d1f04a2211e78a470242ac110003-new3   1   0          7            0        1mb            1mb
green  open   master-index-5b010c60f21111e6a49e0242ac110003-new3    1   0    3150494       660024      2.5gb          2.5gb
green  open   staging-index-e4010da4110ba377d100f050cb4440db-new3   1   0       3676            7      5.7mb          5.7mb
green  open   master-index-e4010da4110ba377d100f050cb4440db-new3    1   0      35682           26     21.1mb         21.1mb
green  open   master-index-974baa8067a611e789690242ac110003         1   0        177            4    320.9kb        320.9kb
green  open   staging-index-98b28be03c1411e783570242ac110002-new3   1   0    3166293      1387020      4.3gb          4.3gb
green  open   master-index-8cd6e43115e9416eb23609486fa053e3-new3    1   0       2370            4      4.7mb          4.7mb
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3-new3   1   0          0            0       159b           159b
green  open   staging-index-ce7aa4a0208611e7808d0242ac110003-new3   1   0        824           14    760.9kb        760.9kb
green  open   master-index-ce7aa4a0208611e7808d0242ac110003-new3    1   0       8646            5      5.3mb          5.3mb
green  open   staging-index-974baa8067a611e789690242ac110003        1   0         14            0     77.6kb         77.6kb
green  open   staging-index-5b010c60f21111e6a49e0242ac110003-new3   1   0    1492007      1152699      5.9gb          5.9gb
green  open   master-index-b5c8d1f04a2211e78a470242ac110003-new3    1   0       4596            2     11.7mb         11.7mb
green  open   master-index-98b28be03c1411e783570242ac110002-new3    1   0        662            5    585.8kb        585.8kb
*** 2017-08-29: run re-index: avoid_create_new_index(no), avoid_skip_reindex(yes), avoid_update_alias(yes): http://bematech-do-jenkins.carol.ai:18080/job/ESReIndex/41/console
   CLOSED: [2017-08-29 Tue 17:08]
*** 2017-08-29: verify json
   CLOSED: [2017-08-29 Tue 17:09]
*** 2017-08-29: run re-index: avoid_create_new_index(yes), avoid_skip_reindex(no), avoid_update_alias(yes): http://bematech-do-jenkins.carol.ai:18080/job/ESReIndex/43/console
   CLOSED: [2017-08-29 Tue 20:06]
1hrs50mins
*** 2017-08-29: run re-index: avoid_create_new_index(yes), avoid_skip_reindex(no), avoid_update_alias(no): http://bematech-do-jenkins.carol.ai:18080/job/ESReIndex/46/console
   CLOSED: [2017-08-29 Tue 20:37]
*** es indices after
green  open   master-index-98b28be03c1411e783570242ac110002-new2    5   0        662            6    757.6kb        757.6kb
green  open   staging-index-b5c8d1f04a2211e78a470242ac110003-new2   5   0          7            0      1.1mb          1.1mb
green  open   master-index-e4010da4110ba377d100f050cb4440db-new2    5   0      35682            0     14.9mb         14.9mb
green  open   master-index-8cd6e43115e9416eb23609486fa053e3-new2    5   0       2370            2      2.9mb          2.9mb
green  open   staging-index-974baa8067a611e789690242ac110003-new2   5   0         14            0    118.2kb        118.2kb
green  open   staging-index-98b28be03c1411e783570242ac110002-new2   5   0    3166293           15      2.6gb          2.6gb
green  open   staging-index-ce7aa4a0208611e7808d0242ac110003-new2   5   0        824            0        1mb            1mb
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3-new2   5   0          0            0       795b           795b
green  open   staging-index-5b010c60f21111e6a49e0242ac110003-new2   5   0    1492007            0      5.9gb          5.9gb
green  open   master-index-ce7aa4a0208611e7808d0242ac110003-new2    5   0       8646            0      6.4mb          6.4mb
green  open   master-index-974baa8067a611e789690242ac110003-new2    5   0        177            0    470.9kb        470.9kb
green  open   master-index-5b010c60f21111e6a49e0242ac110003-new2    5   0    3150494            0        2gb            2gb
green  open   staging-index-e4010da4110ba377d100f050cb4440db-new2   5   0       3676            0      6.4mb          6.4mb
green  open   master-index-b5c8d1f04a2211e78a470242ac110003-new2    5   0       4596            0     12.4mb         12.4mb
*** 2017-08-29: restart mdm
   CLOSED: [2017-08-29 Tue 20:48]
** #  --8<-------------------------- separator ------------------------>8--
** 2017-08-30: Adding cb-14 to replace cb-13
  CLOSED: [2017-08-30 Wed 18:13]
Accorind to #devops-digitialocean, DO will do another maintainace for cb-13 Sep 6th.

I'm adding cb-14 and failover cb-13. Thus we can be better prepared.
*** 2017-08-30: add cb-14
   CLOSED: [2017-08-30 Wed 13:03]
 138.68.58.241  bematech-do-cb-14
*** 2017-08-30: Finish CB rebalancing
   CLOSED: [2017-08-30 Wed 15:38]
*** #  --8<-------------------------- separator ------------------------>8--
*** 2017-08-30: failover cb-13
   CLOSED: [2017-08-30 Wed 15:44]
*** 2017-08-30: shutdown cb-13
   CLOSED: [2017-08-30 Wed 15:44]
*** #  --8<-------------------------- separator ------------------------>8--
*** TODO update jenkins jobs configuration
*** TODO run removenode jenkins pipeline
*** TODO remove cb-13

** 2017-08-30: (2) Run GUI login test from Jenkins: if it's too slow, raise alerts
  CLOSED: [2017-08-30 Wed 21:19]
https://trello.com/c/jNbw6YaW
https://totvshealth.carol.ai

python ./gui_login.py
*** 2017-08-30: create jenkins jobs: http://bematech-do-jenkins.carol.ai:18080/job/GUILoginTest/
   CLOSED: [2017-08-30 Wed 18:35]
*** 2017-08-30: support thresholds
   CLOSED: [2017-08-30 Wed 17:31]
*** 2017-08-30: save screenshot
   CLOSED: [2017-08-30 Wed 17:36]
*** 2017-08-30: gui_login.py: get error code
   CLOSED: [2017-08-30 Wed 18:18]
*** 2017-08-30: support a list of testcases
   CLOSED: [2017-08-30 Wed 18:34]
*** #  --8<-------------------------- separator ------------------------>8--
*** TODO show the screenshot link for the images: share the screenshot folder
http://injenkins.carol.ai:48080/job/GUILoginTest/ws/a.txt
*** HALF get all credentials which can pass
*** why jenkins job show the output very slowly?

** 2017-08-31: add 5 es nodes for Bematech re-indexing
  CLOSED: [2017-08-31 Thu 16:24]
138.68.233.86  bematech-do-es-5 
165.227.19.254  bematech-do-es-6 
138.68.51.171  bematech-do-es-7 
138.197.208.182  bematech-do-es-8 
165.227.30.31  bematech-do-es-9 


138.68.233.86:2702
165.227.19.254:2702
138.68.51.171:2702
138.197.208.182:2702
165.227.30.31:2702
** 2017-08-31: Run re-indexing for bematecht(tenant
  CLOSED: [2017-08-31 Thu 16:25]
https://github.com/TOTVS/mdm/commit/f705c291e87090bc41b8d9164d5b19b278bfee03
#+BEGIN_EXAMPLE
 + "            \"mapping\": {\n"
                  + "              \"fields\": {\n"
                  + "                \"raw\": {\n"
 -                + "                  \"ignore_above\": 256,\n"
 +                + "                  \"ignore_above\": 128,\n"
                  + "                  \"index\": \"not_analyzed\",\n"
                  + "                  \"type\": \"string\"\n"
                  + "                }\n"
#+END_EXAMPLE
open   master-index-03b541d0208111e7bbb0a2f42be00f79        10   2     391939           30        1gb        369.9mb 
open   staging-index-03b541d0208111e7bbb0a2f42be00f79       10   2      31226          400    594.7mb        198.2mb 

master-index-03b541d0208111e7bbb0a2f42be00f79
staging-index-03b541d0208111e7bbb0a2f42be00f79

#+BEGIN_EXAMPLE
curl $es_ip:9200/_cat/indices?v | grep 03b541d0208111e7bbb0a2f42be00f79
curl -XGET "http://${es_ip}:9200/_aliases?pretty" | grep 03b541d0208111e7bbb0a2f42be00f79
#+END_EXAMPLE

es_port=9200
index_name="master-index-03b541d0208111e7bbb0a2f42be00f79-new2"
tmp_dir="/tmp/"
curl "http://${es_ip}:${es_port}/${index_name}/_mapping" | jq '.[]' > "${tmp_dir}/mapping.json"
** Create index with parameters: index list, shard_count(cut to small pieces)
** HALF run job: avoid_create_new_index(no), avoid_skip_reindex(yes), avoid_update_alias(yes)
http://bematech-do-jenkins.carol.ai:18080/job/ESReIndex/47/console
** run job: avoid_create_new_index(yes), avoid_skip_reindex(no), avoid_update_alias(yes)
** run job: avoid_create_new_index(yes), avoid_skip_reindex(no), avoid_update_alias(no)
** 2017-08-31: remove 500GB volume for app-3
  CLOSED: [2017-08-31 Thu 19:14]
** 2017-09-01: remove dns of bematech.carol.ai
  CLOSED: [2017-09-01 Fri 09:30]

** 2017-09-01: verify DNS setting, and change the others: app.carol.ai.
  CLOSED: [2017-09-01 Fri 17:22]
** 2017-09-04: remove bematech-do-es-7
  CLOSED: [2017-09-04 Mon 17:59]
| bematech-do-es-7    |   138.68.51.171 | Type7          | Elasticsearch       | 2017/09/01        |
** 2017-09-06: explore enable nagios monitoring
  CLOSED: [2017-09-06 Wed 19:50]

** 2017-09-13: (2) RunCommandOnServers: Jenkins jobs run remote command faster
  CLOSED: [2017-09-13 Wed 09:19]
http://injenkins.carol.ai:48080/job/RunCommandOnServers/

https://trello.com/c/fe5v5PBs

https://github.com/DennyZhang/remote-commands-servers

http://injenkins.carol.ai:48080/job/RunCommandOnServers/build?delay=0sec

> /tmp/remote-commands-servers.py && vim /tmp/remote-commands-servers.py

docker cp /tmp/remote-commands-servers.py ci_jenkins:/usr/sbin/jenkins/scripts/remote-commands-servers.py

docker cp /tmp/remote-commands-servers.py prod_jenkins:/usr/sbin/jenkins/scripts/remote-commands-servers.py

>  /usr/sbin/jenkins/scripts/remote-commands-servers.py && vim /usr/sbin/jenkins/scripts/remote-commands-servers.py

# Paralell
python ./remote-commands-servers.py \
    --server_list "45.33.87.74:2702, explore.carol.ai:22, doc.carol.ai:2702" \
    --command_list "date; hostname" --ssh_username "root" \
    --enable_parallel \
    --ssh_key_file "/Users/mac/.ssh/id_rsa" --key_passphrase "sophia1"

# Sequential
python ./remote-commands-servers.py \
    --server_list "45.33.87.74:2702, explore.carol.ai:22, doc.carol.ai:2702" \
    --command_list "date; hostname" --ssh_username "root" \
    --ssh_key_file "/Users/mac/.ssh/id_rsa" --key_passphrase "sophia1"

# Sequential, abort if any errors
python ./remote-commands-servers.py \
    --server_list "45.33.87.74:2702, explore.carol.ai:22, doc.carol.ai:2702" \
    --command_list "date; false && hostname" --ssh_username "root" \
    --ssh_key_file "/Users/mac/.ssh/id_rsa" --key_passphrase "sophia1"

# Sequential, avoid fast fail
python ./remote-commands-servers.py \
    --server_list "45.33.87.74:2702, explore.carol.ai:22, doc.carol.ai:2702" \
    --command_list "date; false && hostname" --ssh_username "root" \
    --avoid_abort \
    --ssh_key_file "/Users/mac/.ssh/id_rsa" --key_passphrase "sophia1"
** 2017-09-20: master-qa-security: all-in-one: http://injenkins.carol.ai:48080/job/DigitalOceanDeployCookbooks/16/console
  CLOSED: [2017-09-20 Wed 18:13]
** 2017-09-20: deploy Wilson QA env for 1.73; verify GUI login: http://173.230.145.127:18080/job/UpdateSandboxMDM/3/console
  CLOSED: [2017-09-20 Wed 18:34]
1.74 doesn't work

** #  --8<-------------------------- separator ------------------------>8-- :noexport:
** 2017-09-25: add a new CB node to bematech env
  CLOSED: [2017-09-25 Mon 10:21]
Add 165.227.49.1 bematech-do-cb-13

** 2017-09-26: (1) Migrate 2C Continuous Integration to Carol Jenkins: verify 2c jenkins: https://github.com/TOTVS/mdmdevops/wiki/Build-MDMConnector-C2C-Project
  CLOSED: [2017-09-26 Tue 10:20]
https://trello.com/c/ymfYHLbB

http://mdmdev.carol.ai:8080/job/carol-cloud-connector/

3C--v2
https://github.com/TOTVS/mdmconnectors/tree/3C--v2

mdm_build_code.sh
** 2017-09-26: (1) Enable log rotate for gc.log in ES nodes, and update Prod ES nodes
  CLOSED: [2017-09-26 Tue 11:41]
https://trello.com/c/N4ZkblRS
*** old
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# THIS FILE IS MANAGED BY CHEF, DO NOT EDIT MANUALLY, YOUR CHANGES WILL BE OVERWRITTEN!
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# See the source file for context and more information:
# <https://github.com/elastic/cookbook-elasticsearch/blob/master/templates/default/logging.yml.erb>
#
# You may also supply your own template to the elasticsearch cookbook's
# elasticsearch_configure resource using the template_logging_yml and cookbook_logging_yml
# parameters defined here:
# <https://github.com/elastic/cookbook-elasticsearch/blob/master/libraries/resource_configure.rb#L48-L49>
#

# you can override this using by setting a system property, for example -Des.logger.level=DEBUG
es.logger.level: INFO
rootLogger: ${es.logger.level}, console, file
logger:
  # log action execution errors for easier debugging
  action: DEBUG

  # deprecation logging, turn to DEBUG to see them
  deprecation: INFO, deprecation_log_file

  # reduce the logging for aws, too much is logged under the default INFO
  com.amazonaws: WARN
  # aws will try to do some sketchy JMX stuff, but its not needed.
  com.amazonaws.jmx.SdkMBeanRegistrySupport: ERROR
  com.amazonaws.metrics.AwsSdkMetrics: ERROR

  org.apache.http: INFO

  # gateway
  #gateway: DEBUG
  #index.gateway: DEBUG

  # peer shard recovery
  #indices.recovery: DEBUG

  # discovery
  #discovery: TRACE

  index.search.slowlog: TRACE, index_search_slow_log_file
  index.indexing.slowlog: TRACE, index_indexing_slow_log_file

# ----- Configuration set by Chef ---------------------------------------------
# -----------------------------------------------------------------------------

additivity:
  index.search.slowlog: false
  index.indexing.slowlog: false
  deprecation: false

appender:
  console:
    type: console
    layout:
      type: consolePattern
      conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"

  file:
    type: dailyRollingFile
    file: ${path.logs}/${cluster.name}.log
    datePattern: "'.'yyyy-MM-dd"
    layout:
      type: pattern
      conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %.10000m%n"

  # Use the following log4j-extras RollingFileAppender to enable gzip compression of log files.
  # For more information see https://logging.apache.org/log4j/extras/apidocs/org/apache/log4j/rolling/RollingFileAppender.html
  #file:
    #type: extrasRollingFile
    #file: ${path.logs}/${cluster.name}.log
    #rollingPolicy: timeBased
    #rollingPolicy.FileNamePattern: ${path.logs}/${cluster.name}.log.%d{yyyy-MM-dd}.gz
    #layout:
      #type: pattern
      #conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"

  deprecation_log_file:
    type: dailyRollingFile
    file: ${path.logs}/${cluster.name}_deprecation.log
    datePattern: "'.'yyyy-MM-dd"
    layout:
      type: pattern
      conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"

  index_search_slow_log_file:
    type: dailyRollingFile
    file: tmp
    datePattern: "'.'yyyy-MM-dd"
    layout:
      type: pattern
      conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"

  index_indexing_slow_log_file:
    type: dailyRollingFile
    file: current.org
    datePattern: "'.'yyyy-MM-dd"
    layout:
      type: pattern
      conversionPattern: "[%d{ISO8601}][%-5p][%-25c] %m%n"
** 2017-09-26: (2) Routine cleanup: remove old and useless files safely. /data/stagingbackup, old deployment, etc.
  CLOSED: [2017-09-26 Tue 19:03]
https://trello.com/c/WjSw4roX

python /opt/devops/bin/cleanup_old_files.py \
       --working_dir "/data/stagingbackup" \
       --cleanup_type "directory" \
       --filename_pattern "[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]" \
       --examine_only --min_copies 7

*** move folder
compress_folder="2017-09-14"

backup_folder="/data/backup"
cd /data/stagingbackup

time tar -czvf "$backup_folder/${compress_folder}.tar.gz" "$compress_folder"
du -h -d 1 "$compress_folder"
ls -lth "$backup_folder/${compress_folder}.tar.gz" 

# remove old folder
rm -rf "$compress_folder"
*** move json
for example, move any 5000 files at a time to another folder:

tmp_folder="stagingbackup01"
mkdir -p "/data/$tmp_folder"

cd /data/stagingbackup
# copy 5000 files
date; cnt=0; for x in *.json; do let cnt=cnt+1; mv $x "../$tmp_folder"/; if [ $cnt -ge 5000 ]; then break; fi; done; date

du -h -d 1  "/data/$tmp_folder"

cd /data
time tar -czvf "${tmp_folder}.tar.gz" "$tmp_folder"

# ls -lth "/data/$tmp_folder"
# rm -rf "/data//$tmp_folder"
** 2017-09-28: add 2 mdm workers
  CLOSED: [2017-09-28 Thu 10:00]
138.197.212.92:2702
138.68.231.215:2702

138.197.212.92  bematech-do-app-5 
138.68.231.215  bematech-do-app-6 

** 2017-10-03: (0.5) Enable people to download artifact from both repo.carol.ai:18000 and repo.carol.ai:80
  CLOSED: [2017-10-03 Tue 13:26]
https://trello.com/c/L99ZATVK
#+BEGIN_EXAMPLE
<VirtualHost *:80>
ServerName repo.carol.ai
ServerAdmin webmaster@smallco.example.com
ErrorLog /var/log/apache2/repo_error_log
TransferLog /var/log/apache2/repo_access_log
DocumentRoot /var/www
ProxyPass / http://127.0.0.1:18000/
ProxyPassReverse / http://127.0.0.1:18000/
</VirtualHost>
#+END_EXAMPLE
** 2017-10-04: remove bematech-do-app-7
  CLOSED: [2017-10-04 Wed 15:22]
** 2017-10-04: (1) Carol Assistant: host the widget and integration with GIT
  CLOSED: [2017-10-04 Wed 15:40]
https://trello.com/c/3t3zxiJz
http://repo.carol.ai:18080/job/BuildCarolAssistant/

curl -I http://repo.carol.ai:18000/master/v0.6.1/carol-widget.js

https://github.com/TOTVS/mdmdevops/wiki/Carol-Assistant-Release-Pipeline
** 2017-10-04: Add one more CB node for low disk issue
  CLOSED: [2017-10-04 Wed 18:58]
165.227.16.231:2702
138.197.192.172:2702

165.227.16.231 bematech-do-cb-16
** 2017-10-09: wilson QA: http://injenkins.carol.ai:48080/job/DeployDigitalOceanMDMQACluster/75/console
  CLOSED: [2017-10-09 Mon 16:38]
** 2017-10-10: reset totvs password
  CLOSED: [2017-10-10 Tue 15:55]
Change.This@that

Change.that123@that
Change.abc@that123
Change.ABC@this
** 2017-10-10: (1) When running CB backup, lots of high CPU alerts: change threshold higher
  CLOSED: [2017-10-10 Tue 20:24]
https://trello.com/c/QM2Qxpuv
command[check_cpu_load]=<%= @nagios_plugins %>/check_linux_stats.pl -C -w 200 -c 600 -s 5
** 2017-10-13: Create dockerfile to host my wordpresss image
  CLOSED: [2017-10-12 Thu 18:59]
MY_DB_PASSWORD denny123
MY_DB_HOST denny-blog-mysql1.cd0lkgsdilvn.us-east-1.rds.amazonaws.com:3306

vpc-5a2bd222

** 2017-10-13: Create dockerfile to host my wordpresss image
  CLOSED: [2017-10-12 Thu 18:59]
MY_DB_PASSWORD denny123
MY_DB_HOST denny-blog-mysql1.cd0lkgsdilvn.us-east-1.rds.amazonaws.com:3306

vpc-5a2bd222
** 2017-10-13: reindex one given index: master-index-64c1cfe0936611e7a3290e4789ade3a3
  CLOSED: [2017-10-13 Fri 11:18]
root@bematech-do-es-01:~# curl $es_ip:9200/_cat/indices?v | grep 64c1cfe0936611e7a3290e4789ade3a3
curl $es_ip:9200/_cat/indices?v | grep 64c1cfe0936611e7a3290e4789ade3a3
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0
green  open   config-index-64c1cfe0936611e7a3290e4789ade3a3         1   2       1019            4      2.5mb          869kb 
green  open   master-index-64c1cfe0936611e7a3290e4789ade3a3        10   2   18833542      1235041     69.6gb         23.1gb 
green  open   staging-index-64c1cfe0936611e7a3290e4789ade3a3       10   2     808751       253337        3gb            1gb 
** 2nd: master-index-64c1cfe0936611e7a3290e4789ade3a3: http://bematech-do-jenkins.carol.ai:18080/job/ESReIndex/119/console
** 3rd
** 2017-10-18: create Wilson 2 QA sandbox
  CLOSED: [2017-10-18 Wed 17:38]
#+BEGIN_EXAMPLE
so @denny.zhang, when you have time please setup those two new servers with a sandbox for me, they can be identical to the 173.230.145.127


Denny Zhang [3:15 PM] 
Sure. I will give you 2 with master branch deployed by the end of today.
#+END_EXAMPLE
** 2017-10-19: Add one more CB node for low disk warning: bematech-do-cb-18: http://do-cb-001.carol.ai:8091/ui/index.html#/servers/active
  CLOSED: [2017-10-19 Thu 12:20]
| bematech-do-cb-18   |  138.68.52.4
** 2017-10-19: fix bug for updating prod-doc.carol.ai
  CLOSED: [2017-10-19 Thu 18:17]
** 2017-10-21: re-index education tenant: 1st: http://bematech-do-jenkins.carol.ai:18080/job/ESReIndex/122/console
  CLOSED: [2017-10-20 Fri 23:30]
curl $es_ip:9200/_cat/indices?v | grep 8e2d11502c5511e79481a2f42be00f79

es_port=9200
old_index_name="master-index-8e2d11502c5511e79481a2f42be00f79-new2"
new_index_name="master-index-8e2d11502c5511e79481a2f42be00f79-new3"
create_json_file="combined.json"
create_timeout="30m"

curl "http://${es_ip}:${es_port}/${old_index_name}/_settings" > settings.json
cat mapping.json | jq --sort-keys '.' > mapping_sorted.json
cat "mapping_sorted.json" "settings.json" | jq --slurp '.[0] * .[1]' > combined.json

time curl -XPOST "http://${es_ip}:${es_port}/${new_index_name}?timeout=${create_timeout}&wait_for_active_shards=all" -d @"${create_json_file}" 

#+BEGIN_EXAMPLE
master-index-8e2d11502c5511e79481a2f42be00f79-new2

new3

Bruno Volpato [5:43 PM] 
master-index-8e2d11502c5511e79481a2f42be00f79-new2


[5:43] 
let me fetch and fix the mappings


Denny Zhang [5:43 PM] 
Yes, thanks.


[5:43] 
Can I run it tomorrow morning?

The ES cluster is running multiple rebalancing


[5:44] 
I want to retire the VM which DO want to touch first.


Bruno Volpato [5:45 PM] 
uploaded this file
master_8e2d11502c5511e79481a2f42be00f79_mapping.json
4MB
JavaScript/JSON
 Click to download
Add Comment



new messages
Bruno Volpato 
[5:45 PM] 
if we are able to do it today, better.. but if you want to play safe, that's fine too
#+END_EXAMPLE
** 2017-10-21: 2st re-index for education tenant: http://bematech-do-jenkins.carol.ai:18080/job/ESReIndex/123/console
  CLOSED: [2017-10-21 Sat 07:12]
** 2017-10-21: remove 4 vms: lb-2, 3 es nodes
  CLOSED: [2017-10-21 Sat 08:43]
** 2017-10-26: update prod ES nagios log check
  CLOSED: [2017-10-26 Thu 11:26]
** 2017-10-26: Add bematech-do-cb-16 for low disk warning
  CLOSED: [2017-10-26 Thu 13:25]
165.227.57.99 bematech-do-cb-16
** 2017-10-30: Delete closed ES indices
  CLOSED: [2017-10-30 Mon 08:37]
curl $es_ip:9200/_cat/indices?v | grep close

master-index-321bb9606b2111e7b579a2f42be00f79
staging-index-321bb9606b2111e7b579a2f42be00f79"

es_index_list="master-index-321bb9606b2111e7b579a2f42be00f79"
for es_index_name in $es_index_list; do
  curl $es_ip:9200/_cat/indices?v | grep $es_index_name
#   curl -XDELETE "http://$es_ip:9200/${es_index_name}/"
done

curl -XDELETE "http://$es_ip:9200/${es_index_name}/"
curl $es_ip:9200/_cat/indices?v | grep $es_index_name
*** index list
staging-799e458055c611e6bb000401f8d88101
master-96a19880fa7111e6bdb0a2f42be00f79
staging-index-8e2d11502c5511e79481a2f42be00f79
master-index-839920f07e6b11e6b71d0401f8d88101-new3
master-index-8a18aa800e5911e785f24a8136534b63
staging-index-8cd6e43115e9416eb23609486fa053e3-new3
master-index-abae8b30ac9b11e692000401f8d88101-new3
staging-index-8a18aa800e5911e785f24a8136534b63
master-index-f36f9840666011e7b588dec7170128a9
staging-index-f36f9840666011e7b588dec7170128a9
master-index-8e2d11502c5511e79481a2f42be00f79
master-index-1154b0b041d211e7b7c04a8136534b63
staging-index-1154b0b041d211e7b7c04a8136534b63
master-index-8e2d11502c5511e79481a2f42be00f79-new2
staging-cf5e90403d5e11e68ac00401f8d88501
master-index-b74f5a703d5e11e68ac00401f8d88501
master-index-88eee7107e2411e786390e4789ade3a3
staging-index-b74f5a703d5e11e68ac00401f8d88501
staging-index-abae8b30ac9b11e692000401f8d88101-new3
master-799e458055c611e6bb000401f8d88101
staging-index-da1c1280ac9b11e68e250401f8d88501
master-index-da1c1280ac9b11e68e250401f8d88501
master-index-64c1cfe0936611e7a3290e4789ade3a3
master-index-dbb9e10026ce11e7aa220e4789ade3a3-new3
mdm-da1c1280ac9b11e68e250401f8d88501
staging-96a19880fa7111e6bdb0a2f42be00f79
staging-index-dbb9e10026ce11e7aa220e4789ade3a3-new3
master-index-ef2a88c0786d11e79dae0e4789ade3a3
staging-index-ef2a88c0786d11e79dae0e4789ade3a3
staging-index-685778901e1c11e79595a2f42be00f79
master-index-91af8e607b9311e7aa950e4789ade3a3
master-index-1d8cdd30780511e7bd7edec7170128a9
master-index-d5381d00202611e792b4a2f42be00f79
master-index-8cd6e43115e9416eb23609486fa053e3-new3
staging-index-d5381d00202611e792b4a2f42be00f79
staging-index-40e0def05c1f11e7a9954a8136534b63
master-index-40e0def05c1f11e7a9954a8136534b63
staging-index-d104b4e0843111e7b589dec7170128a9
staging-index-ef5d1930786711e7bd7edec7170128a9
staging-index-0acedf50628d11e7b815a2f42be00f79
master-index-d104b4e0843111e7b589dec7170128a9
master-index-685778901e1c11e79595a2f42be00f79
master-index-d104b4e0843111e7b589dec7170128a9
master-index-685778901e1c11e79595a2f42be00f79
staging-index-839920f07e6b11e6b71d0401f8d88101-new3
staging-index-2bb5e4d08dd911e78474dec7170128a9
master-index-2bb5e4d08dd911e78474dec7170128a9
staging-index-0f65ca607d5911e7b17a0e4789ade3a3
master-index-e4010da4110ba377d100f050cb4440db-new3
master-index-0acedf50628d11e7b815a2f42be00f79
master-index-d3ae95b0d12811e69edf0401f8d88501
master-cf5e90403d5e11e68ac00401f8d88501
master-index-ef5d1930786711e7bd7edec7170128a9
staging-index-0c79ae508e6d11e78474dec7170128a9
master-index-92c46710884711e7ab080e4789ade3a3
staging-index-71dd4700628c11e7b2fa4a8136534b63
master-index-0f65ca607d5911e7b17a0e4789ade3a3
staging-index-e4010da4110ba377d100f050cb4440db-new3
staging-index-321bb9606b2111e7b579a2f42be00f79
master-index-71dd4700628c11e7b2fa4a8136534b63
master-index-0d60cee0843911e7b589dec7170128a9
master-index-0c79ae508e6d11e78474dec7170128a9
staging-index-91af8e607b9311e7aa950e4789ade3a3
master-index-aeada2408e6711e78bc30e4789ade3a3
staging-index-92c46710884711e7ab080e4789ade3a3
staging-index-03b541d0208111e7bbb0a2f42be00f79
master-index-321bb9606b2111e7b579a2f42be00f79
staging-index-aeada2408e6711e78bc30e4789ade3a3
staging-index-d3ae95b0d12811e69edf0401f8d88501
staging-index-0d60cee0843911e7b589dec7170128a9

** #  --8<-------------------------- separator ------------------------>8-- :noexport:
** 2017-10-31: CB disk issue: http://bematech-do-jenkins.carol.ai:18080/view/Maintanence/job/DeploySystemDOBematech/174/console
  CLOSED: [2017-10-31 Tue 13:54]
165.227.11.10  bematech-do-cb-19

#+BEGIN_EXAMPLE
Denny Zhang [12:22 PM] 
@bruno & @kungwang

Another thing I want to escalate.

Recently our CB cluster run into low disk again.
Last weekend, we have added one more CB node.

Here is what it looks now. (edited)


Denny Zhang
[12:23 PM] 
uploaded this image: CB disk
Add Comment




jenkins APP [12:23 PM] 
DeploySystemDOBematech - #173 Success after 28 min (Open)


Denny Zhang [12:23 PM] 
My question is: whether it’s because the user data grows fast indeed, or our data structure in CB has changed?


Bruno Volpato
[12:25 PM] 
user data is growing fast, but will have to take a look if we are not cleaning up something properly


Denny Zhang [12:26 PM] 
Got it. Then I will add one more CB node today.

When you have the time, please help me to examine the data. See whether we can do some cleanup.
#+END_EXAMPLE
** 2017-10-31: prod env, and merge to current branch: update cb nodes nagios: enable nagios couchbase log check
  CLOSED: [2017-10-31 Tue 16:58]

** 2017-10-31: remove 2 ES nodes, update jenkins and remove VMs
  CLOSED: [2017-10-31 Tue 21:36]
service elasticsearch stop

  165.227.0.28  bematech-do-es-23 
 138.68.46.142  bematech-do-es-24 

** 2017-10-31: (2) Identify and fix outdated DNS setting for all 3028 DNS records.
  CLOSED: [2017-10-31 Tue 21:53]
https://trello.com/c/Hry1ZApE
./delete_aws.sh "." "./delete1.txt" "ZEFB94UUWPTC2"
./delete_aws.sh "app.carol.ai." "./delete1.txt" "ZEFB94UUWPTC2"
./delete_aws.sh "bematech.carol.ai." "./delete1.txt" "ZEFB94UUWPTC2"
** 2017-10-31: totvslabs dns change A hosts to CNAME
  CLOSED: [2017-10-31 Tue 22:00]
Denny Zhang [2:59 PM] 
Robson, poll the full list of DNS entries.

No entries point to old haproxy nodes. So that part is good.


[3:01] 
Currently we have below A hosts
```carol.ai.
app.carol.ai.
bematechn.carol.ai.
schulz.carol.ai.
totvs.carol.ai.
weather.carol.ai.
```

It’s better we use CNAME, if possible.

I will update weather.carol.ai and schulz.carol.ai to CNAME today.

Then change others next week, if above changes won’t bring new issues.
** HALF use CNAME: weather.carol.ai, schulz.carol.ai
** TODO use CNAME: app.carol.ai, bematechn.carol.ai, totvs.carol.ai
** 2017-11-01: deploy explore env for the limitation
  CLOSED: [2017-11-01 Wed 13:43]
** 2017-11-03: update backup alerts
  CLOSED: [2017-11-03 Fri 09:56]
Nagios APP [12:30 AM] 
localhost/Current Load is CRITICAL:
CRITICAL - load average: 4.57, 4.49, 4.15


Denny Zhang [12:33 AM] 
Will change the CPU alert threshold tomorrow.

Thus people in this channel will less bothered by above alerts.
** 2017-11-03: make sure https://www.dennyzhang.com/slack_join works
  CLOSED: [2017-11-03 Fri 14:07]
** 2017-11-04: shutdown and retire cb-16
  CLOSED: [2017-11-04 Sat 10:46]
** 2017-11-04: Add | bematech-do-cb-20
  CLOSED: [2017-11-04 Sat 10:46]
138.197.220.13 bematech-do-cb-20
** 2017-11-04: start app-6
  CLOSED: [2017-11-04 Sat 13:27]
** 2017-11-04: add bematech-do-cb-16
  CLOSED: [2017-11-04 Sat 13:27]
138.68.22.237 bematech-do-cb-16

** 2017-11-07: Delete DNS entries
  CLOSED: [2017-11-07 Tue 13:11]
#+BEGIN_EXAMPLE
NameType
tenant004605.carol.ai.CNAME
tenant005074.carol.ai.CNAME
tenant006130.carol.ai.CNAME
tenant032178.carol.ai.CNAME
tenant061685.carol.ai.CNAME
tenant063626.carol.ai.CNAME
tenant068061.carol.ai.CNAME
tenant085520.carol.ai.CNAME
tenant104031.carol.ai.CNAME
tenant124448.carol.ai.CNAME
tenant133648.carol.ai.CNAME
tenant151515.carol.ai.CNAME
tenant154885.carol.ai.CNAME
tenant161351.carol.ai.CNAME
tenant217717.carol.ai.CNAME
tenant226502.carol.ai.CNAME
tenant230150.carol.ai.CNAME
tenant247707.carol.ai.CNAME
tenant248711.carol.ai.CNAME
tenant254826.carol.ai.CNAME
tenant260736.carol.ai.CNAME
tenant267201.carol.ai.CNAME
tenant353213.carol.ai.CNAME
tenant354243.carol.ai.CNAME
#+END_EXAMPLE

#+BEGIN_EXAMPLE

NameType
tenant357555delete81954.carol.ai.CNAME
tenant360414.carol.ai.CNAME
tenant367251.carol.ai.CNAME
tenant376705.carol.ai.CNAME
tenant376705delete93395.carol.ai.CNAME
tenant378770.carol.ai.CNAME
tenant385263.carol.ai.CNAME
tenant386348.carol.ai.CNAME
tenant403576.carol.ai.CNAME
tenant418150.carol.ai.CNAME
tenant420710.carol.ai.CNAME
tenant421880.carol.ai.CNAME
tenant422255.carol.ai.CNAME
tenant438031delete63646.carol.ai.CNAME
tenant442817.carol.ai.CNAME
tenant442817delete72406.carol.ai.CNAME
tenant456856.carol.ai.CNAME
tenant480217.carol.ai.CNAME
tenant481622.carol.ai.CNAME
tenant481622delete37328.carol.ai.CNAME
tenant488427.carol.ai.CNAME
tenant506065.carol.ai.CNAME
tenant508650.carol.ai.CNAME
tenant508650delete44349.carol.ai.CNAME
#+END_EXAMPLE

#+BEGIN_EXAMPLE
NameType
tenant512300delete5844.carol.ai.CNAME
tenant514735.carol.ai.CNAME
tenant521886.carol.ai.CNAME
tenant521886delete78136.carol.ai.CNAME
tenant524265.carol.ai.CNAME
tenant525231.carol.ai.CNAME
tenant525627.carol.ai.CNAME
tenant532136.carol.ai.CNAME
tenant584748.carol.ai.CNAME
tenant604720delete62814.carol.ai.CNAME
tenant622146delete76082.carol.ai.CNAME
tenant640446.carol.ai.CNAME
tenant640446delete79030.carol.ai.CNAME
tenant640587.carol.ai.CNAME
tenant653426delete48863.carol.ai.CNAME
tenant657202.carol.ai.CNAME
tenant671455.carol.ai.CNAME
tenant673262.carol.ai.CNAME
tenant688112.carol.ai.CNAME
tenant688112delete22139.carol.ai.CNAME
tenant700517.carol.ai.CNAME
tenant703171.carol.ai.CNAME
tenant703171delete23152.carol.ai.CNAME
tenant705520.carol.ai.CNAME
#+END_EXAMPLE

#+BEGIN_EXAMPLE
NameType
tenant714572.carol.ai.CNAME
tenant715384.carol.ai.CNAME
tenant716465.carol.ai.CNAME
tenant716465delete88497.carol.ai.CNAME
tenant723776.carol.ai.CNAME
tenant724467.carol.ai.CNAME
tenant728856.carol.ai.CNAME
tenant735468.carol.ai.CNAME
tenant751063delete70822.carol.ai.CNAME
tenant755756.carol.ai.CNAME
tenant755756delete32226.carol.ai.CNAME
tenant761018.carol.ai.CNAME
tenant761200.carol.ai.CNAME
tenant777820.carol.ai.CNAME
tenant820540.carol.ai.CNAME
tenant825354.carol.ai.CNAME
tenant826703.carol.ai.CNAME
tenant831216.carol.ai.CNAME
tenant851507.carol.ai.CNAME
tenant873505delete58654.carol.ai.CNAME
tenant873727.carol.ai.CNAME
tenant874767delete79593.carol.ai.CNAME
tenant878231.carol.ai.CNAME
tenant878724delete11152.carol.ai.CNAME
#+END_EXAMPLE
** 2017-11-07: Robson delete 4 obselete DNS entries
  CLOSED: [2017-11-07 Tue 16:09]
#+BEGIN_EXAMPLE
paulodelete58416.carol.ai.CNAME
rob1delete82048.carol.ai.CNAME
testdelete61210.carol.ai.CNAME
workshopdelete78958.carol.ai.CNAME
#+END_EXAMPLE
** 2017-11-07: Robson kibana check
  CLOSED: [2017-11-07 Tue 16:36]
** 2017-11-08: add one more worker: bematech-do-app-7
  CLOSED: [2017-11-08 Wed 09:46]
| bematech-do-app-7   |  138.197.217.239 | Type2          | app worker          |
** 2017-11-08: (2) check_haproxy_fd Nagios check fails from time to time
  CLOSED: [2017-11-08 Wed 12:28]
https://trello.com/c/JGS9kw1K
** 2017-11-09: es_audit: 1.78-es5-migration: http://injenkins.carol.ai:48080/job/DockerDeployAllInOne/85/console
  CLOSED: [2017-11-09 Thu 11:10]
#+BEGIN_EXAMPLE
[2017-11-08T00:16:31,716][INFO ][o.e.n.Node               ] [all-in-one-DockerDeployAllInOne-77] initializing ...
[2017-11-08T00:16:31,758][ERROR][o.e.b.Bootstrap          ] Exception
java.lang.IllegalStateException: failed to obtain node locks, tried [[/usr/share/elasticsearch/totvs_audit_DockerDeployAllInOne_77es, /usr/share/elasticsearch_audit/totvs_audit_DockerDeployAllInOne_77es]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?
        at org.elasticsearch.env.NodeEnvironment.<init>(NodeEnvironment.java:261) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.node.Node.<init>(Node.java:265) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.node.Node.<init>(Node.java:245) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:233) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:233) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:342) [elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:132) [elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:123) [elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:70) [elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:134) [elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.cli.Command.main(Command.java:90) [elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) [elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) [elasticsearch-5.6.3.jar:5.6.3]
[2017-11-08T00:16:31,765][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [all-in-one-DockerDeployAllInOne-77] uncaught exception in thread [main]
org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: failed to obtain node locks, tried [[/usr/share/elasticsearch/totvs_audit_DockerDeployAllInOne_77es, /usr/share/elasticsearch_audit/totvs_audit_DockerDeployAllInOne_77es]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:123) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:70) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:134) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) ~[elasticsearch-5.6.3.jar:5.6.3]
Caused by: java.lang.IllegalStateException: failed to obtain node locks, tried [[/usr/share/elasticsearch/totvs_audit_DockerDeployAllInOne_77es, /usr/share/elasticsearch_audit/totvs_audit_DockerDeployAllInOne_77es]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?
        at org.elasticsearch.env.NodeEnvironment.<init>(NodeEnvironment.java:261) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.node.Node.<init>(Node.java:265) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.node.Node.<init>(Node.java:245) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:233) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:233) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:342) ~[elasticsearch-5.6.3.jar:5.6.3]
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:132) ~[elasticsearch-5.6.3.jar:5.6.3]
        ... 6 more
#+END_EXAMPLE
** 2017-11-09: prod env support: load archive files, deploy master branch, retire 2 workers
  CLOSED: [2017-11-09 Thu 16:20]
** 2017-11-09: add 3 env
  CLOSED: [2017-11-09 Thu 18:24]
WEBSITE_PORT=80
WP_HOMEURL=http://brain.dennyzhang.com
WP_SITEURL=http://brain.dennyzhang.com:8083
** 2017-11-09: remove 2 VMs
  CLOSED: [2017-11-09 Thu 21:29]
** 2017-11-09: Can't login to admin https://www.dennyzhang.com/wp-admin
  CLOSED: [2017-11-09 Thu 23:09]

** 2017-11-14: Add 2 workers
  CLOSED: [2017-11-14 Tue 08:06]
165.227.18.191  bematech-do-app-6 
165.227.17.174  bematech-do-app-7 
** 2017-11-14: slack token about prod log env
  CLOSED: [2017-11-14 Tue 12:35]
slack token
xoxb-100486418085-0dm75CcwARHExvVesBsRqEqX -> xoxb-271677810085-jGBLEDMTC47SqOnUpzatL3gf
** 2017-11-14: False positive: http://bematech-do-jenkins.carol.ai:18080/job/ConfirmJarFileInMDMUI/246/console
  CLOSED: [2017-11-14 Tue 18:52]
http://bematech-do-jenkins.carol.ai:18080/job/RunCommandOnServers/60/console
** 2017-11-14: jenkins disk issue: we don't have enough disk; run another backup and delete the old one
  CLOSED: [2017-11-14 Tue 19:10]
sudo mkdir -p /data/backup2; sudo mount -o discard,defaults /dev/disk/by-id/scsi-0DO_Volume_volume-sfo2-151 /data/backup2; echo /dev/disk/by-id/scsi-0DO_Volume_volume-sfo2-151 /data/backup2 ext4 defaults,nofail,discard 0 0 | sudo tee -a /etc/fstab
** 2017-11-15: Add one more CB nod
  CLOSED: [2017-11-15 Wed 08:09]
138.68.8.188  bematech-do-cb-27 
** 2017-11-16: remove bematech-do-es-30, and remove VM: http://bematech-do-jenkins.carol.ai:18080/view/Pipeline/job/PipeLineRemoveNodes/43/console
  CLOSED: [2017-11-16 Thu 11:44]
138.68.253.129 bematech-do-es-30
** 2017-11-18: restart app-5 and app-6: http://bematech-do-jenkins.carol.ai:18080/job/RestartAllmdmBematechDO/53/console
  CLOSED: [2017-11-18 Sat 00:34]

** 2017-11-22: Add one more CB and ES node: http://bematech-do-jenkins.carol.ai:18080/view/Pipeline/job/PipeLineAddNodes/104/console
  CLOSED: [2017-11-22 Wed 23:41]
165.227.17.250 bematech-do-es-35 
165.227.24.67 bematech-do-cb-18
** 2017-12-01: Add 10 es nodes for bematech reindexing: http://bematech-do-jenkins.carol.ai:18080/view/Maintanence/job/DeploySystemDOBematech/223/console
  CLOSED: [2017-12-01 Fri 18:22]
http://bematech-do-jenkins.carol.ai:18080/view/Pipeline/job/PipeLineAddNodes/105/console
  159.89.140.13  bematech-do-es-1  
  138.68.59.205  bematech-do-es-4  
  159.89.140.37  bematech-do-es-5  
  159.89.140.16  bematech-do-es-7  
  159.89.140.11  bematech-do-es-8  
  159.89.140.52  bematech-do-es-9  
  159.89.140.56  bematech-do-es-10 
  159.89.140.24  bematech-do-es-11 
   138.68.26.13  bematech-do-es-12 
 165.227.49.160  bematech-do-es-13 


159.89.140.13:2702
138.68.59.205:2702
159.89.140.37:2702
159.89.140.16:2702
159.89.140.11:2702
159.89.140.52:2702
159.89.140.56:2702
159.89.140.24:2702
138.68.26.13:2702
165.227.49.160:2702

** 2017-12-03: Mitu re-index task
  CLOSED: [2017-12-03 Sun 23:03]
http://bematech-do-jenkins.carol.ai:18080/job/ESReIndexDenny/

https://github.com/TOTVS/mdmdevops/wiki/Dec,-2017:-ES-migration-for-new-schema:-Prod-DO-Env

es_ip=45.79.80.81
es_port=9200

curl $es_ip:$es_port/_cat/indices?v

ssh -N -p 22 -i id_rsa -f root@172.17.0.2 -L *:9200:localhost:9200 -n /bin/bash

ssh -i /var/jenkins_home/.ssh/ci_id_rsa root@45.79.80.81 date
** all-in-one sandbox test
#+BEGIN_EXAMPLE
green  open   config-index-8cd6e43115e9416eb23609486fa053e3    1   0        421           11      1.2mb          1.2mb
green  open   config-index-e4010da4110ba377d100f050cb4440db    1   0        258            1    309.9kb        309.9kb
green  open   staging-index-e4010da4110ba377d100f050cb4440db   1   0       3676          101      5.2mb          5.2mb
green  open   master-index-e4010da4110ba377d100f050cb4440db    1   0         52            3     88.7kb         88.7kb
green  open   master-index-8cd6e43115e9416eb23609486fa053e3    1   0         29            1     95.5kb         95.5kb
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3   1   0          1            0     90.3kb         90.3kb
#+END_EXAMPLE

config-index-8cd6e43115e9416eb23609486fa053e3
config-index-e4010da4110ba377d100f050cb4440db
staging-index-e4010da4110ba377d100f050cb4440db
master-index-e4010da4110ba377d100f050cb4440db
master-index-8cd6e43115e9416eb23609486fa053e3
staging-index-8cd6e43115e9416eb23609486fa053e3
*** es: http://bematech-do-jenkins.carol.ai:18080/job/ESReIndexDenny/4/console
*** es: http://bematech-do-jenkins.carol.ai:18080/job/ESReIndexDenny/5/console
*** es: http://bematech-do-jenkins.carol.ai:18080/job/ESReIndexDenny/6/console
** run in explore env
45.79.80.81
** HALF test in explore env
**** 2017-12-03: totvslabs.carol.ai: root 8cd6e43115e9416eb23609486fa053e3 and totvslabs tenant
  CLOSED: [2017-11-30 Thu 18:23]
admin@totvslabs.com/Foobar1!

green  open   config-index-8cd6e43115e9416eb23609486fa053e3         1   0        464            4      1.3mb          1.3mb
green  open   master-index-8cd6e43115e9416eb23609486fa053e3-new2    5   0       1968          410      2.9mb          2.9mb
green  open   staging-index-8cd6e43115e9416eb23609486fa053e3-new2   5   0          0            0       795b           795b

#+BEGIN_EXAMPLE
config-index-8cd6e43115e9416eb23609486fa053e3
master-index-8cd6e43115e9416eb23609486fa053e3-new2
staging-index-8cd6e43115e9416eb23609486fa053e3-new2
#+END_EXAMPLE

green  open   config-index-e4010da4110ba377d100f050cb4440db         1   0        774           37      5.2mb          5.2mb 
green  open   master-index-e4010da4110ba377d100f050cb4440db-new2    5   0      36735            0       12mb           12mb 
green  open   staging-index-e4010da4110ba377d100f050cb4440db-new2   5   0       4359           82        8mb            8mb 

#+BEGIN_EXAMPLE
config-index-e4010da4110ba377d100f050cb4440db
master-index-e4010da4110ba377d100f050cb4440db-new2
staging-index-e4010da4110ba377d100f050cb4440db-new2
#+END_EXAMPLE
*** HALF run with all other tenants
http://bematech-do-jenkins.carol.ai:18080/job/ESReIndexDenny/14/console
http://bematech-do-jenkins.carol.ai:18080/job/ESReIndexDenny/15/console
http://bematech-do-jenkins.carol.ai:18080/job/ESReIndexDenny/19/console

#+BEGIN_EXAMPLE
green  open   config-index-2e30a710c57711e79a230242ac110003         1   0        208            5    272.6kb        272.6kb 
green  open   staging-index-6e4ad210c3df11e792b20242ac110003        1   0          5            0    139.8kb        139.8kb 
green  open   staging-index-b9184120c3de11e7afee0242ac110003        1   0        228            0    210.6kb        210.6kb 
green  open   master-index-b9184120c3de11e7afee0242ac110003         1   0        208            0    261.4kb        261.4kb 
green  open   master-index-0ab28af0c57811e79a230242ac110003         1   0          0            0       159b           159b 
green  open   staging-index-0ab28af0c57811e79a230242ac110003        1   0          0            0       159b           159b 
green  open   master-index-6e4ad210c3df11e792b20242ac110003         1   0         86            0    122.7kb        122.7kb 
green  open   config-index-98b28be03c1411e783570242ac110002         1   0        166           27    487.8kb        487.8kb 
green  open   master-index-f6d6c7b0bad911e7a8650242ac110003         1   0          1            0     12.2kb         12.2kb 
green  open   staging-index-f0d9d380bcd611e785120242ac110003        1   0          0            0       159b           159b 
green  open   staging-index-2e30a710c57711e79a230242ac110003        1   0        678         1514    720.5kb        720.5kb 
green  open   config-index-5b010c60f21111e6a49e0242ac110003         1   0       2206           14      6.2mb          6.2mb 
green  open   staging-index-df330bb0c3de11e7afee0242ac110003        1   0          0            0       159b           159b 
green  open   staging-index-f6d6c7b0bad911e7a8650242ac110003        1   0          0            0       159b           159b 
green  open   master-index-f0d9d380bcd611e785120242ac110003         1   0          0            0       159b           159b 
green  open   config-index-81ad4330ca3611e793900242ac110003         1   0          1            0       27kb           27kb 
green  open   master-index-72503980a85b11e7a2900242ac110003         1   0        641            0    588.2kb        588.2kb 
green  open   config-index-0409c810c3e011e792b20242ac110003         1   0          4            1     53.1kb         53.1kb 
green  open   config-index-f0d9d380bcd611e785120242ac110003         1   0          5            0    146.1kb        146.1kb 
green  open   master-index-eb841160bad911e7a8650242ac110003         1   0        100            0     92.4kb         92.4kb 
green  open   staging-index-e95f5000c57611e79a230242ac110003        1   0         53            0      104kb          104kb 
green  open   master-index-e95f5000c57611e79a230242ac110003         1   0       1539            0    896.5kb        896.5kb 
green  open   config-index-b3bbd120c57711e79a230242ac110003         1   0         34            9    231.4kb        231.4kb 
green  open   config-index-0ab28af0c57811e79a230242ac110003         1   0          0            0       159b           159b 
green  open   config-index-df330bb0c3de11e7afee0242ac110003         1   0          1            0     23.6kb         23.6kb 
green  open   staging-index-895b6da0c57711e79a230242ac110003        1   0          0            0       159b           159b 
green  open   master-index-895b6da0c57711e79a230242ac110003         1   0          0            0       159b           159b 
green  open   config-index-ce7aa4a0208611e7808d0242ac110003         1   0        272            4      450kb          450kb 
green  open   config-index-01ab7460bada11e7a8650242ac110003         1   0         86            3    197.8kb        197.8kb 
green  open   config-index-b5c8d1f04a2211e78a470242ac110003         1   0       3673            5      9.4mb          9.4mb 
green  open   staging-index-380f9e90c57611e79a230242ac110003        1   0          0            0       159b           159b 
green  open   master-index-380f9e90c57611e79a230242ac110003         1   0          0            0       159b           159b 
green  open   staging-index-ce7aa4a0208611e7808d0242ac110003-new2   5   0        824            0        1mb            1mb 
green  open   staging-index-b3bbd120c57711e79a230242ac110003        1   0        159            1    172.6kb        172.6kb 
green  open   staging-index-eb841160bad911e7a8650242ac110003        1   0          9            0     93.6kb         93.6kb 
green  open   config-index-8d45c660a85b11e7a2900242ac110003         1   0        766            6    693.1kb        693.1kb 
green  open   staging-index-72503980a85b11e7a2900242ac110003        1   0         61           49    223.2kb        223.2kb 
green  open   config-index-380f9e90c57611e79a230242ac110003         1   0          0            0       159b           159b 
green  open   config-index-c0865a60c57711e79a230242ac110003         1   0         42            4    186.1kb        186.1kb 
green  open   master-index-81ad4330ca3611e793900242ac110003         1   0          0            0       159b           159b 
green  open   master-index-b3bbd120c57711e79a230242ac110003         1   0       1995            0        1mb            1mb 
green  open   config-index-6e4ad210c3df11e792b20242ac110003         1   0         58            3    139.4kb        139.4kb 
green  open   config-index-72503980a85b11e7a2900242ac110003         1   0        236            1    357.7kb        357.7kb 
green  open   config-index-895b6da0c57711e79a230242ac110003         1   0          1            0      9.2kb          9.2kb 
green  open   config-index-eb841160bad911e7a8650242ac110003         1   0        201            4    439.5kb        439.5kb 
green  open   config-index-f6d6c7b0bad911e7a8650242ac110003         1   0          7            0     87.9kb         87.9kb 
green  open   staging-index-81ad4330ca3611e793900242ac110003        1   0          0            0       159b           159b 
green  open   staging-index-b5c8d1f04a2211e78a470242ac110003-new2   5   0     138001        36245    643.9mb        643.9mb 
green  open   staging-index-78617ec0cb5a11e7904e0242ac110003        1   0          0            0       159b           159b 
green  open   master-index-7d2a7d40b35811e7ae870242ac110003         1   0          0            0       159b           159b 
green  open   config-index-f4a6fd30c3de11e7afee0242ac110003         1   0         78           10    223.7kb        223.7kb 
green  open   staging-index-98b28be03c1411e783570242ac110002-new2   5   0    2507013        29849      1.2gb          1.2gb 
green  open   config-index-0580a610c3df11e7afee0242ac110003         1   0         83            6    194.3kb        194.3kb 
green  open   master-index-ce7aa4a0208611e7808d0242ac110003-new2    5   0       8883            0        6mb            6mb 
green  open   config-index-7d2a7d40b35811e7ae870242ac110003         1   0          2            0     18.7kb         18.7kb 
green  open   master-index-f4a6fd30c3de11e7afee0242ac110003         1   0        106            0    132.5kb        132.5kb 
green  open   staging-index-0580a610c3df11e7afee0242ac110003        1   0         33            0    201.6kb        201.6kb 
green  open   staging-index-f4a6fd30c3de11e7afee0242ac110003        1   0         61            9    177.2kb        177.2kb 
green  open   master-index-01ab7460bada11e7a8650242ac110003         1   0         51            0     82.5kb         82.5kb 
green  open   master-index-0580a610c3df11e7afee0242ac110003         1   0        156            0    211.5kb        211.5kb 
green  open   config-index-ff9eb260c57711e79a230242ac110003         1   0        151            1    172.6kb        172.6kb 
green  open   config-index-78617ec0cb5a11e7904e0242ac110003         1   0          6            0     78.5kb         78.5kb 
green  open   staging-index-c0865a60c57711e79a230242ac110003        1   0        104          298    330.7kb        330.7kb 
green  open   master-index-8d45c660a85b11e7a2900242ac110003         1   0    4002050           10      1.5gb          1.5gb 
green  open   master-index-a5daf5e0c57711e79a230242ac110003         1   0       1544            0    883.4kb        883.4kb 
green  open   staging-index-8d45c660a85b11e7a2900242ac110003        1   0        115            0      8.8mb          8.8mb 
green  open   staging-index-a5daf5e0c57711e79a230242ac110003        1   0         53            0    104.2kb        104.2kb 
green  open   staging-index-01ab7460bada11e7a8650242ac110003        1   0          1            0     40.7kb         40.7kb 
green  open   master-index-c0865a60c57711e79a230242ac110003         1   0       1570            0    963.8kb        963.8kb 
green  open   staging-index-7d2a7d40b35811e7ae870242ac110003        1   0          0            0       159b           159b 
green  open   master-index-98b28be03c1411e783570242ac110002-new2    5   0       2201            0      1.1mb          1.1mb 
green  open   config-index-e95f5000c57611e79a230242ac110003         1   0         36            6    147.6kb        147.6kb 
green  open   master-index-cd2bc4e0c57611e79a230242ac110003         1   0       1538            0    897.5kb        897.5kb 
green  open   staging-index-0409c810c3e011e792b20242ac110003        1   0         39            0     91.4kb         91.4kb 
green  open   staging-index-5c0cde40c3df11e792b20242ac110003        1   0          0            0       159b           159b 
green  open   master-index-5c0cde40c3df11e792b20242ac110003         1   0          0            0       159b           159b 
green  open   staging-index-cd2bc4e0c57611e79a230242ac110003        1   0         53            0    104.2kb        104.2kb 
green  open   staging-index-5b010c60f21111e6a49e0242ac110003-new2   5   0    1492162          114      5.9gb          5.9gb 
green  open   config-index-a5daf5e0c57711e79a230242ac110003         1   0        137            3    234.8kb        234.8kb 
green  open   master-index-5b010c60f21111e6a49e0242ac110003-new2    5   0    3151761         2291        2gb            2gb 
green  open   staging-index-ff9eb260c57711e79a230242ac110003        1   0        208          104    207.3kb        207.3kb 
green  open   master-index-df330bb0c3de11e7afee0242ac110003         1   0          0            0       159b           159b 
green  open   master-index-2e30a710c57711e79a230242ac110003         1   0       1538            0        1mb            1mb 
green  open   master-index-ff9eb260c57711e79a230242ac110003         1   0       1425            0    824.8kb        824.8kb 
green  open   config-index-5c0cde40c3df11e792b20242ac110003         1   0          1            0        9kb            9kb 
green  open   config-index-b9184120c3de11e7afee0242ac110003         1   0         85            5    194.1kb        194.1kb 
green  open   master-index-78617ec0cb5a11e7904e0242ac110003         1   0          0            0       159b           159b 
green  open   master-index-b5c8d1f04a2211e78a470242ac110003-new2    5   0       1448            0      4.6mb          4.6mb 
green  open   config-index-cd2bc4e0c57611e79a230242ac110003         1   0         33            4    184.4kb        184.4kb 
green  open   master-index-0409c810c3e011e792b20242ac110003         1   0         51            0     94.3kb         94.3kb 
#+END_EXAMPLE

config-index-2e30a710c57711e79a230242ac110003
staging-index-6e4ad210c3df11e792b20242ac110003
staging-index-b9184120c3de11e7afee0242ac110003
master-index-b9184120c3de11e7afee0242ac110003
master-index-0ab28af0c57811e79a230242ac110003
staging-index-0ab28af0c57811e79a230242ac110003
master-index-6e4ad210c3df11e792b20242ac110003
config-index-98b28be03c1411e783570242ac110002
master-index-f6d6c7b0bad911e7a8650242ac110003
staging-index-f0d9d380bcd611e785120242ac110003
staging-index-2e30a710c57711e79a230242ac110003
config-index-5b010c60f21111e6a49e0242ac110003
staging-index-df330bb0c3de11e7afee0242ac110003
staging-index-f6d6c7b0bad911e7a8650242ac110003
master-index-f0d9d380bcd611e785120242ac110003
config-index-81ad4330ca3611e793900242ac110003
master-index-72503980a85b11e7a2900242ac110003
config-index-0409c810c3e011e792b20242ac110003
config-index-f0d9d380bcd611e785120242ac110003
master-index-eb841160bad911e7a8650242ac110003
staging-index-e95f5000c57611e79a230242ac110003
master-index-e95f5000c57611e79a230242ac110003
config-index-b3bbd120c57711e79a230242ac110003
config-index-0ab28af0c57811e79a230242ac110003
config-index-df330bb0c3de11e7afee0242ac110003
staging-index-895b6da0c57711e79a230242ac110003
master-index-895b6da0c57711e79a230242ac110003
config-index-ce7aa4a0208611e7808d0242ac110003
config-index-01ab7460bada11e7a8650242ac110003
config-index-b5c8d1f04a2211e78a470242ac110003
staging-index-380f9e90c57611e79a230242ac110003
master-index-380f9e90c57611e79a230242ac110003
staging-index-ce7aa4a0208611e7808d0242ac110003-new2
staging-index-b3bbd120c57711e79a230242ac110003
staging-index-eb841160bad911e7a8650242ac110003
config-index-8d45c660a85b11e7a2900242ac110003
staging-index-72503980a85b11e7a2900242ac110003
config-index-380f9e90c57611e79a230242ac110003
config-index-c0865a60c57711e79a230242ac110003
master-index-81ad4330ca3611e793900242ac110003
master-index-b3bbd120c57711e79a230242ac110003
config-index-6e4ad210c3df11e792b20242ac110003
config-index-72503980a85b11e7a2900242ac110003
config-index-895b6da0c57711e79a230242ac110003
config-index-eb841160bad911e7a8650242ac110003
config-index-f6d6c7b0bad911e7a8650242ac110003
staging-index-81ad4330ca3611e793900242ac110003
staging-index-b5c8d1f04a2211e78a470242ac110003-new2
staging-index-78617ec0cb5a11e7904e0242ac110003
master-index-7d2a7d40b35811e7ae870242ac110003
config-index-f4a6fd30c3de11e7afee0242ac110003
staging-index-98b28be03c1411e783570242ac110002-new2
config-index-0580a610c3df11e7afee0242ac110003
master-index-ce7aa4a0208611e7808d0242ac110003-new2
config-index-7d2a7d40b35811e7ae870242ac110003
master-index-f4a6fd30c3de11e7afee0242ac110003
staging-index-0580a610c3df11e7afee0242ac110003
staging-index-f4a6fd30c3de11e7afee0242ac110003
master-index-01ab7460bada11e7a8650242ac110003
master-index-0580a610c3df11e7afee0242ac110003
config-index-ff9eb260c57711e79a230242ac110003
config-index-78617ec0cb5a11e7904e0242ac110003
staging-index-c0865a60c57711e79a230242ac110003
master-index-8d45c660a85b11e7a2900242ac110003
master-index-a5daf5e0c57711e79a230242ac110003
staging-index-8d45c660a85b11e7a2900242ac110003
staging-index-a5daf5e0c57711e79a230242ac110003
staging-index-01ab7460bada11e7a8650242ac110003
master-index-c0865a60c57711e79a230242ac110003
staging-index-7d2a7d40b35811e7ae870242ac110003
master-index-98b28be03c1411e783570242ac110002-new2
config-index-e95f5000c57611e79a230242ac110003
master-index-cd2bc4e0c57611e79a230242ac110003
staging-index-0409c810c3e011e792b20242ac110003
staging-index-5c0cde40c3df11e792b20242ac110003
master-index-5c0cde40c3df11e792b20242ac110003
staging-index-cd2bc4e0c57611e79a230242ac110003
staging-index-5b010c60f21111e6a49e0242ac110003-new2
config-index-a5daf5e0c57711e79a230242ac110003
master-index-5b010c60f21111e6a49e0242ac110003-new2
staging-index-ff9eb260c57711e79a230242ac110003
master-index-df330bb0c3de11e7afee0242ac110003
master-index-2e30a710c57711e79a230242ac110003
master-index-ff9eb260c57711e79a230242ac110003
config-index-5c0cde40c3df11e792b20242ac110003
config-index-b9184120c3de11e7afee0242ac110003
master-index-78617ec0cb5a11e7904e0242ac110003
master-index-b5c8d1f04a2211e78a470242ac110003-new2
config-index-cd2bc4e0c57611e79a230242ac110003
master-index-0409c810c3e011e792b20242ac110003
*** TODO delete closed indices

** 2017-12-05: totvslabs: delete the closed explore index
  CLOSED: [2017-12-05 Tue 11:27]
#+BEGIN_EXAMPLE
]0;root@aio: /root@aio:/# curl $es_ip:9200/_cat/indices?v | grep close
curl $es_ip:9200/_cat/indices?v | grep close
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 23750  100 23750    0     0  35702      0 --:--:-- --:--:-- --:--:-- 35714
       close  config-index-2e30a710c57711e79a230242ac110003                                                                 
       close  staging-index-6e4ad210c3df11e792b20242ac110003                                                                
       close  staging-index-b9184120c3de11e7afee0242ac110003                                                                
       close  master-index-b9184120c3de11e7afee0242ac110003                                                                 
       close  master-index-0ab28af0c57811e79a230242ac110003                                                                 
       close  staging-index-0ab28af0c57811e79a230242ac110003                                                                
       close  master-index-6e4ad210c3df11e792b20242ac110003                                                                 
       close  config-index-98b28be03c1411e783570242ac110002                                                                 
       close  master-index-f6d6c7b0bad911e7a8650242ac110003                                                                 
       close  staging-index-f0d9d380bcd611e785120242ac110003                                                                
       close  staging-index-2e30a710c57711e79a230242ac110003                                                                
       close  config-index-5b010c60f21111e6a49e0242ac110003                                                                 
       close  staging-index-df330bb0c3de11e7afee0242ac110003                                                                
       close  staging-index-f6d6c7b0bad911e7a8650242ac110003                                                                
       close  staging-index-8cd6e43115e9416eb23609486fa053e3-new2                                                           
       close  master-index-f0d9d380bcd611e785120242ac110003                                                                 
       close  config-index-81ad4330ca3611e793900242ac110003                                                                 
       close  master-index-72503980a85b11e7a2900242ac110003                                                                 
       close  config-index-0409c810c3e011e792b20242ac110003                                                                 
       close  config-index-f0d9d380bcd611e785120242ac110003                                                                 
       close  master-index-eb841160bad911e7a8650242ac110003                                                                 
       close  staging-index-e95f5000c57611e79a230242ac110003                                                                
       close  master-index-e95f5000c57611e79a230242ac110003                                                                 
       close  config-index-b3bbd120c57711e79a230242ac110003                                                                 
       close  config-index-0ab28af0c57811e79a230242ac110003                                                                 
       close  config-index-df330bb0c3de11e7afee0242ac110003                                                                 
       close  staging-index-895b6da0c57711e79a230242ac110003                                                                
       close  master-index-895b6da0c57711e79a230242ac110003                                                                 
       close  config-index-ce7aa4a0208611e7808d0242ac110003                                                                 
       close  config-index-01ab7460bada11e7a8650242ac110003                                                                 
       close  config-index-b5c8d1f04a2211e78a470242ac110003                                                                 
       close  staging-index-380f9e90c57611e79a230242ac110003                                                                
       close  master-index-380f9e90c57611e79a230242ac110003                                                                 
       close  staging-index-ce7aa4a0208611e7808d0242ac110003-new2                                                           
       close  staging-index-b3bbd120c57711e79a230242ac110003                                                                
       close  staging-index-eb841160bad911e7a8650242ac110003                                                                
       close  config-index-8d45c660a85b11e7a2900242ac110003                                                                 
       close  staging-index-72503980a85b11e7a2900242ac110003                                                                
       close  config-index-380f9e90c57611e79a230242ac110003                                                                 
       close  config-index-c0865a60c57711e79a230242ac110003                                                                 
       close  master-index-81ad4330ca3611e793900242ac110003                                                                 
       close  master-index-b3bbd120c57711e79a230242ac110003                                                                 
       close  config-index-6e4ad210c3df11e792b20242ac110003                                                                 
       close  config-index-72503980a85b11e7a2900242ac110003                                                                 
       close  config-index-895b6da0c57711e79a230242ac110003                                                                 
       close  config-index-eb841160bad911e7a8650242ac110003                                                                 
       close  config-index-f6d6c7b0bad911e7a8650242ac110003                                                                 
       close  staging-index-81ad4330ca3611e793900242ac110003                                                                
       close  config-index-e4010da4110ba377d100f050cb4440db                                                                 
       close  staging-index-b5c8d1f04a2211e78a470242ac110003-new2                                                           
       close  staging-index-78617ec0cb5a11e7904e0242ac110003                                                                
       close  master-index-7d2a7d40b35811e7ae870242ac110003                                                                 
       close  config-index-f4a6fd30c3de11e7afee0242ac110003                                                                 
       close  config-index-8cd6e43115e9416eb23609486fa053e3                                                                 
       close  master-index-8cd6e43115e9416eb23609486fa053e3-new2                                                            
       close  staging-index-98b28be03c1411e783570242ac110002-new2                                                           
       close  config-index-0580a610c3df11e7afee0242ac110003                                                                 
       close  master-index-ce7aa4a0208611e7808d0242ac110003-new2                                                            
       close  config-index-7d2a7d40b35811e7ae870242ac110003                                                                 
       close  master-index-f4a6fd30c3de11e7afee0242ac110003                                                                 
       close  staging-index-0580a610c3df11e7afee0242ac110003                                                                
       close  staging-index-f4a6fd30c3de11e7afee0242ac110003                                                                
       close  master-index-01ab7460bada11e7a8650242ac110003                                                                 
       close  master-index-0580a610c3df11e7afee0242ac110003                                                                 
       close  config-index-ff9eb260c57711e79a230242ac110003                                                                 
       close  config-index-78617ec0cb5a11e7904e0242ac110003                                                                 
       close  staging-index-c0865a60c57711e79a230242ac110003                                                                
       close  master-index-8d45c660a85b11e7a2900242ac110003                                                                 
       close  master-index-a5daf5e0c57711e79a230242ac110003                                                                 
       close  staging-index-8d45c660a85b11e7a2900242ac110003                                                                
       close  staging-index-a5daf5e0c57711e79a230242ac110003                                                                
       close  staging-index-01ab7460bada11e7a8650242ac110003                                                                
       close  master-index-c0865a60c57711e79a230242ac110003                                                                 
       close  staging-index-7d2a7d40b35811e7ae870242ac110003                                                                
       close  master-index-98b28be03c1411e783570242ac110002-new2                                                            
       close  config-index-e95f5000c57611e79a230242ac110003                                                                 
       close  master-index-cd2bc4e0c57611e79a230242ac110003                                                                 
       close  staging-index-0409c810c3e011e792b20242ac110003                                                                
       close  master-index-e4010da4110ba377d100f050cb4440db-new2                                                            
       close  staging-index-5c0cde40c3df11e792b20242ac110003                                                                
       close  master-index-5c0cde40c3df11e792b20242ac110003                                                                 
       close  staging-index-cd2bc4e0c57611e79a230242ac110003                                                                
       close  staging-index-5b010c60f21111e6a49e0242ac110003-new2                                                           
       close  config-index-a5daf5e0c57711e79a230242ac110003                                                                 
       close  master-index-5b010c60f21111e6a49e0242ac110003-new2                                                            
       close  staging-index-e4010da4110ba377d100f050cb4440db-new2                                                           
       close  staging-index-ff9eb260c57711e79a230242ac110003                                                                
       close  master-index-df330bb0c3de11e7afee0242ac110003                                                                 
       close  master-index-2e30a710c57711e79a230242ac110003                                                                 
       close  master-index-ff9eb260c57711e79a230242ac110003                                                                 
       close  config-index-5c0cde40c3df11e792b20242ac110003                                                                 
       close  config-index-b9184120c3de11e7afee0242ac110003                                                                 
       close  master-index-78617ec0cb5a11e7904e0242ac110003                                                                 
       close  master-index-b5c8d1f04a2211e78a470242ac110003-new2                                                            
       close  config-index-cd2bc4e0c57611e79a230242ac110003                                                                 
       close  master-index-0409c810c3e011e792b20242ac110003                                                                 
#+END_EXAMPLE
** 2017-12-05: git revert change 1.79 and 1.80
  CLOSED: [2017-12-05 Tue 18:41]
https://github.com/TOTVS/mdm/commit/2d9bdd657fd1bd54bd7b0849ad17e8c5df17e2d1

1.79:
- https://github.com/TOTVS/mdm/commit/535950ae974970ecf8a2d2aa7dbcadcc08734064

1.80:
- https://github.com/TOTVS/mdm/pull/953
- https://github.com/TOTVS/mdm/commit/27cdc59a0a6c1df426104e5325031cc8405c4540
** 2017-12-06: remove: | bematech-do-es-7    |   159.89.140.16
  CLOSED: [2017-12-06 Wed 09:35]
** 2017-12-06: add one more cb node: bematech-do-cb-12
  CLOSED: [2017-12-06 Wed 09:35]
165.227.53.27 bematech-do-cb-12

| bematech-do-cb-12   |   165.227.53.27 | Type3          | Couchbase           |
** 2017-12-07: Add one more CB node
  CLOSED: [2017-12-07 Thu 16:21]
138.68.53.222 bematech-do-cb-13
| bematech-do-cb-13   |   138.68.53.222 | Type3          | Couchbase           |
** 2017-12-15: change email password
  CLOSED: [2017-12-15 Fri 07:45]
Change.This@that

Change.That@this1
Change.This@123that
Change.That@123this

Change.This@that

** 2017-12-15: add one more CB node
  CLOSED: [2017-12-15 Fri 07:53]
138.68.245.159 bematech-do-cb-14
| bematech-do-cb-14   |   138.68.245.159 | Type3          | Couchbase           |
** 2017-12-27: Add one more CB node
  CLOSED: [2017-12-27 Wed 13:08]
159.89.128.219 bematech-do-cb-28
| bematech-do-cb-28   |  159.89.128.219 | Type3          | Couchbase           |
** 2018-01-07: Add one more CB node for disk issue: http://bematech-do-jenkins.carol.ai:18080/view/Pipeline/job/PipeLineAddNodes/110/console
  CLOSED: [2018-01-07 Sun 13:35]
165.227.15.85
** 2018-01-11: Add one more CB node
  CLOSED: [2018-01-11 Thu 13:37]
| bematech-do-cb-29   |  138.68.231.193 | Type3          | Couchbase           |
138.68.231.193 bematech-do-cb-29
** 2018-01-11: Check CB node rebalancing
  CLOSED: [2018-01-11 Thu 18:48]
** 2018-01-11: [#A] start mdm worker nodes
  CLOSED: [2018-01-11 Thu 18:48]

** 2018-01-12: retire one CB node: delete VM
  CLOSED: [2018-01-12 Fri 15:09]
| bematech-do-cb-27   |    138.68.8.188 | Type3          | Couchbase           | 2018/01/12        |
138.68.8.188 bematech-do-cb-27
** 2018-01-14: Add two more CB nodes to replace the existing one
  CLOSED: [2018-01-14 Sun 11:44]
| bematech-do-cb-30   | 138.197.214.177 | Type3          | Couchbase           |
| bematech-do-cb-31   |    138.68.8.182 | Type3          | Couchbase           |

138.197.214.177  bematech-do-cb-30 
138.68.8.182  bematech-do-cb-31 
** #  --8<-------------------------- separator ------------------------>8-- :noexport:
** 2018-01-16: fix es alias in prod env
  CLOSED: [2018-01-16 Tue 17:29]
root@bematech-do-es-2:/var/log/elasticsearch# curl "http://${es_ip}:9200/_alias/staging-321bb9606b2111e7b579a2f42be00f79"
{"staging-index-321bb9606b2111e7b579a2f42be00f79":{"aliases":{"staging-321bb9606b2111e7b579a2f42be00f79":{}}}}

root@bematech-do-es-2:/var/log/elasticsearch#
root@bematech-do-es-2:/var/log/elasticsearch#
root@bematech-do-es-2:/var/log/elasticsearch#
root@bematech-do-es-2:/var/log/elasticsearch#
root@bematech-do-es-2:/var/log/elasticsearch# curl http://bematech-do-es-1:9200/staging-321bb9606b2111e7b579a2f42be00f79/_alias
{"staging-index-321bb9606b2111e7b579a2f42be00f79":{"aliases":{"staging-321bb9606b2111e7b579a2f42be00f79":{}}},"staging-index-321bb9606b2111e7b579a2f42be00f79-new2":{"aliases":{"staging-321bb9606b2111e7b579a2f42be00f79":{}}}}


curl -XPOST "http://$es_ip:9200/_aliases" -d '{
"actions" : [
{"add":{"index":"staging-index-321bb9606b2111e7b579a2f42be00f79-new3","alias":"staging-321bb9606b2111e7b579a2f42be00f79"}},
{"remove":{"index":"staging-index-321bb9606b2111e7b579a2f42be00f79","alias":"staging-321bb9606b2111e7b579a2f42be00f79"}}]}'

curl -XPOST "http://$es_ip:9200/_aliases" -d '{
"actions" : [
{"remove":{"index":"staging-index-321bb9606b2111e7b579a2f42be00f79-new2","alias":"staging-321bb9606b2111e7b579a2f42be00f79"}}]}'

curl http://bematech-do-es-1:9200/staging-321bb9606b2111e7b579a2f42be00f79/_alias

** 2018-01-16: fix 3 wilson's QA envs and explore env
  CLOSED: [2018-01-16 Tue 17:29]
** 2018-01-23: prod-doc 505 error
  CLOSED: [2018-01-23 Tue 09:24]
** 2018-01-25: Add 5 ES nodes
  CLOSED: [2018-01-25 Thu 14:05]
 159.89.141.172  bematech-do-es-010 
  159.89.131.41  bematech-do-es-011 
  159.89.139.43  bematech-do-es-012 
 165.227.53.121  bematech-do-es-013 
 159.89.131.106  bematech-do-es-014 

159.89.141.172:2702
159.89.131.41:2702
159.89.139.43:2702
165.227.53.121:2702
159.89.131.106:2702
138.197.192.172:2702
** 2018-01-25: pipeline job to check everything
  CLOSED: [2018-01-25 Thu 15:49]

** 2018-01-25: python heapq
  CLOSED: [2018-01-25 Thu 21:24]
http://www.koderdojo.com/blog/binary-heap-and-heapq-in-python

import heapq
 
# initializing list
li = [5, 7, 9, 1, 3]
 
# using heapify to convert list into heap
heapq.heapify(li) # a minheap
heapq._heapify_max(li) # for a maxheap! 

# printing created heap
print (list(li))
 
# using heappush() to push elements into heap
# pushes 4
heapq.heappush(li,4)
 
# printing modified heap
print (list(li))
 
# using heappop() to pop smallest element
print (heapq.heappop(li))

print (list(li))
*** 2 attributes
import heapq
li = []
heapq._heapify_max(li) 
heapq.heappush(li, (5, 'write code'))
heapq.heappush(li, (7, 'release product'))
heapq.heappush(li, (1, 'write spec'))
heapq.heappush(li, (3, 'create tests'))
print (list(li))

heapq.heappop(li)
*** more sample
#+BEGIN_SRC python
import heapq

heap = [12, 19, 88, 62, 2, 14, 92]

heapq.heapify(heap)

heapq.heappush(heap, 3)

while len(heap) > 0:
    print(heapq.heappop(heap), end=' ')

# 2 3 12 14 19 62 88 92
#+END_SRC
** 2018-01-29: retire jobs:  delete bematech-cb-31 and 3 es nodes, then run digitalocean mdm-cluster test again
  CLOSED: [2018-01-29 Mon 09:23]
165.227.5.218  bematech-do-es-33 
165.227.13.11  bematech-do-es-34 
165.227.17.250  bematech-do-es-35 

** 2018-01-31: Enforce daily mdm-config backup
  CLOSED: [2018-01-31 Wed 16:32]
** 2018-02-01: add 3 es nodes
  CLOSED: [2018-01-31 Wed 17:59]
   138.68.8.229  bematech-do-es-015 
159.89.153.251  bematech-do-es-016 
138.68.234.235  bematech-do-es-017 
** 2018-02-01: master-index-2d590100a80511e7a833dec7170128a9: http://bematech-do-jenkins.carol.ai:18080/job/ESReIndex/311/console
  CLOSED: [2018-02-01 Thu 20:23]
green  open   master-index-2d590100a80511e7a833dec7170128a9        10   2    4255087        36016      8.2gb          2.7gb
** 2018-02-03: (1) Update the code to apply latest carol.ai certificate: 1.81-certificate: http://injenkins.carol.ai:48080/view/Pipeline/job/PipelineActiveSprint/208/console
  CLOSED: [2018-02-03 Sat 09:19]
Kung Wang [4:36 PM]
@denny.zhang, just forwarded you the *.carol.ai certificate in your email

Denny Zhang (DevOps) [4:37 PM]
Let me check
Will apply to explore env very soon.

https://app.carol.ai:52443


https://github.com/TOTVS/mdmdevops/pull/578/files
** 2018-04-20: jenkins pipeline fail to the job
  CLOSED: [2018-04-20 Fri 08:15]
* 2016                                                             :noexport:
** 2016-03-02: 今天感冒，一直在刷知乎上关于健康的帖子。总结一点，早睡早起最重要
** 2016-03-03: 无事发生，各项工作无实质进展
** 2016-03-06: 和Shake沙克晚上聊天。了解到国内北上广IT的薪资涨得吓人
** 2016-03-07: 在zoho申请好了自己域名的邮箱: denny@dennyzhang.com
** 2016-03-09: 跑audit elasticsearch cluster; 与UU和mingli深入的1-1交谈
** 2016-03-10: 通过elasticsearch的community cookbooks成功地在一个node中起了两个es集群。人家这个代码的可配置性还是很不错的.
** 2016-03-12: 将DevOps的日常积累总结整理放到confluence上
** 2016-03-13: 与Grace去华阳慢走，闲谈
** 2016-03-15: go over the main DevOps tickets
** 2016-03-15: 离开景安项目前，和所有人过了一遍下一阶段DevOps propose的tasks
** 2016-03-18: Stabilize DevOps Roadmap
** 2016-03-24: totvslabs: enable backup server
** 2016-04-07: DOCS-155: 定义和使用一个common bash library, 以降低脚本的大量代码冗余
** 2016-04-07: DOCS-157: LongRunAllInOne在关键步骤支持hook操作
** 2016-04-08: 将TOTVSLabs的CI环境重建了一下，并烧了相应的Docker Image
** 2016-04-09: chef handler
** 2016-04-12: DOCS-130: 在国内提供公网可访问环境来host我们的wiki,jira和git
** 2016-04-15: TOTVSLabs项目的community cookbooks不再通过berks来下载
** 2016-04-19: Chef deployment with neither chef server nor kitchen berkshelf
** 2016-04-25: shellcheck: run bash code quality check
** 2016-05-02: fix all shellcheck errors in devops_scripts and devops_public repo
** 2016-05-05: switch chef-solo to chef-zero
** 2016-05-05: chef-zero and chef audit mode
** 2016-05-14: finish the audit for ssh login event
** #  --8<-------------------------- separator ------------------------>8--
** 2016-05-17: chef Apache restart issue: docker golden image has issue with Apache pid file
** 2016-05-19: rubocop for chef serverspec code
** 2016-05-20: misc change
1. Jenkins improvement: support collect files and run system jbos as non-root user.
2. Rebuild osc shadowsock machine
** 2016-05-20: shellcheck skip certain files in repo base
** 2016-05-23: totvslabs autoscaling refactoring: remove python scripts
** 2016-05-24: Test autoscaling feature in digitalocean
** 2016-05-26: TOTVSLabs after multi nodes deployment, GUI can't load
** 2016-05-30: Enable Jenkins python code check: PythonCodeQualityCheck
** 2016-05-30: DOCS-230: VerifyGUILogin job: 接受不同的GUI登录验证的请求
** 2016-05-30: DOCS-217: 在common library, 添加一个参数检查方法: check_list_fields
** 2016-05-30: DOCS-227: 实现一个community cookbook: general_security
** 2016-05-31: Support mdm prod env deployment
** #  --8<-------------------------- separator ------------------------>8--
** 2016-06-01: mdm prod env: setup iptables for security consideration
** 2016-06-01: Run remote commands on multiple servers
** 2016-06-01: firewall configuration: allow intranet request, and only accept incoming requests from port 80, 443, 22
** 2016-06-02: TOTVS: code build issue; replace ip address with repo.fluigdata.com
** 2016-06-02: RunRemoteCommand: run one command in multiple servers
** 2016-06-03: Doc: How to make a new release
** 2016-06-03: Jenkins security enhancement: identity malicious users and delete them
** #  --8<-------------------------- separator ------------------------>8--
** 2016-06-04: split bash and serverspec common library into modules
** 2016-06-04: DOCS-236: ip_ping_reachable函数: Jenkins job中的parameters, 误别出IP，并确认能ping得通
** 2016-06-05: DOCS-207: LongRunCluster: Create staging cluster environment and do continuous upgrade rehearsal
** 2016-06-05: add comment for common functions of bash and serverspec
** 2016-06-06: 巴西驾照事宜
** 2016-06-06: DOCS-141: 不同项目中，快速实现service_status_all.sh功能
** 2016-06-07: load balancer allow to url proxy by path pattern
** 2016-06-07: upgrade Elasticsearch from 2.1.1 to 2.3.3
** 2016-06-08: DOCS-238: devops_provision_os.sh: install scripts and configure linux OS for chef deployment
** 2016-06-09: longruncluster finally works!!
** 2016-06-09: enable db replica for aio, cluster deployment test
** 2016-06-10: DOCS-240: ip_ssh_reachable函数: 判断输入参数中，ssh server能被访问
** 2016-06-10: sprint-32 code build failure; check server ssh availability
** 2016-06-10: DOCS-239: PackageActionReport: list recent actions of packages installation or removal
** 2016-06-12: DOCS-235: wait_for.sh: 带timeout的wait, 等待某个条件满足
** 2016-06-12: DOCS-241: db_summary_report.sh: 报告系统的数据量
** 2016-06-12: DOCS-34: Demo: monitoring, 一键式建立起对环境的自动化监控和报警
** #  --8<-------------------------- separator ------------------------>8--
** 2016-06-13: DOCS-185: ListOSPackages命令行工具: 列出当前环境装了哪些软件及版本号
** 2016-06-14: DOCS-237: monitor the network latency in between machines
** 2016-06-14: Debug digitalocean server ssh slow issue: iptables rules
** 2016-06-15: kitchen test with or with cached docker image: use kitchen user to run hooks
** 2016-06-16: DOCS-244: chef community cookbook devops_basic: basic devops setup
** 2016-06-16: DOCS-245: MonitorNewActiveSprint: Detect whether active sprint is changed
** 2016-06-17: DOCS-243: 检测出当前机器相比OS安装后，新装了哪些软件包
** #  --8<-------------------------- separator ------------------------>8--
** 2016-07-21: https://www.dennyzhang.com/devops_books/
** 2016-07-24: https://www.dennyzhang.com/false_negative/
** #  --8<-------------------------- separator ------------------------>8--
** 2016-07-25: Sandbox audit improvement: UpdateSandboxMDM update, automate elasticsearch_audit test: http://104.131.129.100:48080/job/DockerDeploySandboxActiveSprint/188/console
  CLOSED: [2016-07-25 Mon 09:23]
** 2016-07-25: On-demand support for testing elasticsearch audit deployment in sandbox
  CLOSED: [2016-07-25 Mon 12:59]
** 2016-07-25: sandbox customize elasticsearch_audit memory setting
  CLOSED: [2016-07-25 Mon 13:52]
** 2016-07-25: wiki: Manually Start Services Without Initscripts: https://github.com/TOTVS/mdmdevops/wiki/Manually-Start-Services-Without-Initscripts
  CLOSED: [2016-07-25 Mon 15:13]
** 2016-07-26: re-install docker jenkins
  CLOSED: [2016-07-26 Tue 17:56]
** 2016-07-27: Fix incomptabile issue of kitchen-docker RubyGem, which blocks hadoop cluster CI test
  CLOSED: [2016-07-27 Wed 11:31]
** 2016-07-28: start a digitalocean VM for autoscale workstation: http://prodjenkins.fluigdata.com:18080/view/All/job/DeployAutoscaleWorkstation/5/console
  CLOSED: [2016-07-28 Thu 22:05]
** 2016-07-29: prod env support, since /var/lib/jenkins/.ssh/ci_id_rsa has been changed unexpectly, which blocks all ssh actions
  CLOSED: [2016-07-29 Fri 07:13]
]1337;RemoteHost=root@prod-app-1]1337;CurrentDir=/root]1337;ShellIntegrationVersion=2;shell=bash]133;C;]133;D;0]1337;RemoteHost=root@prod-app-1]1337;CurrentDir=/root]133;Aroot@prod-app-1:~# ]133;Bstat ~/.ssh/authorized_keys
stat ~/.ssh/authorized_keys
]133;C;  File: current.org
  Size: 4198      	Blocks: 16         IO Block: 4096   regular file
Device: fd01h/64769d	Inode: 1314032     Links: 1
Access: (0600/-rw-------)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2016-07-28 18:51:13.049051008 -0400
Modify: 2016-07-27 18:09:20.933051008 -0400
Change: 2016-07-27 18:09:20.933051008 -0400
 Birth: -

jenkins@prod-nagios-jenkins-1:~/.ssh$ stat ci_id_rsa
stat ci_id_rsa
  File: diary.org
  Size: 3244      	Blocks: 8          IO Block: 4096   regular file
Device: fd01h/64769d	Inode: 1313167     Links: 1
Access: (0400/-r--------)  Uid: (  106/ jenkins)   Gid: (  114/ jenkins)
Access: 2016-07-28 16:27:15.822118000 -0400
Modify: 2016-07-28 16:27:15.810118000 -0400
Change: 2016-07-28 16:27:15.810118000 -0400
 Birth: -
jenkins@prod-nagios-jenkins-1:~/.ssh$ date
date
Thu Jul 28 19:11:01 EDT 2016
jenkins@prod-nagios-jenkins-1:~/.ssh$
** 2016-07-29: add instruction for adding new app nodes in wiki: https://github.com/TOTVS/mdmdevops/wiki/How-To-Deploy-a-High-Availability-MDM-Cluster-Env#further-questions-how-to-add-more-app-nodes-to-existing-cluster
  CLOSED: [2016-07-29 Fri 15:17]
** 2016-07-29: [#A] couchbase cluster enables email alerts: Setting -> Alerts -> Email Alerts
  CLOSED: [2016-07-29 Fri 16:32]
http://stackoverflow.com/questions/10878052/couchbase-email-alert-setup-using-smtp-gmail-com
** 2016-07-30: #937 Deployment should support some mdm app nodes are not served as loadbalancer backend servers: https://trello.com/c/yngWNpRn
  CLOSED: [2016-07-30 Sat 10:01]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-08-01: Wiki: Deployment with Backup Redo Feature: https://github.com/TOTVS/mdmdevops/wiki/Deployment-with-Backup-Redo-Feature
  CLOSED: [2016-08-01 Mon 10:08]
** 2016-08-02: Routine support for new activesprint
  CLOSED: [2016-08-02 Tue 07:11]
** 2016-08-02: update totovslabs email setting in Chef: https://github.com/TOTVS/mdmdevops/pull/186
  CLOSED: [2016-08-02 Tue 07:13]
** 2016-08-03: bugfixing for bootstrap_mdm_sandbox.sh: docker image update issue
  CLOSED: [2016-08-03 Wed 07:46]
** 2016-08-03: DevOps doc refine and re-org
  CLOSED: [2016-08-03 Wed 11:32]
** 2016-08-04: fix mdm start timing issue, for 3 nodes deployment
  CLOSED: [2016-08-04 Thu 08:37]
** 2016-08-04: blog: https://www.dennyzhang.com/sandbox_setup/
  CLOSED: [2016-08-04 Thu 17:31]
** 2016-08-06: finish couchbase backup logic: https://github.com/TOTVS/mdmdevops/blob/sprint-38/cookbooks/backup-mdm/files/default/cb_backup.py
  CLOSED: [2016-08-06 Sat 14:21]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-08-08: haproxy health check use CURL check instead of basic port check
  CLOSED: [2016-08-08 Mon 10:00]
https://trello.com/c/PcLs0Oux

https://github.com/TOTVS/mdmdevops/commit/634f5d722a9fa092b15790271b29c6984a46cc7e

/api/v1/tenants/domain/totvslab

curl -I https://app.fluigdata.com/api/v1/admin/tenants/domain/totvslabs

curl -I https://127.0.0.1/api/v1/admin/tenants/domain/totvslabs

curl -k -I https://127.0.0.1/api/v1/admin/tenants/domain/totvslabs
** 2016-08-09: deployment improvement: don't download elasticsearch-2.3.3.deb from uncontrolled repo server: http://104.131.129.100:48080/job/DockerDeployAPPWithoutCache/24/console
  CLOSED: [2016-08-09 Tue 11:38]
** 2016-08-09: python db backup: capture and report the failure of cbbackup
  CLOSED: [2016-08-09 Tue 12:46]
** 2016-08-09: Fault-tolerance Test: If shutdown one couchbase node, mdm will fail to work: https://trello.com/c/VqKq77ky
  CLOSED: [2016-08-09 Tue 14:22]
** 2016-08-10: smartcontact elasticsearch 9200 port is widely open for anyone
  CLOSED: [2016-08-10 Wed 09:58]
** 2016-08-10: backup/redo on-demand changes
  CLOSED: [2016-08-10 Wed 10:08]
** 2016-08-12: Automated the check for insecure TCP Ports in prod env: http://prodjenkins.fluigdata.com:18080/view/Security/job/TCPScanReport/6/console
  CLOSED: [2016-08-12 Fri 11:42]
** 2016-08-12: [#A] Fire Drill For Prod Env Backup And Restore: https://github.com/TOTVS/mdmdevops/wiki/DB-Backup-In-Prod-Env#fire-drill-for-prod-env-backuprestore
  CLOSED: [2016-08-12 Fri 14:48]
13GB: couchbase system (11264 MB)

9GB: session (9216MB)

prod env: 25600 MB, cluster
prod env: 18432 GB: session


ssh root@138.68.48.50

http://138.68.48.50:8091

# prod env:
http://159.203.198.129:8091

tmux new -s denny

date
scp -i /home/denny/id_rsa -r root@138.68.4.184:/mnt/cb-backup-01/backup /home/denny/
date

mkdir -p /home/denny/
cat > /home/denny/id_rsa <<EOF
-----BEGIN RSA PRIVATE KEY-----
...
-----END RSA PRIVATE KEY-----
EOF
chmod 400 /home/denny/id_rsa

ssh -i /home/denny/id_rsa root@138.68.4.184 date
** 2016-08-12: #1023 TCP Port scan for critical VMs in public Cloud: https://trello.com/c/yuurOMr2
  CLOSED: [2016-08-12 Fri 17:00]
** 2016-08-13: [#A] update prod env firewall
  CLOSED: [2016-08-13 Sat 23:08]
iptables-save > /home/denny/20160814_rules.v4
cp /lib/ufw/user.rules  /home/denny/20160814_ufw_user_rules.v4

- Jenkins can only access port 22
- nagios open from office ip
- pallow requests from all given nodes
- allow port 9200 and 8091 from office ip
** #  --8<-------------------------- separator ------------------------>8--
** 2016-08-17: h2o-mdm: http://104.131.129.100:48080/job/DockerDeployCookbooks/14/console
  CLOSED: [2016-08-17 Wed 00:11]
git@bitbucket.org:lrpdevops/mdmdevops-totvslabs.git
h2o-mdm
sprint-38-deploy

cd /var/lib/jenkins/code/DockerDeployCookbooks/sprint-38-deploy/mdmdevops-totvslabs/cookbooks/h2o-mdm
export INSTANCE_NAME=h2o-mdm-DockerDeployCookbooks-9
export KITCHEN_YAML=.kitchen.yml

export KEEP_FAILED_INSTANCE=true
export KEEP_INSTANCE=false
export DOCKER_PORT_FORWARD_PREFIX=21
export CLEAN_START=false
export PACKAGE_URL='http://172.17.0.2:18000'
export DOCKER_ENDPOINT=172.17.0.1:4243
export TLS_FOLDER=/var/lib/jenkins/code/dockeraio/master/mdmdevops/misc/tls
export APP_BRANCH_NAME=sprint-38
export FRAMEWORK_BRANCH_NAME=sprint-38

export branch_name=sprint-38-deploy
** 2016-08-17: routine support for new active sprint
  CLOSED: [2016-08-17 Wed 09:20]
** 2016-08-17: enable ufw for all critical VMs in digitalocean
  CLOSED: [2016-08-17 Wed 10:38]
** 2016-08-17: update logic of get_previous_sprint_name for branch convention change
  CLOSED: [2016-08-17 Wed 10:52]
** 2016-08-17: wiki: How to deploy mdm and mdmbackup in different nodes: https://github.com/TOTVS/mdmdevops/wiki/Advanced-Questions-About-Deployment#question-how-to-deploy-mdm-and-mdmbackup-in-different-nodes
  CLOSED: [2016-08-17 Wed 15:58]
** 2016-08-18: [[https://trello.com/c/ROU0JNtu][#1082]] Need to upgrade nodejs to v4.5.0, for code build:
  CLOSED: [2016-08-18 Thu 14:32]
** 2016-08-19: support for backup/redo deployment and doc improvement
  CLOSED: [2016-08-19 Fri 13:02]
** 2016-08-19: support for backup/redo deployment and doc improvement
  CLOSED: [2016-08-19 Fri 13:02]
** 2016-08-19: cluster deployment guide is missing /var/lib/jenkins/.ssh folder
  CLOSED: [2016-08-19 Fri 14:50]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-08-23: backup/redo hosts file setting
  CLOSED: [2016-08-23 Tue 07:48]
git@bitbucket.org:lrpdevops/mdmdevops-totvslabs.git

1.39-redo
** 2016-08-24: mdmbackup doesn't restart, when jar file changes
  CLOSED: [2016-08-24 Wed 06:47]
** 2016-08-24: keep config file in sync
  CLOSED: [2016-08-24 Wed 11:03]
** 2016-08-24: integrate mdm healthcheck to Jenkins CI and nagios monitoring: https://github.com/TOTVS/mdmdevops/pull/211/files
  CLOSED: [2016-08-24 Wed 13:19]
** 2016-08-25: [#A] Debug mdmbackup issue: "/opt/logs/backup/staging/totvslabs" can't be found
  CLOSED: [2016-08-25 Thu 08:41]
** 2016-08-25: Support for code build improvement at frontend: http://104.131.129.100:52080/job/BuildMDMRepo/6/console
  CLOSED: [2016-08-25 Thu 12:59]
1.39-fe--use-common-node-modules
*** failure 1: Error: EPERM: operation not permitted, chmod '... mdm/node_modules/gulp/bin/gulp.js'
INFO] MDM Parent Project ................................ SUCCESS [0.580s]
[INFO] MDM Model ......................................... SUCCESS [1.619s]
[INFO] MDM Connector Model ............................... SUCCESS [2.334s]
[INFO] MDM AI Engine ..................................... SUCCESS [0.217s]
[INFO] MDM Application ................................... SUCCESS [3.200s]
[INFO] MDM Tools ......................................... SUCCESS [0.610s]
[INFO] MDM Backup Server Plugin .......................... SUCCESS [0.558s]
[INFO] MDM SmartContacts Plugin .......................... SUCCESS [4.360s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 15.151s
[INFO] Finished at: Tue Aug 23 07:25:02 UTC 2016
[INFO] Final Memory: 59M/1485M
[INFO] ------------------------------------------------------------------------
+ build_frontend_code /var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm true true
+ local code_dir=/var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm
+ local skip_test=true
+ local clean_fe_build=true
+ cd /var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm
+ npm config set loglevel info
npm info it worked if it ends with ok
npm info using npm@2.15.9
npm info using node@v4.5.0
npm info config set "loglevel" "info"
npm info ok
+ npm link gulp
npm info it worked if it ends with ok
npm info using npm@2.15.9
npm info using node@v4.5.0
npm info build /var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm/node_modules/gulp
npm info linkStuff gulp@3.9.1
npm ERR! Linux 3.13.0-79-generic
npm ERR! argv "/usr/local/bin/node" "/usr/local/bin/npm" "link" "gulp"
npm ERR! node v4.5.0
npm ERR! npm  v2.15.9
npm ERR! path /var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm/node_modules/gulp/bin/gulp.js
npm ERR! code EPERM
npm ERR! errno -1
npm ERR! syscall chmod

npm ERR! Error: EPERM: operation not permitted, chmod '/var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm/node_modules/gulp/bin/gulp.js'
npm ERR!     at Error (native)
npm ERR!  { [Error: EPERM: operation not permitted, chmod '/var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm/node_modules/gulp/bin/gulp.js']
npm ERR!   errno: -1,
npm ERR!   code: 'EPERM',
npm ERR!   syscall: 'chmod',
npm ERR!   path: '/var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm/node_modules/gulp/bin/gulp.js' }
npm ERR!
npm ERR! Please try running this command again as root/Administrator.

npm ERR! Please include the following file with any support request:
npm ERR!     /var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm/npm-debug.log
make: *** [all] Error 255
+ shell_exit
+ errorcode=2
+ echo -e '\n\nShow latest commits for TOTVS/mdm'
*** failure 2: com.mycila:license-maven-plugin:3.0.rc1:check
fs/node_modules/.bin/strip-bom
[WARNING] Unknown file extension: /var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm/node_modules/gulp/node_modules/vinyl-fs/node_modules/through2/LICENSE
[WARNING] Missing header in: /var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm/node_modules/gulp/node_modules/vinyl-fs/node_modules/glob-watcher/node_modules/gaze/node_modules/globule/test/fixtures/expand/deep/deeper/deepest/deepest.txt
[WARNING] Unknown file extension: /var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm/node_modules/gulp/completion/powershell
Unknown file extension: /var/lib/jenkins/code/1.39-fe--use-common-node-modules/mdm/node_modules/gulp/node_modules/vinyl-fs/node_modules/through2/node_modules/readable-stream/node_modules/string_decoder/LICENSE
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3.694s
[INFO] Finished at: Tue Aug 23 07:26:17 UTC 2016
[INFO] Final Memory: 17M/681M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.mycila:license-maven-plugin:3.0.rc1:check (default) on project mdm: Some files do not have the expected license header -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
make: *** [all] Error 1
+ shell_exit
+ errorcode=2
+ echo -e '\n\nShow latest commits for TOTVS/mdm'
** 2016-08-25: chef update ruby 2.1 to ruby 2.2
  CLOSED: [2016-08-25 Thu 15:59]
root@4afc3fa21c12:~/mdmdevops#  ruby --version | grep "2.2"
root@4afc3fa21c12:~/mdmdevops# ruby --version
ruby 2.1.9p490 (2016-03-30 revision 54437) [x86_64-linux-gnu]
** 2016-08-26: security improvement: remove digitalocean token from source code
  CLOSED: [2016-08-26 Fri 14:48]
** 2016-08-26: [[https://trello.com/c/PQQwsgqe][#11112]] sandbox Jenkins enable theme plugin to add company logo
  CLOSED: [2016-08-26 Fri 18:58]
http://104.131.129.100:18080

```
Bruno Volpato [11:24 AM]

btw, created a theme based on jenkins-material-theme for TOTVS Labs

[11:25]
it’s just download the Simple Theme Plugin and use the css from (preferably a copy of) http://www.brunocandido.com/jenkins-material-theme.css?as (edited)

[11:26]
you can check it here

[11:26]
http://159.203.196.76:8080/

[11:26]
not required, just that it looks way better and has the company logo :smile:

denny zhang [11:26 AM]
Cool. It adds values to our sandbox, Bruno.

Will enforce this, when I'm available.
```
** 2016-08-28: update prod jenkins jobs for new nodes of cb4 and app4
  CLOSED: [2016-08-28 Sun 12:24]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-08-29: wiki: Update ufw, when add remove Nodes For Prod Env: https://github.com/TOTVS/mdmdevops/wiki/Update-ufw,-when-add-remove-Nodes-For-Prod-Env
  CLOSED: [2016-08-29 Mon 10:56]
** 2016-08-29: [[https://trello.com/c/gDRySald][#1116]] In Prod env, automate firewall setting for adding or removing nodes
  CLOSED: [2016-08-29 Mon 11:30]
** 2016-08-29: enable firewall by default for both local and cloud deployment
  CLOSED: [2016-08-29 Mon 18:11]
** 2016-08-30: add wiki: Sandbox ChangeLog: https://github.com/TOTVS/mdmdevops/wiki/Sandbox-ChangeLog
  CLOSED: [2016-08-30 Tue 17:07]
** 2016-08-30: [#A] how to inject github deploy key for CI Jenkins code build? :IMPORTANT:
  CLOSED: [2016-08-30 Tue 22:16]
mdm_deploy_key
** 2016-08-31: routine support for a new active sprint
  CLOSED: [2016-08-31 Wed 08:01]
** 2016-08-31: add wiki: https://github.com/TOTVS/mdmdevops/wiki/MDM-Sandbox-Setup-Instruction
  CLOSED: [2016-08-31 Wed 14:19]
** 2016-09-01: Removed 2 app nodes out of LB backend servers: http://45.55.6.34:18080/job/DeploySystem/77/parameters
  CLOSED: [2016-09-01 Thu 12:34]
ok

Bruno Volpato [12:08 PM]
with prodapp1 it’s working fine?

[12:08]
@denny.zhang can we manually remove the others from the LB?

[12:08]
since we know what to expect from the other 2 now

denny zhang [12:08 PM]
remove app02 and app03 out of LB?

Bruno Volpato [12:08 PM]
yes

denny zhang [12:09 PM]
Will do today.
** 2016-09-02: upgrade docker daemon in public Jenkins machine; enable tcp port scan for this machine
  CLOSED: [2016-09-02 Fri 09:59]
docker --version

docker exec -it docker-jenkins service jenkins stop
docker exec -it docker-jenkins service apache2 stop
docker stop docker-jenkins

service docker stop

curl -sSL https://get.docker.com/ | sudo sh

docker --version

docker start docker-jenkins
docker exec -it docker-jenkins service jenkins start
docker exec -it docker-jenkins service apache2 start

iptables -L -n | grep 18000
telnet 104.236.159.226 18080
** 2016-09-02: fix potential security issue: update /root/.ssh/authorized_keys in prod env: http://45.55.6.34:18080/job/RunCommandOnServers/94/parameters/
  CLOSED: [2016-09-02 Fri 14:25]
grep -v ssh.login@totvs.com /root/.ssh/authorized_keys > /tmp/authorized_keys
cp /tmp/authorized_keys /root/.ssh/authorized_keys
chmod 400 /root/.ssh/authorized_keys
** 2016-09-02: build push latest, and release the change
  CLOSED: [2016-09-02 Fri 17:20]
http://67.205.137.11:18080

docker tag totvslabs/mdm:v1.1 totvslabs/mdm:latest
docker push totvslabs/mdm:v1.1
docker push totvslabs/mdm:latest

iptables -L -n | grep 18000
telnet 67.205.137.11 18000

#+BEGIN_EXAMPLE
root@mdm-jenkins-ci:/tmp# docker history totvslabs/mdm:latest
IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT
0bd22b38a691        About an hour ago   /bin/sh -c #(nop) CMD ["/bin/bash"]             0 B
bff75fa79ee4        About an hour ago   |5 chef_version=12.13.37 devops_branch=master   1.839 GB
983428b19bef        2 hours ago         /bin/sh -c #(nop) ADD file:8f84bab631a1ef2760   3.244 kB
35e1e85359f0        2 hours ago         /bin/sh -c #(nop) ARG devops_branch=master      0 B
cf1c5ee78669        2 hours ago         /bin/sh -c #(nop) ARG chef_version=12.13.37     0 B
8680d3f533e6        2 hours ago         /bin/sh -c #(nop) ARG devops_repo_name=mdmdev   0 B
5ca63bdfec4d        2 hours ago         /bin/sh -c #(nop) ARG devops_git_repo=git@git   0 B
6cbd019f265f        2 hours ago         /bin/sh -c #(nop) ARG working_dir=/root/        0 B
077cfaf8a386        2 hours ago         /bin/sh -c #(nop) MAINTAINER TOTVS Labs <denn   0 B
679cb6087606        2 hours ago         /bin/sh -c #(nop) CMD ["/bin/bash"]             0 B
7cf0bcff0c2e        2 hours ago         |2 jenkins_port=18080 jenkins_version=2.19 /b   336.3 MB
696a58778c3c        3 days ago          /bin/sh -c #(nop) ADD file:e2749f64ed057f3e50   68 B
b794ecc6992a        7 days ago          /bin/sh -c #(nop) ARG jenkins_version=2.19      0 B
b54d611111e9        7 days ago          /bin/sh -c #(nop) ARG jenkins_port=18080        0 B
3370f42bc0c7        7 days ago          /bin/sh -c #(nop) MAINTAINER TOTVS Labs <denn   0 B
ff6011336327        3 weeks ago         /bin/sh -c #(nop) CMD ["/bin/bash"]             0 B
<missing>           3 weeks ago         /bin/sh -c sed -i 's/^#\s*\(deb.*universe\)$/   1.895 kB
<missing>           3 weeks ago         /bin/sh -c rm -rf /var/lib/apt/lists/*          0 B
<missing>           3 weeks ago         /bin/sh -c set -xe   && echo '#!/bin/sh' > /u   194.6 kB
<missing>           3 weeks ago         /bin/sh -c #(nop) ADD file:4f5a660d3f5141588d   187.8 MB
#+END_EXAMPLE
** 2016-09-04: Coordinate and test for 1.40 code build failure
  CLOSED: [2016-09-04 Sun 09:16]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-09-06: add wiki: How To Restart Elasticsearch Cluster: https://github.com/TOTVS/mdmdevops/wiki/Prod-Env-Maintenance-Guide#how-to-restart-elasticsearch-cluster
  CLOSED: [2016-09-06 Tue 09:27]
** 2016-09-06: support for performance issue in prod env: app memory for GC; elasticsearch cluster restart issue; couchbase value eviction
  CLOSED: [2016-09-06 Tue 12:58]
** 2016-09-07: prod jenkins enable plugins: jobConfigHistory(audit jenkins change), thinBackup(backup)
  CLOSED: [2016-09-07 Wed 09:10]
** 2016-09-07: Add customized dashboard to Prod Jenkins
  CLOSED: [2016-09-07 Wed 11:18]
** 2016-09-07: Enable fingerprint for code build in prod jenkins
  CLOSED: [2016-09-07 Wed 14:58]
** 2016-09-07: manually enable logging elasticsearch slow query: http://45.55.6.34:18080/job/RunCommandOnServers/100/console
  CLOSED: [2016-09-07 Wed 18:40]
cd /var/lib/jenkins/code/DockerDeployAllInOne/1.40-deploy/mdmdevops-totvslabs/cookbooks/all-in-one
export INSTANCE_NAME=all-in-one-DockerDeployAllInOne-137
export KITCHEN_YAML=.kitchen.yml

export branch_name=1.40-deploy
export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true
export CLEAN_START=false
export ENABLE_MORE_TEST=false
export PACKAGE_URL='http://172.17.0.2:18000'
export APP_BRANCH_NAME=1.40
export FRAMEWORK_BRANCH_NAME=1.40
** 2016-09-07: [[https://trello.com/c/mTYau0GF][#1139]] Monitor elasticsearch slow query: http://104.131.129.100:48080/job/DockerDeployAllInOne/139/console
  CLOSED: [2016-09-07 Wed 21:46]
git@bitbucket.org:lrpdevops/mdmdevops-totvslabs.git

https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-slowlog.html

cd /var/lib/jenkins/code/DockerDeployAllInOne/1.40-deploy/mdmdevops-totvslabs/cookbooks/all-in-one
export INSTANCE_NAME=all-in-one-DockerDeployAllInOne-130
export KITCHEN_YAML=.kitchen.yml

export branch_name=1.40-deploy

export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true
export CLEAN_START=false
export ENABLE_MORE_TEST=false
export PACKAGE_URL='http://172.17.0.2:18000'
export APP_BRANCH_NAME=1.40
export FRAMEWORK_BRANCH_NAME=1.40

curl http://172.17.0.3:9200/_search?q=150 | jq '.took'
** 2016-09-08: Add wiki: Customized Docker Images: https://github.com/TOTVS/mdmdevops/wiki/Customized-Docker-Images
  CLOSED: [2016-09-08 Thu 10:09]
** 2016-09-09: Add Date and Time to Bash History: http://104.131.129.100:48080/job/DockerDeployAllInOne/146/console
  CLOSED: [2016-09-09 Fri 16:39]
** 2016-09-09: [[https://trello.com/c/oBIe7H1J][#1136]] jenkins node generates tons of /tmp/npm-* folders: http://104.131.129.100:48080/job/DockerDeploySandboxActiveSprint/178/console
  CLOSED: [2016-09-09 Fri 17:37]
1.40-build-1141-npm-directories
Found some related links by google.

No clear clue why it's happening, looks like a bug of npm. So we may add below command at the end of code build:
find /tmp/ -name 'npm-*' -type d -mmin +10 -exec rm -rf {} +

https://github.com/npm/npm/issues/6761
https://github.com/npm/npm/issues/6855
https://nemisj.com/tmp-npm-xxx/

#+BEGIN_EXAMPLE
]0;root@jenkins: /tmproot@jenkins:/tmp# ls -lth
ls -lth
total 28M
-rw-r--r--    1 jenkins jenkins 2.1K Aug 29 23:10 client.json
-rw-r--r--    1 jenkins jenkins  136 Aug 29 23:10 client.rb
drwxr-xr-x    2 jenkins jenkins 4.0K Aug 29 22:57 hsperfdata_jenkins
drwxr-xr-x 1113 jenkins jenkins  44K Aug 29 22:55 npm-1870-85940bba
drwxr-xr-x 1119 jenkins jenkins  40K Aug 29 22:53 npm-1538-dd14059a
drwxr-xr-x 1115 jenkins jenkins  44K Aug 29 20:25 npm-32683-bb3291a8
drwxr-xr-x 1121 jenkins jenkins  52K Aug 29 20:21 npm-32352-bbdb35d6
drwxr-xr-x    2 root    root    4.0K Aug 29 19:12 hsperfdata_root
drwxr-xr-x 1116 jenkins jenkins  48K Aug 29 16:43 npm-29703-daca1880
drwxr-xr-x 1121 jenkins jenkins  40K Aug 29 16:40 npm-29372-e234e66c
drwxr-xr-x    3 jenkins jenkins 4.0K Aug 19 16:58 npm-23448-0c3b8676
drwxr-xr-x    3 jenkins jenkins 4.0K Aug 19 16:55 npm-23112-1eec1071
drwxr-xr-x 1115 jenkins jenkins  44K Aug 18 23:50 npm-20171-0da76cd1
drwxr-xr-x 1115 jenkins jenkins  40K Aug 18 23:47 npm-20066-691c1c12
drwxr-xr-x 1119 jenkins jenkins  48K Aug 18 23:45 npm-19844-005549d8
drwxr-xr-x 1117 jenkins jenkins  48K Aug 18 23:43 npm-19639-3792baca
drwxr-xr-x 1114 jenkins jenkins  48K Aug 18 22:41 npm-15557-7f4c1fee
...
...
drwxr-xr-x    3 jenkins jenkins 4.0K Aug  3 00:06 npm-4116-61a29e83
drwxr-xr-x    3 jenkins jenkins 4.0K Aug  3 00:03 npm-3771-1220d478
drwxr-xr-x    3 jenkins jenkins 4.0K Aug  3 00:02 jenkins
drwxr-xr-x    3 jenkins jenkins 4.0K Aug  3 00:00 npm-3460-c0179b6b
drwxr-xr-x    2 jenkins jenkins 4.0K Aug  2 23:48 jna--1712433994
drwxr-xr-x    2 jenkins jenkins 4.0K Aug  2 23:48 jetty-0.0.0.0-18080-war--any-
-rw-r--r--    1 jenkins jenkins 1.7M Aug  2 23:48 winstone3384093491859314076.jar
]0;root@jenkins: /tmproot@jenkins:/tmp# ls -lt /tmp/npm-1870-85940bba
ls -lt npm-1870-85940bba
total 4492
drwxr-xr-x   4 jenkins jenkins  4096 Aug 29 22:55 unpack-db6164c36d31
drwxr-xr-x 515 jenkins jenkins 20480 Aug 29 22:55 registry.npmjs.org
drwxr-xr-x   2 jenkins jenkins  4096 Aug 29 22:55 unpack-74da47ad0e6e
drwxr-xr-x   2 jenkins jenkins  4096 Aug 29 22:55 unpack-03d19a11cddb
drwxr-xr-x   2 jenkins jenkins  4096 Aug 29 22:55 unpack-2e941f10f222
drwxr-xr-x   2 jenkins jenkins  4096 Aug 29 22:55 unpack-1e28e085ba9e
...
...
]0;root@jenkins: /tmproot@jenkins:/tmp# ls -lth /tmp/npm-1870-85940bba/unpack-db6164c36d31
ls -lth /tmp/npm-1870-85940bba/unpack-db6164c36d31
total 56K
drwxr-xr-x 3 jenkins jenkins 4.0K Aug 29 22:55 test
-rw-r--r-- 1 jenkins jenkins  950 Oct  2  2015 package.json
-rw-r--r-- 1 jenkins jenkins 1.2K Oct  2  2015 CHANGES
-rw-r--r-- 1 jenkins jenkins 1.1K Jun 25  2015 LICENSE
-rw-r--r-- 1 jenkins jenkins 4.2K Jun 25  2015 README.md
-rw-r--r-- 1 jenkins jenkins 1.1K Jun 25  2015 array.js
-rw-r--r-- 1 jenkins jenkins 1.3K Jun 25  2015 for-of.js
-rw-r--r-- 1 jenkins jenkins  581 Jun 25  2015 get.js
-rw-r--r-- 1 jenkins jenkins 2.7K Jun 25  2015 index.js
-rw-r--r-- 1 jenkins jenkins  452 Jun 25  2015 is-iterable.js
-rw-r--r-- 1 jenkins jenkins 1.2K Jun 25  2015 string.js
-rw-r--r-- 1 jenkins jenkins  187 Jun 25  2015 valid-iterable.js
drwxr-xr-x 2 jenkins jenkins 4.0K Jun 25  2015 #
#+END_EXAMPLE
** 2016-09-10: Improve docker image security: remove jar and ks
  CLOSED: [2016-09-10 Sat 07:06]
** 2016-09-10: jenkins enable Naginator Plugin: add an automatic retry http://104.131.129.100:48080/job/DockerDeploySandboxActiveSprint/190/console
  CLOSED: [2016-09-10 Sat 09:31]
/var/log/jenkins.log

jenkins@jenkins:~/plugins$ ls -lth
total 4.2M
-rw-r--r-- 1 jenkins jenkins  42K Sep  9 06:42 naginator.jpi
-rw-r--r-- 1 jenkins jenkins 221K Sep  9 06:42 matrix-project.jpi
-rw-r--r-- 1 jenkins jenkins 161K Sep  9 06:42 script-security.jpi
-rw-r--r-- 1 jenkins jenkins 323K Sep  9 06:42 junit.jpi
-rw-r--r-- 1 jenkins jenkins  44K Sep  9 06:42 structs.jpi
-rw-r--r-- 1 jenkins jenkins 3.3M Sep  9 06:42 bouncycastle-api.jpi
drwxr-xr-x 4 jenkins jenkins 4.0K Aug 29 16:38 icon-shim
drwxr-xr-x 5 jenkins jenkins 4.0K Aug 29 16:38 matrix-auth
drwxr-xr-x 4 jenkins jenkins 4.0K Aug 29 16:38 simple-theme-plugin
-rw-r--r-- 1 jenkins jenkins 6.1K Aug 29 16:38 simple-theme-plugin.jpi
-rw-r--r-- 1 jenkins jenkins  84K Aug 29 16:38 matrix-auth.jpi
-rw-r--r-- 1 jenkins jenkins  25K Aug 29 16:38 icon-shim.jpi
** #  --8<-------------------------- separator ------------------------>8--
** 2016-09-12: add wiki: Jenkins Admin User: https://github.com/TOTVS/mdmdevops/wiki/Sandbox-FAQ#jenkins-admin-user
  CLOSED: [2016-09-12 Mon 12:02]
** 2016-09-12: remove all sensitive files in other docker images used for FD project
  CLOSED: [2016-09-12 Mon 17:15]
totvslabs/mdm_jenkins
totvslabs/autoscale-cluster
# totvslabs/hadoop
** 2016-09-12: Routine support: code build failure; keep config file in sync
  CLOSED: [2016-09-12 Mon 23:01]
** 2016-09-13: Create Dockerfile for building autoscale image: http://104.131.129.100:48080/job/AutoscaleDockerCBClusterTest/20/console
  CLOSED: [2016-09-13 Tue 10:14]
docker stop my-test; docker rm my-test
docker run -t -i --privileged -h mytest --name my-test    totvslabs/autoscale-cluster:latest  /bin/bash

docker tag totvslabs/autoscale-cluster:v1.0  totvslabs/autoscale-cluster:latest
docker push  totvslabs/autoscale-cluster:v1.0
docker push totvslabs/autoscale-cluster:latest

2016-09-12 13:42:14        Command "sleep 300"
2016-09-12 13:42:14          exit_status
2016-09-12 13:47:14        Enter passphrase for key '/root/.ssh/id_rsa': [32m    should eq 0[0m
2016-09-12 13:47:14
2016-09-12 13:47:14        Command "/opt/mdm/bin/check_couchbase_capacity.sh"
2016-09-12 13:47:14          exit_status
2016-09-12 13:47:14        [32m    should not eq 1[0m
2016-09-12 13:47:14
2016-09-12 13:47:14        Command "sleep 300"
2016-09-12 13:47:14          exit_status
2016-09-12 13:52:14        Enter passphrase for key '/root/.ssh/id_rsa': [32m    should eq 0[0m
2016-09-12 13:52:14
** 2016-09-13: Sandbox Support For Sandro from Brazil
  CLOSED: [2016-09-13 Tue 21:17]
** 2016-09-14: routine support for new active sprint
  CLOSED: [2016-09-14 Wed 06:43]
** 2016-09-14: wilson sandbox support
  CLOSED: [2016-09-14 Wed 08:21]
** 2016-09-14: plot data for couchbase mdm-staging: indicator for the data visualization
  CLOSED: [2016-09-14 Wed 17:10]
http://repo.fluigdata.com:18000/prodenv_db_summary_report/db_summary_report.txt

Bruno Volpato [10:02 AM]
you're not tracking staging @denny.zhang ?

[10:02]
probably it's a move between staging and master, and it will be usual in our platform

denny zhang [10:04 AM]
Compared to others, record count of mdm-staging is very very huge. So I opt it out. (edited)

[10:06]
Any tips to make the data report more reasonable and meaningful?

Bruno Volpato [12:25 PM]
it’s huge, but still an indicator that we need to take care of

[12:26]
ideally the master will be bigger than staging, as soon as data flow to the golden record

[12:26]
staging is just temporary storage, master is the real thing.

denny zhang [12:31 PM]
I see. Then let me add a dedicated diagram for it.

So it won't mess up other metrics.
** 2016-09-15: add conditional switch for old and new jenkins update
  CLOSED: [2016-09-15 Thu 09:32]
** 2016-09-16: deep dive rrd
  CLOSED: [2016-09-16 Fri 17:42]
** 2016-09-16: For digitalocean sandbox deployment, avoid unsafe port forwarding for docker
  CLOSED: [2016-09-16 Fri 20:44]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-09-19: manually fix file mode of rsyslog for prod env
  CLOSED: [2016-09-19 Mon 17:57]
** 2016-09-20: [[https://trello.com/c/By2PGApP][#1134]] mdm-initscritp.log is empty, but \*.log.1 grows huge in prod env
  CLOSED: [2016-09-20 Tue 08:18]
138.68.61.54
*** TODO [#A] prod env logrotate: mdm-initscritp.log is empty, but *.log.1 is huge
/usr/sbin/logrotate -v /etc/logrotate.conf

root@prod-app-3:/etc/logrotate.d# ls -lth /opt/mdm/logs/mdm-initscript.log*
-rw-r--r-- 1 root adm  44G Aug 30 18:53 /opt/mdm/logs/mdm-initscript.log.1
-rw-r--r-- 1 root adm    0 Aug 30 18:42 /opt/mdm/logs/mdm-initscript.log.4
-rw-r--r-- 1 root adm    0 Aug 30 18:42 /opt/mdm/logs/mdm-initscript.log.3
-rw-r--r-- 1 root adm    0 Aug 27 06:54 /opt/mdm/logs/mdm-initscript.log
-rw-r--r-- 1 root adm 103M Aug 26 13:38 /opt/mdm/logs/mdm-initscript.log.2
-rw-r--r-- 1 root adm 561M Aug 18 19:59 /opt/mdm/logs/mdm-initscript.log.5
-rw-r--r-- 1 root adm 391M Aug 17 14:48 /opt/mdm/logs/mdm-initscript.log.6
-rw-r--r-- 1 root adm 3.4G Aug 15 17:00 /opt/mdm/logs/mdm-initscript.log.7

Nagios BOT [4:20 AM]
159.203.198.98/check_disk_rootfs is WARNING:
DISK WARNING used :  / 14.91% free

denny zhang [6:44 AM]
app03 is running to low disk. A lot of disk capacity has been taken by logfiles.

Confirmed logrotate daemon is not running. Let me manually enforce a logrotate, then looking into this root cause.


Nagios BOT [6:45 AM]
159.203.198.98/check_disk_rootfs is OK

Bruno Volpato [7:25 AM]
@denny.zhang any idea on why sometimes the latest logs are being appended to mdm-initscript.log.1, for example?

[7:25]
I expect the latest always to be mdm-initscript.log

denny zhang [7:27 AM]
Yes, that's a strange thing.

mdm-initscript.log is empty, thus logrorate doesn't work. But mdm-initscript.log.1 keeps growing.

Not sure about the root cause yet. I suspect we run something like "mv mdm-initscript.log mdm-initscript.log.1" somehow. Need more evidence.
*** TODO make sure logrotate works properly
*** TODO digitalocean logrotate
service 'rsyslog' do
  supports status: true
  start_command '/etc/init.d/rsyslog start'
  stop_command '/etc/init.d/rsyslog stop'
  status_command '/etc/init.d/rsyslog status'
  action [:enable, :start]
  # TODO: disable rsylog start for digitalocean
  not_if "cat /proc/1/cgroup | tail -n 1 | grep '/$'"
end
*** TODO [#A] fail to start rsyslog
#+BEGIN_EXAMPLE
root@denny-01:~# /etc/init.d/rsyslog start
/etc/init.d/rsyslog start
root@denny-01:~# echo $?
echo $?
1
#+END_EXAMPLE
** 2016-09-21: [[https://github.com/TOTVS/mdmdevops/wiki/SonarQube-Report-To-Analysis-Java-Code][Add wiki]]: SonarQube Report To Analysis Java Code
  CLOSED: [2016-09-21 Wed 18:02]
** 2016-09-21: [[https://github.com/TOTVS/mdmdevops/wiki/VPS-Comparison:-Linode-VS-DigitalOcean][Add wiki]]: VPS Comparison: Linode VS DigitalOcean
  CLOSED: [2016-09-21 Wed 22:12]
https://vincentcox.com/linode-vs-digitalocean-vs-shared-hosting/
http://wpvsblogger.com/linode-vs-digitalocean/
https://joshtronic.com/2016/02/07/ten-dollar-showdown-linode-kvm-versus-digitalocean/
http://www.itekhost.net/digitalocean-vs-linode/

- Twice larger for the memory. We're memory intensive application for ES, CB and mdm application
- GUI is ugly
- Linode gives more bandwidth, but DigitalOcean doesn’t charge for overage while Linode does, which is a big plus.
- Upon testing two sites from the same place, one using DigitalOcean and one using Linode, I found that DO is faster.
- Linode suffered from a DDOS for several days.

Reddit discussion:
https://www.reddit.com/r/Cloud/comments/52vplp/whether_i_should_migrate_from_digitalocean_to/
#+BEGIN_EXAMPLE
Thank you for considering Linode for your future hosting!

>1 - More maintenance times

Maintenance periods can be frustrating but we do our best to perform them during a time that is not normal business hours. We also have way's to prevent you from experiencing major downtime from our normal scheduled maintenance like our cloning your Linode to a new Linode - swaping your IP between the to Linode's. That way your only downtime is between the IP swap which is about that same amount of time as a simple reboot.

The network maintenance we are currently working on in our Atlanta datacenter is to upgrading the entire network to better protect ourselves from DDoS attacks. Here is more information on what they are planning on doing:

https://status.linode.com/incidents/c1nbps0rq9q3

>2 - Frequent DDoS attacks, sometimes happening for days.

This is something as stated above that we have been actively working on. We have bulked our network up so we can handle large DDos attacks and get them mitigated so it prevents any connectivity issues. Here is our blog post that talks more specific details on what we are implementing:

https://blog.linode.com/2016/01/29/christmas-ddos-retrospective/

The maintenance is something that we will never be able to prevent since a lot of time's they are for security and hardware health reasons. We do offer many way's to prevent the maintenance from causing an extensive amount of downtime like I mentioned above. We do hope to have less frequent maintenance of course, but we always try to make sure everything is in great working order and everything is secure.

I hope that clears it up! If you have any other questions, please feel free to reach out to us at any time.

Kind Regards,
Holden M.
Linode Support
#+END_EXAMPLE
** 2016-09-22: branch merge: 1.40/1.41 to master; and run full cycle deployment tests
  CLOSED: [2016-09-22 Thu 11:45]
** 2016-09-22: [[https://trello.com/c/SlQkNuBL][#1192]] Update Sonar to run check in both repos and opt out certain rules
  CLOSED: [2016-09-22 Thu 16:05]
** 2016-09-23: Support Roboson to host [[https://github.com/TOTVS/mdmdevops/wiki/MDM-Server-List#docfluigdatacom][document website]]
  CLOSED: [2016-09-23 Fri 09:29]
** 2016-09-23: java opts improvement for mdm and mdmbackup initscript
  CLOSED: [2016-09-23 Fri 11:43]
** 2016-09-23: [[https://github.com/TOTVS/mdmdevops/wiki/Metric:-Drive-Constant-And-Positive-Changes][Add wiki]]: Metric: Drive Constant And Positive Changes
  CLOSED: [2016-09-23 Fri 17:04]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-09-26: [[https://trello.com/c/K69DtIfm][#1195]] Code build performance improvement
  CLOSED: [2016-09-26 Mon 16:30]
** 2016-09-27: bug fixing for java files not updated in app02 and app03
  CLOSED: [2016-09-27 Tue 15:51]
** 2016-09-28: Support Robson to setup and maintain http://prod-doc.fluigdata.com
  CLOSED: [2016-09-28 Wed 11:29]
** 2016-09-30: disable logrotate for java gc
  CLOSED: [2016-09-30 Fri 06:54]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-10-04: [#A] dockerfile to build metric project                    :IMPORTANT:
  CLOSED: [2016-10-04 Tue 10:19]
docker run -t -P -d  totvslabs/java:v1.0 /bin/bash

http://injenkins.fluigdata.com:5601/

http://injenkins.fluigdata.com:5601/app/kibana

docker rmi denny/datareport:v1.1

docker stop my-test; docker rm my-test
docker run -t -d --privileged -p 5601:5601 -h mytest --name my-test totvslabs/datareport:v1.1 /usr/bin/supervisord -n -c /etc/supervisor/supervisord.conf

docker exec -it my-test bash

curl http://localhost:9200/_cat/indices?v

# inject data
export DATA_SOURCE_URL="http://repo.fluigdata.com:18000/prodenv_db_summary_report/db_summary_report.txt"
wget -O /tmp/db_summary_report.txt $DATA_SOURCE_URL
cat /tmp/db_summary_report.txt > /var/log/data_report.log

tail -f /var/log/logstash.log
*** misc command
# Create db schema for kibana
curl http://localhost:9200/_cat/indices?v

curl http://localhost:9200/.kibana/ | python -m json.tool

# configure default index
curl -XPUT http://localhost:9200/.kibana/index-pattern/logstash-* -d '{"title" : "logstash-*",  "timeFieldName": "@timestamp"}'
curl -XPUT http://localhost:9200/.kibana/config/4.6.1 -d '{"defaultIndex" : "logstash-*"}'

# inject data
export DATA_SOURCE_URL="http://repo.fluigdata.com:18000/prodenv_db_summary_report/db_summary_report.txt"
wget -O /tmp/db_summary_report.txt $DATA_SOURCE_URL
cat /tmp/db_summary_report.txt > /var/log/data_report.log

tail -f /var/log/logstash.log
** 2016-10-04: collect HAProxy metrics
  CLOSED: [2016-10-04 Tue 21:50]
** 2016-10-05: [[https://github.com/TOTVS/mdmdevops/wiki/Metrics-Item-List-We-Collect][Add wiki]]: Metrics Item List We Collect
  CLOSED: [2016-10-05 Wed 09:17]
** 2016-10-06: Migrate http://www.totvslabs.com from webhostingpad to DigitalOcean
  CLOSED: [2016-10-06 Thu 08:48]

old ip: 69.65.10.209

ssh root@104.236.159.226

cd /var/www/
git clone git@github.com:TOTVS/site.git
mv /var/www/site  /var/www/totvslabs_www
vim /etc/apache2/ports.conf

> /etc/apache2/sites-enabled/totvslabs_www.conf && vim  /etc/apache2/sites-enabled/totvslabs_www.conf

lsof -i tcp:8090

service apache2 stop
service apache2 start

curl http://127.0.0.1:8090

curl http://www.fluigdata.com
curl http://doc.fluigdata.com
curl http://docs.fluigdata.com
** 2016-10-06: Enhance kibana authentication: password protection
  CLOSED: [2016-10-06 Thu 11:55]
** 2016-10-06: [[https://github.com/TOTVS/mdmdevops/wiki/MDM-Server-List][Update wiki]]: list all services used by fluigdata project
  CLOSED: [2016-10-06 Thu 13:07]
** 2016-10-06: meature haproxy metrics: https visits, time performance, concurrent sessions
  CLOSED: [2016-10-06 Thu 17:16]
** 2016-10-07: [[https://trello.com/c/PSjqmnzO][#1236]] Change mdm app in sandbox to listen on 80/443 by default
  CLOSED: [2016-10-07 Fri 12:01]
** 2016-10-07: devops: Enable justniffer in wilson's QA env, to detect insecure https requests
  CLOSED: [2016-10-07 Fri 18:14]
http://totvslabs.fluigdata.com:3000/?email=admin%40totvslabs.com&password=Foobar1%21#/explore

/usr/bin/justniffer -i lo -l "%request.header - %request.timestamp(%T %D) - %request.header.host - %response.code"

curl "http://localhost/?email=test%40test.com&password=ChangeMe123"

curl -XPOST "http://localhost" -H "content-type: application/json" -d "{ \"email\": \"test@test.com\", \"password\": \"ChangeMe123\" }"
*** TODO enable in wilson env
ssh root@138.68.14.146

nohup /usr/bin/justniffer -i docker0  -l "%request.header - %request.timestamp(%T %D) - %request.header.host - %response.code" >> /root/test.log &

tail -f /root/test.log

for((i=0; i< 10; i++)); do { curl http://172.17.0.1:8443 ;}; done

curl http://172.17.0.1:8443
*** background
Do you mind if I install a http sniffer service in your sandbox? The impact should be minimal. I will only run it for a week.
- Won't impact existing application communication
- Resource overhead is low.

Here is the context.

I noticed we have some insecure http requests(shown at the bottom). Not sure whether we have more.
Thus I want to sniffer and dump all http headers in an actively used env. Then examine whether we have more issues or security holes.

Matt Stow [8:45 AM]
I’m not sure how to reproduce, but occasionally the user’s email and password is in plain text in the url, like so:
`http://totvslabs.fluigdata.com:3000/?email=admin%40totvslabs.com&password=Foobar1%21#/explore`
** 2016-10-08: Update logback.yml to avoid log duplication
  CLOSED: [2016-10-08 Sat 07:00]
** 2016-10-08: Update vagrantfile to dump log file to both console and logfile
  CLOSED: [2016-10-08 Sat 07:30]
** 2016-10-08: Issue support for Kung's sandbox setup
  CLOSED: [2016-10-08 Sat 08:58]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-10-10: Enforce scheduled monitor for wilson kibana and data-report kibana
  CLOSED: [2016-10-10 Mon 08:54]
** 2016-10-10: [[https://github.com/TOTVS/mdmdevops/wiki/MDM-Server-List#1071702142415601][Add wiki]]: Dump the kibana maintenance information to wiki
  CLOSED: [2016-10-10 Mon 09:14]
** 2016-10-10: [[https://github.com/TOTVS/mdmdevops/wiki/Infra-Metrics-List-For-Cost-Evaluation][Add wiki]] Metrics From Infra for cost measurement
  CLOSED: [2016-10-10 Mon 09:46]
** 2016-10-10: fix chef warnings: method access to node attributes (node.foo.bar) is deprecated
  CLOSED: [2016-10-10 Mon 17:25]
#+BEGIN_EXAMPLE

just a warn that I noticed

[2:27]
 ```[2016-10-10T06:21:46+00:00] INFO: Processing template[/opt/mdm/config/mdm.yml] action create (app-mdm::default line 65)[2016-10-10T06:21:46+00:00] WARN: method access to node attributes (node.foo.bar) is deprecated and will be removed in Chef 13, please use bracket syntax (node["foo"]["bar"])
[2016-10-10T06:21:46+00:00] WARN: method access to node attributes (node.foo.bar) is deprecated and will be removed in Chef 13, please use bracket syntax (node["foo"]["bar"])
[2016-10-10T06:21:46+00:00] WARN: method access to node attributes (node.foo.bar) is deprecated and will be removed in Chef 13, please use bracket syntax (node["foo"]["bar"])
[2016-10-10T06:21:46+00:00] WARN: method access to node attributes (node.foo.bar) is deprecated and will be removed in Chef 13, please use bracket syntax (node["foo"]["bar"])
```

denny zhang [2:30 PM]
Thanks. Yes, I know.

We're using fixed version of chef. It won't incur problems to us.

You just remind me to put some effort to remove the warnings.
#+END_EXAMPLE
** 2016-10-11: New active sprint: routine update and intergration test
  CLOSED: [2016-10-11 Tue 07:50]
** 2016-10-12: [[https://trello.com/c/K84uXnRY][#1240]] In Sandbox, UpdateJenkinsItself doesn't work with git deploy protected by passphrase
  CLOSED: [2016-10-12 Wed 12:48]
** 2016-10-12: Robson support for kibana connectivitiy issue
  CLOSED: [2016-10-12 Wed 14:03]
** 2016-10-12: mockup for infrastruture: machine nodes; invoice
  CLOSED: [2016-10-12 Wed 17:42]
http://repo.fluigdata.com:18000/system_metric/infra_report.txt

property_name:OverallPercentage && item_name:HardwareUtilization

export DATA_SOURCE_URL="http://repo.fluigdata.com:18000/system_metric/infra_report.txt"
wget -O /tmp/db_summary_report.txt $DATA_SOURCE_URL
cat /tmp/db_summary_report.txt >> /var/log/data_report.log

tail -f /var/log/logstash.log

# update data
ssh  root@104.236.159.226
tmux attach -t denny
docker exec -it docker-jenkins bash
cd /var/www/repo/system_metric
> infra_report.txt && vim infra_report.txt
** 2016-10-12: setup new kibana server
  CLOSED: [2016-10-12 Wed 17:43]
** 2016-10-12: replicate kibana setting and mockup data to demo system
  CLOSED: [2016-10-12 Wed 18:12]
** 2016-10-12: [[https://github.com/TOTVS/mdmdevops/wiki/Infra-Metrics-List-For-Cost-Evaluation][Add Wiki]]: Infra Metrics List For Cost Evaluation
  CLOSED: [2016-10-12 Wed 19:02]
** 2016-10-13: decommission prod-autoscale out of prod env, and update all jenkins jobs: http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/110/console
  CLOSED: [2016-10-13 Thu 10:14]
** 2016-10-13: get the number more real
  CLOSED: [2016-10-13 Thu 10:42]
** 2016-10-13: Branch merge(1.44 to master) and Routine CI tests
  CLOSED: [2016-10-13 Thu 11:26]
** 2016-10-13: [#A] create your own document_id in logstash
  CLOSED: [2016-10-13 Thu 12:49]
https://discuss.elastic.co/t/how-to-create-my-own-document-id-in-logstash/1416/2

/opt/logstash/bin/logstash -f /opt/logstash/data_report.conf

cat /tmp/db_summary_report.txt >> /var/log/data_report.log

tail -f /var/log/logstash.log

> /opt/logstash/data_report.conf && vim /opt/logstash/data_report.conf

input {
  file {
    path => "/var/log/data_report.log"
    start_position => beginning
  }
}

filter {
    if [path] =~ "data_report" {
        grok {
            match => {"message" => "\[%{HTTPDATE:log_timestamp}\] %{NOTSPACE:item_name} %{NOTSPACE:property_name} %{NOTSPACE:property_value:float}"
            }
       }
       date {
          match => [ "log_timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
       }
       ruby {
          code => "require 'digest/md5';
          event['@metadata']['computed_id'] = Digest::MD5.hexdigest(event['message'])"
        }
   }
}

output {
  elasticsearch {
    hosts => ['localhost:9200']
    document_id => "%{[@metadata][computed_id]}"
  }
  stdout { codec => rubydebug }
}
** 2016-10-13: caculate metric on daily basis
  CLOSED: [2016-10-13 Thu 15:33]
** 2016-10-14: verify DNS in digitalocean setting: http://injenkins.fluigdata.com:48080/job/DigitalOceanDeployCookbooks/61/console
  CLOSED: [2016-10-14 Fri 16:12]
curl http://fluigdata.com/web
curl http://fluigdata.com
curl www.fluigdata.com
curl www.fluigdata.com:8080
curl http://www.fluigdata.com
curl http://www.fluigdata.com:8080

curl http://127.0.0.1
** 2016-10-14: [[https://trello.com/c/Zhf1u9UJ][#1256]] www.fluigdata.com doesn't point to landing page
  CLOSED: [2016-10-14 Fri 16:12]
** 2016-10-15: [[https://trello.com/c/KRPfqugl][#1257]] After aio/sandbox deployment, elasticsearch has UNASSIGNED shards.
  CLOSED: [2016-10-15 Sat 15:20]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-10-17: [[https://trello.com/c/NUdGPzQK][#1262]] UpdateJenkinItSelf Job should shadow input of passphrase parameter
  CLOSED: [2016-10-17 Mon 09:08]
** 2016-10-17: [[https://github.com/TOTVS/mdmdevops/wiki/MDM-Server-List#machine-list-in-linode][Update Wiki]]: Update all Linode machine info to wiki
  CLOSED: [2016-10-17 Mon 17:19]
** 2016-10-18: [[https://github.com/TOTVS/mdmdevops/wiki/Customized-Docker-Images][Update wiki]]: jenkins docker image
  CLOSED: [2016-10-18 Tue 20:47]
** 2016-10-18: Fix all-in-one certificate: http://injenkins.fluigdata.com:48080/job/DockerDeployAllInOne/3/console
  CLOSED: [2016-10-18 Tue 23:07]
https://github.com/docker/docker/issues/8943

2016-10-18 07:49:06 -----> Starting Kitchen (v1.4.1)
2016-10-18 07:49:07 -----> Creating <default-ubuntu-1404>...
2016-10-18 07:49:07        An error occurred trying to connect: Post https://172.17.0.1:4243/v1.24/build?buildargs=%7B%7D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&rm=1&shmsize=0&ulimits=null: x509: cannot validate certificate for 172.17.0.1 because it doesn't contain any IP SANs
2016-10-18 07:49:07 >>>>>> ------Exception-------
2016-10-18 07:49:07 >>>>>> Class: Kitchen::ActionFailed
2016-10-18 07:49:07 >>>>>> Message: Failed to complete #create action: [Expected process to exit with [0], but received '1'
2016-10-18 07:49:07 ---- Begin output of docker -H tcp://172.17.0.1:4243 --tlsverify --tlscacert=../../misc/tls/ca.pem --tlscert=../../misc/tls/cert.pem --tlskey=../../misc/tls/key.pem build -f /mnt/jenkins/code/DockerDeployAllInOne/1.46/mdmdevops/cookbooks/all-in-one/Dockerfile-kitchen20161018-22678-1hktm60 - ----
2016-10-18 07:49:07 STDOUT:
** 2016-10-19: jenkins-mdm: http://injenkins.fluigdata.com:48080/job/DockerDeployBasicCookbooks/4/console
  CLOSED: [2016-10-19 Wed 11:20]
wget -O test/shared/mdm_deploy_key http://172.17.0.1/mdm_deploy_key

git_deploykey = '/var/lib/jenkins/.ssh/github_id_rsa'
  describe command("docker cp /tmp/kitchen/data/mdm_deploy_key #{container_name}:" \
                   "#{git_deploykey}") do
    its(:exit_status) { should eq 0 }
** 2016-10-19: build docker with a bigger machine flavor
  CLOSED: [2016-10-19 Wed 12:07]
** 2016-10-19: [[https://trello.com/c/Z4z3hgTo][#1260]] Run daily CI and automation test in Linode, instead of DigitalOcean
  CLOSED: [2016-10-19 Wed 12:08]
** 2016-10-19: Support Roboson sandbox upgrade issue; And Deployment guide improvement per feedback.
  CLOSED: [2016-10-19 Wed 14:52]
** [#A] 2016-10-19: Build latest jenkins docker image             :IMPORTANT:
  CLOSED: [2016-10-19 Wed 16:40]
docker stop my-test; docker rm my-test
docker run -t -P -d --name my-test -p 18080:18080 totvslabs/jenkins:v1.0 /bin/bash

docker exec -it my-test bash

export jenkins_port="18080"
export jenkins_version="2.19"
export jenkins_username="chefadmin"
export jenkins_passwd="TOTVS123FD"

cat  > /tmp/jenkins_credential <<EOF
export jenkins_username="chefadmin"
export jenkins_passwd="TOTVS123FD"
EOF
** 2016-10-19: remove linode2380574: Linode 12288  45.79.163.253	Newark, NJ, USA
  CLOSED: [2016-10-19 Wed 16:42]
ssh -p 2702 root@45.33.87.74

http://45.33.87.74:48080
** 2016-10-19: [[https://trello.com/c/S3hxk9Uh][#113]] The flush option is working just the first time after I build: http://injenkins.fluigdata.com:48080/job/DigitalOceanDeployCookbooks/1/console
  CLOSED: [2016-10-19 Wed 18:20]
How To Verify:

1. Have a fresh sandbox deployment and initialize system.
2. Run UpdateJenkinsItself with 1.45
3. Manually stop mdm
4. Run UpdateSandboxMDM with ensure_app_start true.
5. We should see that chef will start mdm process.
6. Run UpdateSandboxMDM again with db flush enabled
7. After deployment, check mdm.yml and make sure db flush is false.
8. Run UpdateSandboxMDM again with db flush enabled
9. Make sure the process has been restarted and db flush will be triggered again.

ssh root@138.68.19.32
** 2016-10-20: Update config file for index.query.bool.max_clause_count, and run integration tests
  CLOSED: [2016-10-20 Thu 16:39]
** 2016-10-20: Get notified when a new system deployment is triggered: slack enable everything.
  CLOSED: [2016-10-20 Thu 17:12]
denny zhang [10:00 AM]
Looks like we have just finished a deployment here.

Do we need Prod Jenkins to send us a slack notification, whether a new system deployment is triggered?
** 2016-10-21: hack document.js to update couchbase document size edit limit
  CLOSED: [2016-10-21 Fri 14:11]
http://stackoverflow.com/questions/19090611/couchbase-2-2-0-document-size-editing-limit
http://dbfuturist.blogspot.sg

2000, 2500

cd /opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public/js/

cb_docslimit=30000
cb_docbyteslimit=45000
sed -i "s/^  docsLimit: [0-9]*,/  docsLimit: $cb_docslimit,/g" documents.js
sed -i "s/^  docBytesLimit: [0-9]*,/  docBytesLimit: $cb_docbyteslimit,/g" documents.js

grep "^  docBytesLimit: [0-9]*," documents.js
grep "^  docsLimit: [0-9]*," documents.js
#  --8<-------------------------- separator ------------------------>8--

You can raise the limit or disable completely on version 2.2:

To raise the limit;

edit file: /opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public/js/documents.js

var DocumentsSection = {

  docsLimit: 1000,

  docBytesLimit: 2500,

init: function () {

var self = this;
Edit the docBytesLimit variable set to 2500 and increase it to your preferred value.

To disable completely;

You can comment out the conditional statement and return a false value. At line 362 comment out the statement and return false:

function isJsonOverLimited(json) {
  //return getStringBytes(json) > self.docBytesLimit;
  return false;
}
Hope this helps.. There are limitations as to how much your WYSYWIG editor can handle. So please be careful and as always editing core files can have negative results. We did it on our system and it works for us.
** 2016-10-21: Jenkins change notification from gmail to slack
  CLOSED: [2016-10-21 Fri 14:40]
su jenkins
cd ~/jobs

jenkins@jenkins:~/jobs$ find -name config.xml | xargs grep devops.totvslabs@dennyzhang.com
find -name config.xml | xargs grep devops.totvslabs@dennyzhang.com
./RubyCodeQualityCheck/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeployESCluster/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./BuildMDMRepoCodeActiveSprint/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./BashCodeQualityCheck/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./AutoscaleDockerCBClusterTest/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./AutoscaleDigitalOceanTest/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./GitPullCode/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./MonitorMDMNewActiveSprint/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./TCPScanReport/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./TestPreparation/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./SonarCodeQualityCheck/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeploySandboxPreviousActiveSprint/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./MonitorGitFileChangesFrameworkActiveSprint/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./MonitorServerFileChanges/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./AutoscaleDockerESClusterTest/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./LongRunAllInOne/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./PythonCodeQualityCheck/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./AutoscaleDockerClusterTest/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeployCBCluster/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./MonitorITResource/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./MonitorGitFileChangesMDMMaster/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DigitalOceanDeployCookbooks/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DeployDigitalOceanMDMCluster/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./MonitorGitFileChangesMDMActiveSprint/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeployFeatureCookbooks/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./InstallDockerCluster/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeploySandboxActiveSprint/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeployAPPWithoutCache/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeployCookbookUnStable/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeploySandboxUpgradeLegacyEnv/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeployMDMCluster/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./MonitorGitFileChangesFrameworkMaster/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./BackupCriticalData/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./LongRunCluster/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeployHadoopCookbooks/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./InstallDigitalOceanCluster/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./MonitorFrameworkNewActiveSprint/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeployBasicCookbooks/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./DockerDeployAllInOne/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./AutoscaleDigitalOceanClusterTest/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./BuildMDMRepoCodeMaster/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
./CommonServerCheck/config.xml:      <recipients>devops.totvslabs@dennyzhang.com</recipients>
** #  --8<-------------------------- separator ------------------------>8--
** 2016-10-24: [[https://trello.com/c/vnRZEq8z][#1298]] Duplicate ES nodes are found in cluster mdm deployment.
  CLOSED: [2016-10-24 Mon 10:03]
curl '172.17.0.4:9200/_cat/nodes'

curl '172.17.0.4:9200/_cat/nodes' | grep "\- \-" | grep -v backup

root@ubuntu:/home/denny# curl '172.17.0.4:9200/_cat/nodes' | grep "\- \-" | grep -v backup
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   632  100   632    0     0  18215      0 --:--:-- --:--:-- --:--:-- 18588
172.17.0.6 172.17.0.6 16 95 3.63 - - kitchen-mdm-app-nocache-node3node
172.17.0.5 172.17.0.5 41 95 3.63 - - kitchen-mdm-app-nocache-node2node
172.17.0.6 172.17.0.6 16 95 3.63 - - kitchen-mdm-app-nocache-node3node
172.17.0.5 172.17.0.5 41 95 3.63 - - kitchen-mdm-app-nocache-node2node

curl '172.17.0.4:9200/_cat/shards'
root@ubuntu:/home/denny# curl '172.17.0.4:9200/_cat/shards'
master-index-46078234297e400a1648d9c427dc8c4b  0 p STARTED      2  15.5kb 172.17.0.5 node2-kitchen-cluster-mdm-3nodes
staging-index-e4010da4110ba377d100f050cb4440db 0 p STARTED 114022 147.2mb 172.17.0.4 node1-kitchen-cluster-mdm-3nodes
master-index-e4010da4110ba377d100f050cb4440db  0 p STARTED    274 465.1kb 172.17.0.5 node2-kitchen-cluster-mdm-3nodes
staging-index-46078234297e400a1648d9c427dc8c4b 0 p STARTED  29390  37.2mb 172.17.0.4 node1-kitchen-cluster-mdm-3nodes
master-index-8cd6e43115e9416eb23609486fa053e3  0 p STARTED   2034   2.2mb 172.17.0.5 node2-kitchen-cluster-mdm-3nodes
staging-index-8cd6e43115e9416eb23609486fa053e3 0 p STARTED      1  83.7kb 172.17.0.4 node1-kitchen-cluster-mdm-3nodes
** 2016-10-25: Denny: remove old vm of blog: 45.79.161.77 -> 104.237.149.124
  CLOSED: [2016-10-25 Tue 07:12]
** 2016-10-25: http://injenkins.fluigdata.com:48080/job/PythonCodeQualityCheck/137/console
  CLOSED: [2016-10-25 Tue 10:15]
** 2016-10-25: docker image: on-line service: https://www.dennyzhang.com/demo_jenkins/
  CLOSED: [2016-10-25 Tue 11:50]
# start container: demojenkins.dennyzhang.com:18080
docker stop devops-jenkins; docker rm devops-jenkins
docker run -t -d -h jenkins --name devops-jenkins --privileged -p 18080:18080 -p 18000:80 -p 9000:9000 denny/jenkins:v3 /usr/bin/supervisord -n -c /etc/supervisor/supervisord.conf

http://demojenkins.dennyzhang.com:18080

docker exec -it devops-jenkins bash

service jenkins start
service apache2 start

http://injenkins.fluigdata.com:18080
** 2016-10-25: sandbox testcase failure, because run out of disk
  CLOSED: [2016-10-25 Tue 12:02]
Git update code for git@github.com:TOTVS/mdm.git to /var/lib/jenkins/code/1.47/mdm
+ '[' '!' -d /var/lib/jenkins/code/1.47/mdm ']'
+ mkdir -p /var/lib/jenkins/code/1.47
+ cd /var/lib/jenkins/code/1.47
+ git clone --depth 1 git@github.com:TOTVS/mdm.git --branch 1.47 --single-branch
Cloning into 'mdm'...
error: unable to write file mdm-tools/src/main/resources/fcfo_xsmall.json
error: unable to write file mdm-tools/src/main/resources/key.p12
error: unable to write file mdm-tools/src/main/resources/marelliSa1010Data.json
error: unable to write file mdm-tools/src/main/resources/rest_resource.vm
error: unable to write file mdm-tools/src/main/resources/robson_fields.json
error: unable to write file mdm-tools/src/main/resources/robson_template.json
error: unable to write file mdm-tools/src/main/resources/sample_contact.json
error: unable to write file mdm-tools/src/main/resources/sample_contact_schema.json
error: unable to write file mdm-tools/src/main/resources/tweet.json
fatal: cannot create directory at 'plugins': No space left on device
warning: Clone succeeded, but checkout failed.
You can inspect what was checked out with 'git status'
and retry the checkout with 'git checkout -f HEAD'

+ shell_exit
+ errorcode=128
+ echo -e '\n\nShow latest commits for TOTVS/mdm'
** 2016-10-25: cleanup legacy code, which fails sandbox test: http://injenkins.fluigdata.com:48080/job/DockerDeploySandboxPreviousActiveSprint/6/console
  CLOSED: [2016-10-25 Tue 12:59]
** 2016-10-26: sandbox test fail: devicemapper container run out of disk: http://injenkins.fluigdata.com:48080/job/DockerDeploySandboxUpgradeLegacyEnv/11/console
  CLOSED: [2016-10-26 Wed 08:24]
** 2016-10-26: Ruby code style improvement: fixing warning found by latest version of rubocop
  CLOSED: [2016-10-26 Wed 13:18]
** 2016-10-27: [[https://github.com/TOTVS/mdmdevops/wiki/MDM-Sandbox-For-FrontEnd][Add wiki]]: MDM Sandbox For FrontEnd
  CLOSED: [2016-10-27 Thu 15:16]
** 2016-10-27: fix error for demo jenkins
  CLOSED: [2016-10-27 Thu 18:08]
*** 2016-10-27: http://demojenkins.dennyzhang.com:18080/job/NetworkTCPScanAuditReport/5/console
   CLOSED: [2016-10-27 Thu 17:04]
*** CANCELED http://demojenkins.dennyzhang.com:18080/job/MonitorServerFileChangesAudit/54/console
    CLOSED: [2016-10-27 Thu 17:11]
**** 2016-10-27: http://demojenkins.dennyzhang.com:18080/job/BashCodeQualityCheck/15/console
    CLOSED: [2016-10-27 Thu 18:08]
*** http://demojenkins.dennyzhang.com:18080/job/SonarCodeQualityCheck/1/console
** 2016-10-31: new active sprint: 1.48
  CLOSED: [2016-10-31 Mon 22:32]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-11-01: download source code: http://injenkins.fluigdata.com:48080/job/DockerDeployBasicCookbooks/39/console
  CLOSED: [2016-11-01 Tue 12:35]
git@bitbucket.org:lrpdevops/mdmdevops-totvslabs.git

1.47-deploy
app-mdm

cd /var/lib/jenkins/code/DockerDeployBasicCookbooks/1.47-deploy/mdmdevops-totvslabs/cookbooks/app-mdm
export INSTANCE_NAME=app-mdm-DockerDeployBasicCookbooks-39
export KITCHEN_YAML=".kitchen_fe.yml"

export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true
export CLEAN_START=false
export IMAGE_NAME=totvslabs/mdm:latest
export DOCKER_PORT_FORWARD_PREFIX=33
export PACKAGE_URL='http://172.17.0.1:51081'
export APP_BRANCH_NAME=1.47
export FRAMEWORK_BRANCH_NAME=1.47
export TEST_KITCHEN_YAML=".kitchen_fe.yml"
export CLUSTER_ID=kitchen-mdm-basic
** 2016-11-01: build docker image
  CLOSED: [2016-11-01 Tue 12:35]
docker build -f Dockerfile_mdm_v1_1 -t totvslabs/mdm:v1.1 --rm=true .

latest
** 2016-11-02: Merge 1.47 to master
  CLOSED: [2016-11-02 Wed 09:00]
** 2016-11-02: remove legacy code for 1.47
  CLOSED: [2016-11-02 Wed 09:15]
** 2016-11-02: merge 1.47 to master, then to 1.48
  CLOSED: [2016-11-02 Wed 10:40]
** 2016-11-02: [[https://github.com/TOTVS/mdmdevops/wiki/How-To-Deploy-a-High-Availability-MDM-Cluster-Env#41-run-deploysystem-jenkins-job-to-deploy-system][Update wiki]]: node sequence in server_list of DeploySystem job
  CLOSED: [2016-11-02 Wed 10:54]
# explore CB
173.230.144.20:22
173.230.152.155:22
173.255.210.203:22

# explore ES
173.255.243.91:22
173.255.251.230:22
74.207.247.120:22

# explore Jenkins
45.33.104.7:22

# explore Load Balancer
45.56.88.250:22

# explore APP
45.56.95.149:22

# exploreworker
50.116.11.120:22
** 2016-11-02: configure iptables: http://45.33.104.7:18080/job/RunCommandOnServers/1/console
  CLOSED: [2016-11-02 Wed 10:47]
echo 'root:DevOpsChangeMe1' | chpasswd
mkdir -p /home/denny
iptables-save > /home/denny/20161102_rules.v4
iptables -F; iptables -X
echo 'y' | ufw reset
echo 'y' | ufw enable
ufw default deny incoming
ufw default deny forward
ufw allow 22/tcp

# allow for office ip
ufw allow from 12.145.25.178

# haproxy
ufw allow 443/tcp
ufw allow 80/tcp

# allow from all nodes
ufw allow from 45.33.104.7
ufw allow from 45.56.88.250
ufw allow from 45.56.95.149
ufw allow from 50.116.11.120
ufw allow from 173.230.144.20
ufw allow from 173.230.152.155
ufw allow from 173.255.210.203
ufw allow from 173.255.243.91
ufw allow from 173.255.251.230
ufw allow from 74.207.247.120
** 2016-11-02: nagios fail to check mdmbackup: nagios-nrpe-server starts before iptables is proplery configured
  CLOSED: [2016-11-02 Wed 12:53]
check_disk_rootfs

su nagios

/usr/lib/nagios/plugins/check_nrpe -H 50.116.11.120 -c check_MEMORY
** #  --8<-------------------------- separator ------------------------>8--
** 2016-11-02: mdmbackup doesn't start: wrong hostname in mdmbackup.yml
  CLOSED: [2016-11-02 Wed 13:07]
** 2016-11-02: Ubuntu permanently change hostname
  CLOSED: [2016-11-02 Wed 14:48]
45.33.104.7 explorejenkins
45.56.88.250 exploreloadbalancer
45.56.95.149 exploreapp
50.116.11.120 exploreworker
173.230.144.20 explorecb1
173.230.152.155 explorecb2
173.255.210.203 explorecb3
173.255.243.91 explorees1
173.255.251.230 explorees2
74.207.247.120 explorees3
*** TODO manually change hostname
**** explorecb3
echo "explorecb3" > /etc/hostname
hostname -F /etc/hostname
grep -v ubuntu /etc/hosts > /tmp/hosts
cp /tmp/hosts /etc/hosts

echo "173.255.210.203 explorecb3" >> /etc/hosts
cat /etc/hosts
hostname
ifconfig | grep inet
**** explorecb2
echo "explorecb2" > /etc/hostname
hostname -F /etc/hostname
grep -v ubuntu /etc/hosts > /tmp/hosts
cp /tmp/hosts /etc/hosts

echo "173.230.152.155 explorecb2" >> /etc/hosts
cat /etc/hosts
hostname
ifconfig | grep inet
**** explorecb1
echo "explorecb1" > /etc/hostname
hostname -F /etc/hostname
grep -v ubuntu /etc/hosts > /tmp/hosts
cp /tmp/hosts /etc/hosts

echo "173.230.144.20 explorecb1" >> /etc/hosts
cat /etc/hosts
hostname
ifconfig | grep inet
**** TODO exploreworker
echo "exploreworker" > /etc/hostname
hostname -F /etc/hostname
grep -v ubuntu /etc/hosts > /tmp/hosts
cp /tmp/hosts /etc/hosts

echo "50.116.11.120 exploreworker" >> /etc/hosts
cat /etc/hosts
hostname
ifconfig | grep inet
**** exploreapp
echo "exploreapp" > /etc/hostname
hostname -F /etc/hostname
grep -v ubuntu /etc/hosts > /tmp/hosts
cp /tmp/hosts /etc/hosts

echo "45.56.95.149 exploreapp" >> /etc/hosts
cat /etc/hosts
hostname
ifconfig | grep inet
**** #  --8<-------------------------- separator ------------------------>8--
**** exploreloadbalancer
echo "exploreloadbalancer" > /etc/hostname
hostname -F /etc/hostname
grep -v ubuntu /etc/hosts > /tmp/hosts
cp /tmp/hosts /etc/hosts

echo "45.56.88.250 exploreloadbalancer" >> /etc/hosts
cat /etc/hosts
hostname
ifconfig | grep inet
**** explorejenkins
echo "explorejenkins" > /etc/hostname
hostname -F /etc/hostname
grep -v ubuntu /etc/hosts > /tmp/hosts
cp /tmp/hosts /etc/hosts

echo "45.33.104.7 explorejenkins" >> /etc/hosts
cat /etc/hosts
hostname
ifconfig | grep inet
**** #  --8<-------------------------- separator ------------------------>8--
**** es1
echo "explorees1" > /etc/hostname
hostname -F /etc/hostname
grep -v ubuntu /etc/hosts > /tmp/hosts
cp /tmp/hosts /etc/hosts

echo "173.255.243.91 explorees1" >> /etc/hosts
cat /etc/hosts
hostname
**** es2
echo "explorees2" > /etc/hostname
hostname -F /etc/hostname
grep -v ubuntu /etc/hosts > /tmp/hosts
cp /tmp/hosts /etc/hosts

echo "173.255.251.230 explorees2" >> /etc/hosts
cat /etc/hosts
hostname
**** es3
echo "explorees3" > /etc/hostname
hostname -F /etc/hostname
grep -v ubuntu /etc/hosts > /tmp/hosts
cp /tmp/hosts /etc/hosts

echo "74.207.247.120 explorees3" >> /etc/hosts
cat /etc/hosts
hostname
** 2016-11-02: [[https://github.com/TOTVS/mdmdevops/wiki/MDM-Trouble-Shooting][update wiki]]: manually start elasticsearch
  CLOSED: [2016-11-02 Wed 15:11]
su elasticsearch

/usr/lib/jvm/java-8-oracle-amd64/bin/java -Xms512m -Xmx512m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -server -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Xms512m -Xmx512m -Xss256k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.3.3.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch/elasticsearch.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/usr/share/elasticsearch --default.path.conf=/etc/elasticsearch
** 2016-11-03: disk clean up for official build server: 104.236.159.226
  CLOSED: [2016-11-03 Thu 09:49]
/var/lib/jenkins/jobs/BuildMDMRepoActiveSprint/workspace

root@103c78078504:/var/lib/jenkins# du -h -d 1 .
56K     ./secrets
4.0K    ./workflow-libs
28K     ./.java
16K     ./.local
20K     ./.config
206M    ./.cache
8.0K    ./userContent
436K    ./logs
1.7G    ./code
4.0K    ./nodes
3.6M    ./fingerprints
524K    ./config-history
32K     ./.ssh
16G     ./jobs
1.4M    ./updates
187M    ./.npm
6.1G    ./codeactivesprint
20K     ./global-build-stats
24K     ./serverspec
2.1G    ./.m2
4.9G    ./codeactivesprinttest
124K    ./users
17M     ./.node-gyp
16K     ./workspace
142M    ./plugins
31G     .
** 2016-11-03: gui fail to login: use port 3001, instead of 3000
  CLOSED: [2016-11-03 Thu 11:20]
** 2016-11-03: restart service
  CLOSED: [2016-11-03 Thu 12:27]
** 2016-11-03: define a initscript for frontend service
  CLOSED: [2016-11-03 Thu 12:28]
** 2016-11-03: branch merge: test to master
  CLOSED: [2016-11-03 Thu 12:38]
** 2016-11-03: branch merge: test to master
  CLOSED: [2016-11-03 Thu 12:38]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-11-03: http://prod-doc.fluigdata.com:8083/ runs into 404: http://104.236.159.226:18080/job/CommonServerCheck/
  CLOSED: [2016-11-03 Thu 12:55]
denny zhang [12:51 PM]
The url runs into 404. What if you start it locally?

http://prod-docs.fluigdata.com:8083/fluigdata/process/enrichment/

[12:54]
I manually restart the process.

Now it runs into 200 OK.
** 2016-11-03: Alert slack notification for all events about explore env
  CLOSED: [2016-11-03 Thu 14:36]
#explore_env_alert

- Install Jenkins slack notification
- Configure slack credentials
- Configure DeploySystem job
** 2016-11-03: update jenkins job
  CLOSED: [2016-11-03 Thu 14:45]
 "nagios_mdm":
        {
        "elasticsearch_warn_mem_mb":"6144",
        "elasticsearch_cri_mem_mb":"7168",
        "couchbase_warn_mem_mb":"6144",
        "couchbase_cri_mem_mb":"7168",
        "mdm_warn_mem_mb":"6144",
        "mdm_cri_mem_mb":"7168",
        "mdm_warn_cpu":"200",
        "mdm_cri_cpu":"400",
        "slack_channel":"explore_env_alert",
        "slack_token":"4ethl3yhEDS0t5LApIzhd5Fw"},
 #  --8<-------------------------- separator ------------------------>8--
         "log_slack_token":"xoxb-21659697588-X2eTOzSaAGJF5wJT4ku0l7CW",
         "log_slack_channel":"mdm-logs",
** 2016-11-03: explore jenkins: show changeset after deployment
  CLOSED: [2016-11-03 Thu 14:40]
http://45.33.104.7:18080/job/DeploySystemRehearsal/3/console
cat /root/chef_update/report.txt
** 2016-11-03: sandbox fail to check service status
  CLOSED: [2016-11-03 Thu 14:55]
jenkins@jenkins:/$ ssh -i /var/lib/jenkins/ssh_id_rsa -p 6022 -o StrictHostKeyChecking=no root@172.18.0.1 "lsof -i tcp:3000"
jenkins@jenkins:/$ echo $?
1
** 2016-11-03: http://injenkins.fluigdata.com:48080/job/DockerDeploySandboxFrontendActiveSprint/20/console
  CLOSED: [2016-11-03 Thu 14:55]
service mdmfrontend status

chmod 755 /etc/init.d/mdmfrontend

/etc/init.d/mdmfrontend status

service mdmfrontend status

bash -x /etc/init.d/mdmfrontend start

CHECK_PID_RUNNING=$(ps ax | grep gulp | grep -v grep | sed 's/^\s*\([0-9]*\)\s.*/\1/')

 * Starting mdmfrontend...+ touch /usr/local/var/run/mdm.pid                    │···+
start-stop-daemon --start --chuid mdm --quiet --make-pidfile --pidfile /usr/lo│···cal/var/run/mdm.pid --background -d /home/mdm/code/mount/mdm/app/web-apps/mdm-ui│···/ --startas /bin/bash -- -c 'exec gulp serve:noconfig >>/opt/mdm/logs/mdmfronten│···d-initscript.log 2>&1'

export SRC_DIR=/home/mdm/code/mount
export PIDFILE=/usr/local/var/run/mdm.pid

start-stop-daemon --start --chuid mdm --quiet --make-pidfile --pidfile $PIDFILE --background -d "${SRC_DIR}/mdm/app/web-apps/mdm-ui/" --startas /bin/bash -- -c "exec gulp serve:noconfig >>/home/mdm/logs/mdmfrontend-initscript.log 2>&1"

cat /home/mdm/logs/mdmfrontend-initscript.log

ps -ef | grep gulp
** 2016-11-03: [[https://github.com/TOTVS/mdmdevops/wiki/Frontend-Sandbox-FAQ][Add wiki]]: Frontend Sandbox FAQ
  CLOSED: [2016-11-03 Thu 15:17]
** 2016-11-03: [[https://github.com/TOTVS/mdmdevops/wiki/MDM-Sandbox-For-FrontEnd][Add wiki]]: MDM Sandbox For FrontEnd
  CLOSED: [2016-11-03 Thu 16:04]
** 2016-11-03: fail to start elasticsearch
  CLOSED: [2016-11-03 Thu 18:25]
elasticsearch@ubuntu:/root$ /usr/lib/jvm/java-8-oracle-amd64/bin/java -Xms512m -Xmx512m -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -server -Djava.awt.headless=true -Djava.net.preferIPv4Stack=true -Xms512m -Xmx512m -Xss256k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Djna.nosys=true -Des.path.home=/usr/share/elasticsearch -cp /usr/share/elasticsearch/lib/elasticsearch-2.3.3.jar:/usr/share/elasticsearch/lib/* org.elasticsearch.bootstrap.Elasticsearch start -d -p /var/run/elasticsearch/elasticsearch.pid --default.path.home=/usr/share/elasticsearch --default.path.logs=/var/log/elasticsearch --default.path.data=/usr/share/elasticsearch --default.path.conf=/etc/elasticsearch
Exception in thread "main" BindTransportException[Failed to bind to [9300-9400]]; nested: ChannelException[Failed to bind to: ubuntu/74.207.247.120:9400]; nested: BindException[Cannot assign requested address];
Likely root cause: java.net.BindException: Cannot assign requested address
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:437)
	at sun.nio.ch.Net.bind(Net.java:429)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Refer to the log for complete error details.
** 2016-11-03: persist change: whether to frontend_skip_update_code, frontend_skip_update_cfg
  CLOSED: [2016-11-03 Thu 20:29]
** 2016-11-04: UX sandbox improvement per Kung's feedback
  CLOSED: [2016-11-04 Fri 08:49]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-11-07: update slack token for security concern
  CLOSED: [2016-11-04 Fri 09:07]
#+BEGIN_EXAMPLE
Denny-mac:DeploySystem mac$ git diff
diff --git a/chef/setup_jenkins/jenkins_jobs/DeploySystem/config.xml b/chef/setup_jenkins/jenkins_jobs/DeploySystem/config.xml
index cf78eb1..7f978a3 100644
--- a/chef/setup_jenkins/jenkins_jobs/DeploySystem/config.xml
+++ b/chef/setup_jenkins/jenkins_jobs/DeploySystem/config.xml
@@ -90,7 +90,8 @@
         &quot;mdm_warn_cpu&quot;:&quot;200&quot;,
         &quot;mdm_cri_cpu&quot;:&quot;400&quot;,
         &quot;slack_channel&quot;:&quot;explore_env_alert&quot;,
-        &quot;slack_token&quot;:&quot;4ethl3yhEDS0t5LApIzhd5Fw&quot;},
+        # TODO: use correct slack token
+        &quot;slack_token&quot;:&quot;&quot;},
  &quot;couchbase_mdm&quot;:
         {&quot;memory_quota_mb&quot;:&quot;2048&quot;},
  &quot;elasticsearch_mdm&quot;:
#+END_EXAMPLE

#+BEGIN_EXAMPLE
        # TODO: change to correct log_slack_token in real deployment
        &quot;log_slack_token&quot;:&quot;xoxb-100486418085-0dm75CcwARHExvVesBsRqEqX&quot;,
#+END_EXAMPLE
** 2016-11-07: build latest docker image
  CLOSED: [2016-11-04 Fri 14:15]
** 2016-11-07: reconfigure elasticsearch and CB shards and replicas
  CLOSED: [2016-11-02 Wed 15:12]
        "cb_replicas_count":"1",
        "elasticsearch_replicas_count":"1",
        "elasticsearch_shards_count":"10",
        "elasticsearch_audit_replicas_count":"1",
        "elasticsearch_audit_shards_count":"10",
** 2016-11-07: Use Jenkins or uptimerobot to monitor all critical sites listed in [[https://github.com/TOTVS/mdmdevops/wiki/MDM-Server-List][this wiki]]
  CLOSED: [2016-11-07 Mon 13:34]
ssh root@104.236.159.226

totvs-wilson-kibana
curl -I http://107.170.214.241:5601
** 2016-11-07: [[https://github.com/TOTVS/mdmdevops/wiki/How-To-Deploy-a-High-Availability-MDM-Cluster-Env#1-provision-target-nodes][Update wiki]]: Need to reconfigure host name in Linode env
  CLOSED: [2016-11-07 Mon 13:39]
Currently all hostname is ubuntu.

This confuses /etc/hosts, which results in issues for elasticsearch
elasticsearch@ubuntu:/root$ cat /etc/hosts
173.255.243.91 ubuntu
74.207.247.120    ubuntu
74.207.247.120    ubuntu
127.0.0.1       localhost
74.207.247.120    ubuntu

# The following lines are desirable for IPv6 capable hosts
::1     localhost ip6-localhost ip6-loopback
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
45.56.95.149 app.fluigdata.com

[11:45]
====================
Let me reconfigure hostname manually. And make a note for future
** 2016-11-07: nagios monitor mdmbackup services: http://injenkins.fluigdata.com:48080/job/DockerDeployFeatureCookbooks/44/console
  CLOSED: [2016-11-07 Mon 17:21]
git@bitbucket.org:lrpdevops/mdmdevops-totvslabs.git

cd /var/lib/jenkins/code/DockerDeployFeatureCookbooks/1.48-deploy/mdmdevops-totvslabs/cookbooks/mdm-cluster
export INSTANCE_NAME=mdm-cluster-DockerDeployFeatureCookbooks-44
export KITCHEN_YAML=.kitchen.yml

export branch_name=1.48-deploy

export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true

export TEST_KITCHEN_YAML=".kitchen.yml"
# export DOCKER_PORT_FORWARD_PREFIX=31
export PACKAGE_URL='http://172.17.0.1:51081'
export APP_BRANCH_NAME=1.48
export FRAMEWORK_BRANCH_NAME=1.48
export CLUSTER_ID=kitchen-mdm-feature
** 2016-11-08: Branch out 1.49 from 1.48, and run full cycle CI tests
  CLOSED: [2016-11-08 Tue 07:41]
** 2016-11-09: Support wilson's test for UX sandbox
  CLOSED: [2016-11-09 Wed 10:18]
** 2016-11-09: [[https://github.com/TOTVS/mdmdevops/wiki/MDM-Trouble-Shooting#how-to-manually-start-mdm-gulp-in-ux-sandbox][Update wiki]]: How To Manually Start MDM Gulp in UX Sandbox?
  CLOSED: [2016-11-09 Wed 11:27]
su mdm

cd /home/mdm/code/mount/mdm/app/web-apps/mdm-ui
gulp serve:noconfig
** 2016-11-10: bug fixing for nagios server host check
  CLOSED: [2016-11-10 Thu 08:45]
** 2016-11-11: Avoid tons of check_mdm_log nagios alerts in #devops slack channel
  CLOSED: [2016-11-11 Fri 09:45]
** 2016-11-11: [[https://github.com/TOTVS/mdmdevops/wiki/Nexus-Server-Maintainance][Add Wiki]]: Nexus Server Maintenance
  CLOSED: [2016-11-11 Fri 13:27]
** 2016-11-11: [#A] nexus3 how to import and export repository configuration
  CLOSED: [2016-11-11 Fri 16:53]
https://books.sonatype.com/nexus-book/reference/archiva.html
https://groups.google.com/a/glists.sonatype.com/forum/#!topic/nexus-users/lPZ-JKEOMT0

http://www.sonatype.org/nexus/category/nexus-3/
http://blog.sonatype.com/2010/01/how-to-backup-nexus-configuration-and-repository-artifacts/
http://blog.sonatype.com/2010/04/nexus-tip-moving-artifacts-between-nexus-repositories/
http://serverfault.com/questions/238911/migrate-sonatype-nexus-repo-from-one-machine-to-another

- nexus-configuration: (System -> Support -> System Information)
installDirectory	/home/nexus/nexus-3.1.0-04
workingDirectory	/home/nexus/sonatype-work/nexus3
temporaryDirectory	/home/nexus/sonatype-work/nexus3/tmp

docker cp my-test:/home/nexus/sonatype-work/nexus3 /tmp/

cp -r /tmp/nexus3/* ./

cache and log directories are not needed
https://groups.google.com/a/glists.sonatype.com/forum/#!topic/nexus-users/lPZ-JKEOMT0
** 2016-11-13: [#A] upgrading our nexus server to the latest
  CLOSED: [2016-11-13 Sun 12:23]
npmjs-denny-proxy
proxy
npm

ln -s /home/nexus/nexus-3.1.0-04/bin/nexus /etc/init.d/nexus
*** Requirement
https://issues.sonatype.org/browse/NEXUS-9741

Kung Wang	 [2:11 PM]
The error they got is similar to this one:
https://issues.sonatype.org/browse/NEXUS-9741

[2:11]
error coming from this repo:
@exponent/ex-navigation

denny zhang	 [2:11 PM]
Yes, I'm running local Nexus setup.

Kung Wang	 [2:11 PM]
it’s their dependencies

[2:12]
you can try this repo to see if you can do npm install to pull from your local nexus

[2:12]
https://github.com/TOTVS/explore-mobile
denny zhang	 [2:13 PM]
*** Install latest Nexus: 3.1.0
docker stop my-test; docker rm my-test;

docker run -t -P -d --name my-test -p 8081:8081 totvslabs/mdm:latest /bin/bash

docker exec -it my-test bash

apt-get install -y wget lsof

wget https://sonatype-download.global.ssl.fastly.net/nexus/3/nexus-3.1.0-04-unix.tar.gz
gzip -d nexus-3.1.0-04-unix.tar.gz
tar -xf nexus-3.1.0-04-unix.tar

cd nexus-3.1.0-04

cd bin
./nexus start
./nexus status
ps -ef | grep nexus
# lsof -P -n -p 232 | grep -i listen
lsof -i tcp:8081
*** checkout npm code
# inject /root/.ssh/id_rsa
apt-get update
apt-get install -y git vim

> /root/.ssh/id_rsa && vim /root/.ssh/id_rsa
# /Users/mac/baidu/百度云同步盘/private_data/emacs_stuff/backup_small/ssh_key/github_id_rsa

chmod 600 /root/.ssh/id_rsa

cd /root/
git clone git@github.com:TOTVS/explore-mobile.git

cd explore-mobile

vim ./.npmrc
# registry=http://localhost:8081/nexus/content/groups/npmjs/

npm install
** #  --8<-------------------------- separator ------------------------>8--
** 2016-11-14: [[https://trello.com/c/CHzidhUJ][#1413]] upgrade Nexus server and standardize the maintenance process of Nexus
  CLOSED: [2016-11-14 Mon 08:34]
- Old Repository
| Name                | Type    | Format | Policy   | Comment                                  | Llink                                                                          |
|---------------------+---------+--------+----------+------------------------------------------+--------------------------------------------------------------------------------|
| 3rd party           | hosted  | maven2 | Release  |                                          | http://nexus.fluigdata.com:8081/nexus/content/repositories/thirdparty/         |
| npmjs TOTVS         | hosted  | npm    |          |                                          | http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjstotvs/         |
| Releases            | hosted  | maven2 | Release  |                                          | http://nexus.fluigdata.com:8081/nexus/content/repositories/releases            |
| Snapshots           | hosted  | maven2 | Snapshot |                                          | http://nexus.fluigdata.com:8081/nexus/content/repositories/snapshots/          |
| Apache Snapshots    | proxy   | maven2 | Snapshot | https://repository.apache.org/snapshots/ | http://nexus.fluigdata.com:8081/nexus/content/repositories/apache-snapshots/   |
| Central             | proxy   | maven2 | Release  | https://repo1.maven.org/maven2/          | http://nexus.fluigdata.com:8081/nexus/content/repositories/central/            |
| Codehaus Snapshots  | proxy   | maven2 | Snapshot | https://nexus.codehaus.org/snapshots/    | http://nexus.fluigdata.com:8081/nexus/content/repositories/codehaus-snapshots/ |
| Jitpack             | proxy   | maven2 | Release  | https://jitpack.io/                      | http://nexus.fluigdata.com:8081/nexus/content/repositories/jitpack/            |
| npmjs Central       | proxy   | npm    | N/A      | ??                                       | http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjscentral/       |
| Central M1 shadow   | virtual | maven1 | Release  | ??  Source Nexus Repository ID: Central  | http://nexus.fluigdata.com:8081/nexus/content/shadows/central-m1/              |
| Public Repositories | group   | maven2 |          | Apache Snapshots; Codehaus Snapshots     | http://nexus.fluigdata.com:8081/nexus/content/groups/public/                   |
| npmjs Group         | group   | npm    |          | npmjs TOTVS; npmjs Central               | http://nexus.fluigdata.com:8081/nexus/content/groups/npmjs/                    |

- New Repository
| Name                | Type   | Format | Policy   | Comment                                  | Policy  | Link                                                             |
|---------------------+--------+--------+----------+------------------------------------------+---------+------------------------------------------------------------------|
| thirdparty          | hosted | maven2 | Release  |                                          | Disable | http://nexus3.fluigdata.com:8081/repository/thirdparty/          |
| npmjstotvs          | hosted | npm    |          |                                          | Allow   | http://nexus3.fluigdata.com:8081/repository/npmjstotvs/          |
| Releases            | hosted | maven2 | Release  |                                          | Allow   | http://nexus3.fluigdata.com:8081/repository/Releases/            |
| Snapshots           | hosted | maven2 | Snapshot |                                          | Allow   | http://nexus3.fluigdata.com:8081/repository/Snapshots/           |
| apache-snapshots    | proxy  | maven2 | Snapshot | https://repository.apache.org/snapshots/ | N/A     | http://nexus3.fluigdata.com:8081/repository/apache-snapshots/    |
| Central             | proxy  | maven2 | Release  | https://repo1.maven.org/maven2/          | N/A     | http://nexus3.fluigdata.com:8081/repository/Central/             |
| codehaus-snapshots  | proxy  | maven2 | Snapshot | https://nexus.codehaus.org/snapshots/    | N/A     | http://nexus3.fluigdata.com:8081/repository/codehaus-snapshots/  |
| Jitpack             | proxy  | maven2 | Release  | https://jitpack.io/                      | N/A     | http://nexus3.fluigdata.com:8081/repository/Jitpack/             |
| Public-Repositories | group  | maven2 |          |                                          |         | http://nexus3.fluigdata.com:8081/repository/Public-Repositories/ |
| npmjs-central       | proxy  | npm    |          |                                          |         |                                                                  |
| npmjs-group         | group  | npm    |          |                                          |         | http://nexus3.fluigdata.com:8081/repository/npmjs-group/         |
*** update nexus
# http://nexus.fluigdata.com:8081/nexus/content/repositories/releases
var1="http:\/\/nexus.fluigdata.com:8081\/nexus\/content\/repositories\/releases"
var2="http:\/\/nexus3.fluigdata.com:8081\/repository\/Releases"
find . -name "pom.xml" | xargs sed -i "" "s/$var1/$var2/g"
find . -name "README.md" | xargs sed -i "" "s/$var1/$var2/g"
find . -name ".npmrc" | xargs sed -i "" "s/$var1/$var2/g"
find . -name "npm*.json" | xargs sed -i "" "s/$var1/$var2/g"

# http://nexus.fluigdata.com:8081/nexus/content/groups/npmjs/
var1="http:\/\/nexus.fluigdata.com:8081\/nexus\/content\/groups\/npmjs\/"
var2="http:\/\/nexus3.fluigdata.com:8081\/repository\/npmjs-group\/"
find . -name "pom.xml" | xargs sed -i "" "s/$var1/$var2/g"
find . -name "README.md" | xargs sed -i "" "s/$var1/$var2/g"
find . -name ".npmrc" | xargs sed -i "" "s/$var1/$var2/g"
find . -name "npm*.json" | xargs sed -i "" "s/$var1/$var2/g"

# http://nexus.fluigdata.com:8081/nexus/content/repositories/snapshots
var1="http:\/\/nexus.fluigdata.com:8081\/nexus\/content\/repositories\/snapshots"
var2="http:\/\/nexus3.fluigdata.com:8081\/repository\/Snapshots\/"
find . -name "pom.xml" | xargs sed -i "" "s/$var1/$var2/g"
find . -name "README.md" | xargs sed -i "" "s/$var1/$var2/g"
find . -name ".npmrc" | xargs sed -i "" "s/$var1/$var2/g"
find . -name "npm*.json" | xargs sed -i "" "s/$var1/$var2/g"

# http://nexus.fluigdata.com:8081/nexus/content/groups/public/
var1="http:\/\/nexus.fluigdata.com:8081\/nexus\/content\/groups\/public\/"
var2="http:\/\/nexus3.fluigdata.com:8081\/repository\/Public-Repositories\/"
find . -name "pom.xml" | xargs sed -i "" "s/$var1/$var2/g"
find . -name "README.md" | xargs sed -i "" "s/$var1/$var2/g"
find . -name ".npmrc" | xargs sed -i "" "s/$var1/$var2/g"
find . -name "npm*.json" | xargs sed -i "" "s/$var1/$var2/g"

# http://nexus.fluigdata.com:8081/nexus/content/groups/npmjs/
var1="http:\/\/nexus.fluigdata.com:8081\/nexus\/content\/groups\/npmjs\/"
var2="http:\/\/nexus3.fluigdata.com:8081\/repository\/npmjs-group\/"
find . -name "pom.xml" | xargs sed -i "" "s/$var1/$var2/g"
find . -name "README.md" | xargs sed -i "" "s/$var1/$var2/g"
find . -name ".npmrc" | xargs sed -i "" "s/$var1/$var2/g"
find . -name "npm*.json" | xargs sed -i "" "s/$var1/$var2/g"
find . -name "build_*.sh" | xargs sed -i "" "s/$var1/$var2/g"
**** 2016-11-14: nlpcn deployment
  CLOSED: [2016-11-11 Fri 11:14]
Bruno Volpato	 [1:47 PM]
to deploy the nlpcn, I suggest you downloading from the old nexus and run a manual deploy

[1:47]
https://maven.apache.org/guides/mini/guide-3rd-party-jars-remote.html

denny zhang	 [1:48 PM]
Thanks, Bruno.

I seemed to find out another way to bypass, with no communication with old nexus.
**** 2016-11-14: [#A] nexus error: Could not find artifact org.nlpcn:elasticsearch-sql:jar:2.1.1
  CLOSED: [2016-11-11 Fri 11:23]
http://injenkins.fluigdata.com:48080/job/dennytest/4/console
#+BEGIN_EXAMPLE
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal on project generic-dao: Could not resolve dependencies for project com.totvslabs.framework:generic-dao:jar:1.49.0-SNAPSHOT: Could not find artifact org.nlpcn:elasticsearch-sql:jar:2.1.1 in couchbase (http://files.couchbase.com/maven2) -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :generic-dao
make: *** [all] Error 1


Show latest commits for TOTVS/mdm

Show latest git commits: git log -n 10 --pretty=format:"%h - %an, %ar : %s"
d5c97b0 - dennyzhang, 5 hours ago : update nexus server endpoint. Currently it's a temporary server. Will replace with formal one, before branch merge
#+END_EXAMPLE
**** 2016-11-14: [#A] nexus error: fetch failed 404 http://nexus3.fluigdata.com:8081/repository/npmjs-group/gulp-svgmin/-/gulp-svgmin-1.2.3.tgz
  CLOSED: [2016-11-11 Fri 11:24]
http://injenkins.fluigdata.com:48080/job/dennytest/2/console
#+BEGIN_EXAMPLE
+ cd /var/lib/jenkins/jobs/dennytest/workspace/1.49-devops-1413-new-nexus/mdm
+ rm -Rf build
+ title 'MDM - Build FE - begin'
+ unset PROMPT_COMMAND
+ echo -ne '\033]0;MDM - Build FE - begin\007'
]0;MDM - Build FE - begin+ cd app/web-apps/
+ false
+ npm install
npm WARN package.json fluigdata@0.0.2 No repository field.
npm WARN package.json fluigdata@0.0.2 No README data
npm ERR! fetch failed http://nexus3.fluigdata.com:8081/repository/npmjs-group/gulp-svgmin/-/gulp-svgmin-1.2.3.tgz
npm WARN retry will retry, error on last attempt: Error: fetch failed with status code 404
npm ERR! fetch failed http://nexus3.fluigdata.com:8081/repository/npmjs-group/gulp-postcss/-/gulp-postcss-6.2.0.tgz
npm WARN retry will retry, error on last attempt: Error: fetch failed with status code 404
npm ERR! fetch failed http://nexus3.fluigdata.com:8081/repository/npmjs-group/gulp-filter/-/gulp-filter-3.0.1.tgz
npm WARN retry will retry, error on last attempt: Error: fetch failed with status code 404
npm ERR! fetch failed http://nexus3.fluigdata.com:8081/repository/npmjs-group/gulp-clean-css/-/gulp-clean-css-2.0.13.tgz
npm WARN retry will retry, error on last attempt: Error: fetch failed with status code 404
npm ERR! fetch failed http://nexus3.fluigdata.com:8081/repository/npmjs-group/autoprefixer/-/autoprefixer-6.5.0.tgz
npm WARN retry will retry, error on last attempt: Error: fetch failed with status code 404
npm ERR! fetch failed http://nexus3.fluigdata.com:8081/repository/npmjs-group/gulp-svgmin/-/gulp-svgmin-1.2.3.tgz
npm WARN retry will retry, error on last attempt: Error: fetch failed with status code 404
npm ERR! fetch failed http://nexus3.fluigdata.com:8081/repository/npmjs-group/gulp-postcss/-/gulp-postcss-6.2.0.tgz
npm WARN retry will retry, error on last attempt: Error: fetch failed with status code 404
npm ERR! fetch failed http://nexus3.fluigdata.com:8081/repository/npmjs-group/gulp-filter/-/gulp-filter-3.0.1.tgz
#+END_EXAMPLE
**** 2016-11-14: Questions: Nexus server migration issue
  CLOSED: [2016-11-11 Fri 11:27]
#+BEGIN_EXAMPLE

denny zhang	 [11:00 AM]
Hi @kungwang & @bruno, about Nexus update, I've multiple questions or things need to clarify.

Let me type here one by one.

1. Please grant write access to me for explore-mobile repo. I want to send  a PR for nexus change.

2. Code build of mdm with new Nexus server fails, with below complain.
``` org.nlpcn:elasticsearch-sql:jar:2.1.1 not found.```

http://injenkins.fluigdata.com:48080/job/dennytest/4/console

From Jenkins console, I noticed one related warning.
```[WARNING] The POM for org.nlpcn:elasticsearch-sql:jar:2.1.1 is missing, no dependency information available```

The jar file exists in Release hosted repository of old Nexus server. Was this file uploaded manually? Or we need to update java package dependency somewhere? (edited)

3. About "mvn deploy", what's the normal process we run that? Immediately after creating each new active sprint, or before we merge active sprint to master?

4. What about uploading npm modules?
5. I'm trying to get a list of github repos, I should update for Nexus. Currently I know: mdm, totvslabs-framework and explore-mobile. Any other repos I'm missing?

[11:03]
6. where is http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjs?

I don't see it's defined in old Nexus server.

Here are the related I found.
```http://nexus.fluigdata.com:8081/nexus/content/groups/npmjs/
http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjstotvs/
http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjscentral/```

Bruno Volpato	 [11:09 AM]
1. Kung can give you access. But you shouldn't need Write access to open a PR, you can fork the repo and open a PR with basic Read permissions.

2. Kung added this dependency - but if it was in the repo, probably it was deployed manually. This version was never in Maven Central, I can see that there are a couple other versions on Central - http://mvnrepository.com/artifact/org.nlpcn/elasticsearch-sql - for now we can deploy manually, but I would suggest depending on a version that is distributed on central.

3. We are doing in both ways. But ideally we should add an additional step, which is deploy the non-SNAPSHOT version when we merge to master, but it would require any other deployment to go through a hotfix version.

4. @ffox can answer that, but he mentioned that they use NPM to publish, I don't know the steps.

5. + totvslabs-poc

6. This one that you mentioned was replaced by a group, so it really doesn't exist. where did you find this address?

denny zhang	 [11:11 AM]
1. Nice. Didn't remember this trick. I failed to create a new branch, before I send out a PR. Let me fork the repo first.

Bruno Volpato	 [11:12 AM]
yes, should work in your own fork. and people who have write permission can merge your PR

denny zhang	 [11:12 AM]
2. @kungwang, this part is blocking.

Please help me to fix this manual step. Ideally it's better specified in some config file, which will be polled automatically.

Bruno Volpato	 [11:13 AM]
Denny, a quick way to fix #2 is configure the old nexus as proxied in the new one. after a successful build, you should be able to remove the proxy, because it copies locally.

denny zhang	 [11:13 AM]
6. @bruno here is the file pointing that repo: mdm/app/web-apps/npm-shrinkwrap.json (edited)

[11:15]
2. Let me play the trick, to bypass it. After all tests have passed, I will start over again from the very beginning. For that time, we need to resolve it more gracefully.

[11:17]
5. I don't have read access for totvslabs-poc repo. @bruno & @kungwang, could give me that?
#+END_EXAMPLE
*** BYPASS npm-shrinkwrap.json: http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjs
  CLOSED: [2016-11-12 Sat 10:32]
curl -I http://nexus3.fluigdata.com:8081/repository/npmjs-group/postcss-value-parser/-/postcss-value-parser-3.3.0.tgz
curl -I http://nexus3.fluigdata.com:8081/repository/npmjs-central/postcss-value-parser/-/postcss-value-parser-3.3.0.tgz
curl -I http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjs/postcss-value-parser/-/postcss-value-parser-3.3.0.tgz

curl -I -L http://nexus3.fluigdata.com:8081/repository/npmjs-group/postcss-value-parser/-/postcss-value-parser-3.3.0.tgz
curl -I -L http://nexus3.fluigdata.com:8081/repository/npmjs-group/postcss-value-parser/-/postcss-value-parser-3.3.0.tgz

http://nexus3.fluigdata.com:8081/repository/npmjs-group

1. In mdm/app/web-apps/npm-shrinkwrap.json, we have lots of http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjs.

I don't see the repository defined in nexus.fluigdata.com.

Could you help to confirm and make the possible clean up.
One more thing: I have problem to understand mdm/app/web-apps/npm-shrinkwrap.json

Looks like it fetches things from http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjs/postcss-value-parser/...

However I don't see repo of http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjs in our Nexus server.

Here are the related I found.
http://nexus.fluigdata.com:8081/nexus/content/groups/npmjs/
http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjstotvs/
http://nexus.fluigdata.com:8081/nexus/content/repositories/npmjscentral/
**** 2016-11-14: [#A] upgrading our nexus server to the latest
  CLOSED: [2016-11-13 Sun 12:23]
npmjs-denny-proxy
proxy
npm

ln -s /home/nexus/nexus-3.1.0-04/bin/nexus /etc/init.d/nexus
**** Requirement
https://issues.sonatype.org/browse/NEXUS-9741

Kung Wang	 [2:11 PM]
The error they got is similar to this one:
https://issues.sonatype.org/browse/NEXUS-9741

[2:11]
error coming from this repo:
@exponent/ex-navigation

denny zhang	 [2:11 PM]
Yes, I'm running local Nexus setup.

Kung Wang	 [2:11 PM]
it’s their dependencies

[2:12]
you can try this repo to see if you can do npm install to pull from your local nexus

[2:12]
https://github.com/TOTVS/explore-mobile
denny zhang	 [2:13 PM]
**** Install latest Nexus: 3.1.0
docker stop my-test; docker rm my-test;

docker run -t -P -d --name my-test -p 8081:8081 totvslabs/mdm:latest /bin/bash

docker exec -it my-test bash

apt-get install -y wget lsof

wget https://sonatype-download.global.ssl.fastly.net/nexus/3/nexus-3.1.0-04-unix.tar.gz
gzip -d nexus-3.1.0-04-unix.tar.gz
tar -xf nexus-3.1.0-04-unix.tar

cd nexus-3.1.0-04

cd bin
./nexus start
./nexus status
ps -ef | grep nexus
# lsof -P -n -p 232 | grep -i listen
lsof -i tcp:8081
**** checkout npm code
# inject /root/.ssh/id_rsa
apt-get update
apt-get install -y git vim

> /root/.ssh/id_rsa && vim /root/.ssh/id_rsa
# /Users/mac/baidu/百度云同步盘/private_data/emacs_stuff/backup_small/ssh_key/github_id_rsa

chmod 600 /root/.ssh/id_rsa

cd /root/
git clone git@github.com:TOTVS/explore-mobile.git

cd explore-mobile

vim ./.npmrc
# registry=http://localhost:8081/nexus/content/groups/npmjs/

npm install
** 2016-11-14: org.nlpcn:elasticsearch-sql not found issue: manually upload
  CLOSED: [2016-11-14 Mon 12:48]
** 2016-11-14: [#A] 1.49-deploy: sandbox code build fail: http://injenkins.fluigdata.com:48080/job/DockerDeploySandboxActiveSprint/76/console
  CLOSED: [2016-11-14 Mon 13:52]
http://nexus3.fluigdata.com:8081/repository/Snapshots/com/totvslabs/mdm/mdm/1.49.0-SNAPSHOT/maven-metadata.xml

#+BEGIN_EXAMPLE
+ log '================= Build Backend Code ================='
+ local 'msg================== Build Backend Code ================='
++ date '+[%Y-%m-%d %H:%M:%S]'
+ date_timestamp='[2016-11-14 03:15:40]'
+ echo -ne '[2016-11-14 03:15:40] ================= Build Backend Code =================\n'
[2016-11-14 03:15:40] ================= Build Backend Code =================
+ '[' -n '' ']'
+ cd /var/lib/jenkins/code/1.49-devops-1413-new-nexus/mdm
+ pwd
/var/lib/jenkins/code/1.49-devops-1413-new-nexus/mdm
+ true
+ title 'MDM - Build BE - No Tests'
+ unset PROMPT_COMMAND
+ echo -ne '\033]0;MDM - Build BE - No Tests\007'
]0;MDM - Build BE - No Tests+ mvn clean install -Ponejar -DskipTests=true -fae
[INFO] Scanning for projects...
Downloading: http://nexus3.fluigdata.com:8081/repository/Public-Repositories/com/totvslabs/mdm/mdm/1.49.0-SNAPSHOT/maven-metadata.xml

Downloading: http://nexus3.fluigdata.com:8081/repository/Public-Repositories/com/totvslabs/mdm/mdm/1.49.0-SNAPSHOT/mdm-1.49.0-SNAPSHOT.pom

[ERROR] The build could not read 2 projects -> [Help 1]
[ERROR]
[ERROR]   The project com.totvslabs.mdm:mdm-backupserver-plugin:1.49.0-SNAPSHOT (/var/lib/jenkins/code/1.49-devops-1413-new-nexus/mdm/plugins/mdm-backupserver-plugin/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM: Could not find artifact com.totvslabs.mdm:mdm:pom:1.49.0-SNAPSHOT in fluigdata-repository (http://nexus3.fluigdata.com:8081/repository/Public-Repositories/) and 'parent.relativePath' points at wrong local POM @ line 22, column 10 -> [Help 2]
[ERROR]
[ERROR]   The project com.totvslabs.mdm:mdm-smartcontacts-plugin:1.49.0-SNAPSHOT (/var/lib/jenkins/code/1.49-devops-1413-new-nexus/mdm/plugins/mdm-smartcontacts-plugin/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM: Failure to find com.totvslabs.mdm:mdm:pom:1.49.0-SNAPSHOT in http://nexus3.fluigdata.com:8081/repository/Public-Repositories/ was cached in the local repository, resolution will not be reattempted until the update interval of fluigdata-repository has elapsed or updates are forced and 'parent.relativePath' points at wrong local POM @ line 22, column 10 -> [Help 2]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelException
make: *** [all] Error 1
+ shell_exit
+ errorcode=2
+ echo -e '\n\nShow latest commits for TOTVS/mdm'


Show latest commits for TOTVS/mdm
+ git_log /var/lib/jenkins/code/1.49-devops-1413-new-nexus/mdm
+ echo 'check log at dir: /var/lib/jenkins/code/1.49-devops-1413-new-nexus/mdm'
check log at dir: /var/lib/jenkins/code/1.49-devops-1413-new-nexus/mdm
+ local code_dir=/var/lib/jenkins/code/1.49-devops-1413-new-nexus/mdm
+ local tail_count=10
+ cd /var/lib/jenkins/code/1.49-devops-1413-new-nexus/mdm
+ command='git log -n 10 --pretty=format:"%h - %an, %ar : %s"'
+ echo -e '\nShow latest git commits: git log -n 10 --pretty=format:"%h - %an, %ar : %s"'

Show latest git commits: git log -n 10 --pretty=format:"%h - %an, %ar : %s"
+ eval git log -n 10 '--pretty=format:"%h' - %an, %ar : '%s"'
++ git log -n 10 '--pretty=format:%h - %an, %ar : %s'
d9bb2ce - dennyzhang, 2 days ago : replace nexus url with new official server+ echo -e '\n'


+ echo -e '\n\nShow latest commits for TOTVS/totvslabs-framework'


Show latest commits for TOTVS/totvslabs-framework
+ git_log /var/lib/jenkins/code/1.49-devops-1413-new-nexus/totvslabs-framework
+ echo 'check log at dir: /var/lib/jenkins/code/1.49-devops-1413-new-nexus/mdm'
check log at dir: /var/lib/jenkins/code/1.49-devops-1413-new-nexus/mdm
+ local code_dir=/var/lib/jenkins/code/1.49-devops-1413-new-nexus/totvslabs-framework
+ local tail_count=10
+ cd /var/lib/jenkins/code/1.49-devops-1413-new-nexus/totvslabs-framework
+ command='git log -n 10 --pretty=format:"%h - %an, %ar : %s"'
+ echo -e '\nShow latest git commits: git log -n 10 --pretty=format:"%h - %an, %ar : %s"'

Show latest git commits: git log -n 10 --pretty=format:"%h - %an, %ar : %s"
+ eval git log -n 10 '--pretty=format:"%h' - %an, %ar : '%s"'
++ git log -n 10 '--pretty=format:%h - %an, %ar : %s'
0576edd - dennyzhang, 41 minutes ago : revert elasticsearch-sql to avoid possible incompatible issues+ echo -e '\n'


+ '[' 2 -eq 0 ']'
+ echo ERROR
+ exit 1
Build step 'Execute shell' marked build as failure
Finished: FAILURE
#+END_EXAMPLE
** 2016-11-14: kibana report doesn't refresh couchbase metric: since it's buried in elasticsearch metrics
  CLOSED: [2016-11-14 Mon 15:25]
*** old version
#!/bin/bash -e
export DOWNLOAD_TAG_NAME="tag_v2"
curl -o /tmp/db_summary_report.sh https://raw.githubusercontent.com/TOTVS/mdmpublic/2016-06-24/common_bash/report/db_summary_report.sh
bash -e /tmp/db_summary_report.sh

echo "Generate metrics to /opt/devops/"
dst_dir="system_metric"

echo "Get DB Metrics for data volume"
ssh -i /var/lib/jenkins/.ssh/ci_id_rsa -p 4022 root@repo.fluigdata.com "mkdir -p /var/www/repo/${dst_dir}"
scp -i /var/lib/jenkins/.ssh/ci_id_rsa -P 4022 /opt/devops/dump_db_summary/data_out/couchbase_logstash.txt root@repo.fluigdata.com:/var/www/repo/$dst_dir/couchbase_logstash.txt
scp -i /var/lib/jenkins/.ssh/ci_id_rsa -P 4022 /opt/devops/dump_db_summary/data_out/elasticsearch_logstash.txt root@repo.fluigdata.com:/var/www/repo/$dst_dir/elasticsearch_logstash.txt
ssh -i /var/lib/jenkins/.ssh/ci_id_rsa -p 4022 root@repo.fluigdata.com "cd /var/www/repo/$dst_dir; cat couchbase_logstash.txt elasticsearch_logstash.txt > db_summary_report.txt"

# http://repo.fluigdata.com:18000/prodenv_db_summary_report/db_summary_report.txt

echo "Get HAProxy Metrics for http visits"
stat_dir="haproxy_report"
mkdir -p /opt/devops/$stat_dir/data_out
ssh -i /var/lib/jenkins/.ssh/ci_id_rsa root@159.203.198.171 "python /opt/devops/bin/haproxy_stats_metric.py --haproxy_stats_cmd \"echo 'show stat' | nc -U /var/run/haproxy/admin.sock | grep 'backend-https,BACKEND'\"" > /opt/devops/$stat_dir/data_out/$(date +'%Y-%m-%d-%H').txt

cat /opt/devops/$stat_dir/data_out/* > /opt/devops/$stat_dir/haproxy_report.txt
scp -i /var/lib/jenkins/.ssh/ci_id_rsa -P 4022 /opt/devops/$stat_dir/haproxy_report.txt root@repo.fluigdata.com:/var/www/repo/$dst_dir/haproxy_report.txt

echo "Get Infra Metrics for Cost Evaluation"
stat_dir="infra_report"
mkdir -p /opt/devops/$stat_dir/data_out
ssh -i /var/lib/jenkins/.ssh/ci_id_rsa root@138.68.4.184 "python /opt/devops/bin/infra_stats_metric.py" > /opt/devops/$stat_dir/data_out/$(date +'%Y-%m-%d-%H').txt

cat /opt/devops/$stat_dir/data_out/* > /opt/devops/$stat_dir/infra_report.txt
scp -i /var/lib/jenkins/.ssh/ci_id_rsa -P 4022 /opt/devops/$stat_dir/infra_report.txt root@repo.fluigdata.com:/var/www/repo/$dst_dir/infra_report.txt
** 2016-11-14: explore env: nagios alerts threshold for check_elasticsearch_mem and check_mdm_mem
  CLOSED: [2016-11-14 Mon 15:56]
check_elasticsearch_mem
        "elasticsearch_warn_mem_mb":"10240",
        "elasticsearch_cri_mem_mb":"11264",
check_mdm_mem:
        "mdm_warn_mem_mb":"8192",
        "mdm_cri_mem_mb":"9216",


sed -i 's/check_proc_mem.sh -w 6144 -c 7168/check_proc_mem.sh -w 8192 -c 9216/g' /etc/nagios/nrpe.d/mdm_check.cfg

sed -i 's/check_proc_mem.sh -w 6144 -c 7168/check_proc_mem.sh -w 10240 -c 11264/g' /etc/nagios/nrpe.d/elasticsearch_check.cfg

/etc/init.d/nagios-nrpe-server stop
/etc/init.d/nagios-nrpe-server start

http://45.33.104.7:18080/job/RunCommandOnServers/3/console
** 2016-11-14: [[https://trello.com/c/J1pFMzYc][#1255]] fix chef warning in sandbox UpdateSandboxMDM job: http://injenkins.fluigdata.com:48080/job/DockerDeploySandboxActiveSprint/77/console
  CLOSED: [2016-11-14 Mon 16:07]
** 2016-11-14: [[https://trello.com/c/uPS2znek][#1417]] upgrade elasticsearch-sql jar dependency
  CLOSED: [2016-11-14 Mon 19:56]
** 2016-11-15: branch out 1.50 and run all CI tests
  CLOSED: [2016-11-15 Tue 08:10]
** 2016-11-15: enable ThinBackup for Explore Jenkins
  CLOSED: [2016-11-15 Tue 09:31]
** 2016-11-15: add es04 to explore env: 23.239.4.174
  CLOSED: [2016-11-15 Tue 16:32]
https://github.com/TOTVS/mdmdevops/wiki/Add-a-new-elasticsearch-node-to-existing-ES-cluster
curl -XPUT "http://$es_ip:9200/_cluster/settings" -d '
{
 "persistent": {
   "cluster.routing.allocation.enable": "none"
 }
}
'
** 2016-11-15: [[https://trello.com/c/SZdN86NP][#1388]] mdm nagios improvement: mdmbackup and db replica/sharding: http://injenkins.fluigdata.com:48080/job/DockerDeployFeatureCookbooks/73/console
  CLOSED: [2016-11-15 Tue 21:08]
1.49-1388-replica-check

ssh -N -p 22 -f root@172.17.0.7 -L *:8085:localhost:8085 -n /bin/bash

http://injenkins.fluigdata.com:8085/nagios3

# get couchbase bucket replica count
curl http://localhost:8091/pools/default/buckets/mdm-master | python -m json.tool | grep numReplicas

bash /usr/lib/nagios/plugins/check_elasticsearch_replica.sh

chmod 777 /etc/profile.d/es_to_watch.sh
/usr/lib/nagios/plugins/check_nrpe -H 127.0.0.1 -c check_elasticsearch_replica

server_ip="127.0.01"
server_port="8091"
bucket_name="mdm-master"

# get elasticsearch replica count
curl http://localhost:8091/pools/default/buckets/mdm-master | python -m json.tool | grep numReplicas

/Users/mac/baidu/百度云同步盘/private_data/work/totvs_2016/totvs_code/mdmdevops/dev/mdmdevops/cookbooks/nagios-mdm/files/default/check_elasticsearch_replica.sh
** 2016-11-15: [[https://github.com/TOTVS/mdmdevops/wiki/Add-a-new-elasticsearch-node-to-existing-ES-cluster][add wiki]]: Add a new elasticsearch node to existing ES cluster
  CLOSED: [2016-11-15 Tue 21:08]
** 2016-11-16: bug fixing for Jenkins parameter of explore env
  CLOSED: [2016-11-16 Wed 15:01]
Wrong parameter

[3:01]
Make below change. It should work now.

Denny-mac:~ mac$ diff tmp tmp1
47,48c47,48
<         {"maxSimultaneousThreads":"12",
<         "maxTotalThreads":"64",
---
>         {"max_simultaneous_thread":"12",
>         "max_total_threads":"64",
** 2016-11-16: add app_mdm attributes to keep config file in sync
  CLOSED: [2016-11-16 Wed 15:07]
http://injenkins.fluigdata.com:48080/job/MonitorGitFileChangesMDMActiveSprint/233/console
** 2016-11-17: Update Jenkins for explore env: enable JobConfigHistory and ThinBackup. And update Jenkins docker image
  CLOSED: [2016-11-17 Thu 11:01]
** 2016-11-17: check: new servers for explore env
  CLOSED: [2016-11-17 Thu 08:25]
Kung Wang	 [1:19 AM]
new servers: Explore App Worker2, Explore App Worker3 and Explore ES5 are created, now let me do pre-configuration

[1:19]
https://github.com/TOTVS/mdmdevops/wiki/MDM-Server-List

[1:19]
Host exploreworker2
   HostName 23.92.24.214
   Port 22
   User root
   IdentityFile ~/.ssh/id_rsa
   StrictHostKeyChecking no

Host exploreworker3
   HostName 23.239.4.218
   Port 22
   User root
   IdentityFile ~/.ssh/id_rsa
   StrictHostKeyChecking no

Host explorees5
   HostName 50.116.15.14
   Port 22
   User root
   IdentityFile ~/.ssh/id_rsa
   StrictHostKeyChecking no (edited)
** 2016-11-17: [[https://trello.com/c/z8PxhhBi][#1451]] Update nagios to raise critical mdm log messages to #devops slack channel: http://injenkins.fluigdata.com:48080/job/DockerDeployAllInOne/63/console
  CLOSED: [2016-11-17 Thu 11:46]
ssh -N -p 22 -i /tmp/id_rsa -f root@172.17.0.5 -L *:8085:localhost:8085 -n /bin/bash

echo "java.lang.OutOfMemoryError: Java heap space" >> /opt/mdm/logs/mdm-initscript.log

echo "ab" >> /opt/mdm/logs/mdm-initscript.log
** 2016-11-18: [#A] Nexus code build failure                              :IMPORTANT:
  CLOSED: [2016-11-18 Fri 08:34]
Bruno Volpato	 [11:24 AM]
created the branch early Monday PST

[11:24]
based on 1.49

[11:24]
did you had your changes at branch 1.49 at that time?

denny zhang	 [11:24 AM]
No, I haven't

Bruno Volpato	 [11:24 AM]
so yes, this is why.

[11:25]
let me fix it

[11:25]
oh you already issued the deploy, so it should be fine now

denny zhang	 [11:25 AM]
Bruno, could you please help me to figure out the detail process.

[11:26]
Previously do we need to run "mvn deploy" whenever we have a new active sprint?

If yes, what's our process.

Bruno Volpato	 [11:49 AM]
every time we move the version to next sprint, we run ./versionBump.sh {newVersion}

[11:49]
and this command already issues deploy

[11:49]
this time we deployed too, but probably it was still pointing to the old Nexus, this is why it was missing.

denny zhang	 [11:50 AM]
So, it should be: ./versionBump.sh 1.50. Right?

Bruno Volpato	 [11:50 AM]
`/versionBump.sh 1.50.0-SNAPSHOT`

denny zhang	 [11:50 AM]
Nice. Do we need to build code, before run this?
*** error1
#+BEGIN_EXAMPLE
denny zhang	 [11:03 AM]
Hi Bruno

One question about Nexus.

Whenever we have a new active sprint, do we need to run "mvn deploy" manually?

[11:04]
I got this for 1.50 sandbox test.
http://injenkins.fluigdata.com:52080/job/BuildMDMRepo/1/console

I'm pretty sure if I run "mvn deploy", the problem will go away. Want to check with you for advice.

[11:04]
[ERROR] The build could not read 2 projects -> [Help 1]
[ERROR]
[ERROR]   The project com.totvslabs.mdm:mdm-backupserver-plugin:1.50.0-SNAPSHOT (/var/lib/jenkins/code/1.50/mdm/plugins/mdm-backupserver-plugin/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM: Could not find artifact com.totvslabs.mdm:mdm:pom:1.50.0-SNAPSHOT in fluigdata-repository (http://nexus3.fluigdata.com:8081/repository/Public-Repositories/) and 'parent.relativePath' points at wrong local POM @ line 22, column 10 -> [Help 2]
[ERROR]
[ERROR]   The project com.totvslabs.mdm:mdm-smartcontacts-plugin:1.50.0-SNAPSHOT (/var/lib/jenkins/code/1.50/mdm/plugins/mdm-smartcontacts-plugin/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM: Failure to find com.totvslabs.mdm:mdm:pom:1.50.0-SNAPSHOT in http://nexus3.fluigdata.com:8081/repository/Public-Repositories/ was cached in the local repository, resolution will not be reattempted until the update interval of fluigdata-repository has elapsed or updates are forced and 'parent.relativePath' points at wrong local POM @ line 22, column 10 -> [Help 2]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
#+END_EXAMPLE
*** error2
#+BEGIN_EXAMPLE
[INFO] Total time: 2:50.002s
[INFO] Finished at: Wed Nov 16 03:15:41 UTC 2016
[INFO] Final Memory: 138M/2570M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.5.1:compile (default-compile) on project mdm-backupserver-plugin: Compilation failure -> [Help 1]
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-resources-plugin:3.0.1:resources (default-resources) on project mdm-smartcontacts-plugin: Cannot create resource output directory: /var/lib/jenkins/code/1.50/mdm/plugins/mdm-smartcontacts-plugin/target/classes -> [Help 2]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :mdm-backupserver-plugin
make: *** [all] Error 1
+ shell_exit
#+END_EXAMPLE
** 2016-11-18: [[https://github.com/TOTVS/mdmdevops/wiki/Procedure-Of-Creating-A-new-Active-Sprint][add wiki]]: Procedure Of Creating A new Active Sprint
  CLOSED: [2016-11-18 Fri 09:55]
** 2016-11-18: Reconfigure Jenkins prod and explore env: Redirect all alerts to #fd-alerts slack channels
  CLOSED: [2016-11-18 Fri 13:38]
devops.totvslabs@dennyzhang.com

totvslabs
KcJYiuEaFzgAx22cDreRfUkN
#fd-alerts
** 2016-11-18: [[https://trello.com/c/gkMVyZ6L][#1452]] In Linode envs, if hostname is still ubuntu, chef code refuse to run
  CLOSED: [2016-11-18 Fri 15:53]
Kung Wang	 [2:43 AM]
Denny, when i deploy a new ES server this morning, I see this in /etc/elasticsearch.yml file:

node.name: ubuntu
network.host: ubuntu

they should be
node.name: explorees5
network.host: explorees5

denny zhang	 [7:44 AM]
Hi Kung

I noticed whenever we start a new Linode env, the hostname is ubuntu.

We will need to reconfigure the hostname, before running the deployment.

Would it be related to the issue you mentioned, Kung?

denny zhang	 [7:51 AM]
BTW, I will have this improvement:
- In Linodes env, if hostname is "ubuntu", chef code will refuse to run. And show an error message.
** #  --8<-------------------------- separator ------------------------>8--
** 2016-11-21: [[https://trello.com/c/UnEH7wbE][#1463]] Send slack notification, whenever we have a new active sprint.
  CLOSED: [2016-11-21 Mon 15:24]
https://trello.com/c/UnEH7wbE
http://jerrygamblin.com/2016/11/12/automated-burp-suite-scanning-and-reporting-to-slack/
** 2016-11-21: [[https://github.com/TOTVS/mdmdevops/wiki/How-To-Update-Explore-env][Add wiki]]: How To Update Explore env
  CLOSED: [2016-11-21 Mon 17:43]
** 2016-11-21: create a Jenkins user for mitu
  CLOSED: [2016-11-21 Mon 17:45]

Hi mitu

I've created users for you. Please check.

mitu/Foobar1!
- Prod env: http://prodjenkins.fluigdata.com:18080
- Explore env: http://45.33.104.7:18080
** 2016-11-22: branch out 1.51 and run all CI tests
  CLOSED: [2016-11-22 Tue 10:49]
** 2016-11-23: mdm.yml add index.max_result_window attribute; update dns
  CLOSED: [2016-11-23 Wed 08:23]
** 2016-11-23: [[https://trello.com/c/IdMqjkr2][#138]] Reported bug: fresh deployment fails at flushDatabasesAndLoadSampleData in 1.51
  CLOSED: [2016-11-23 Wed 10:19]
With a fresh all-in-one or sandbox deployment of 1.51, mdm will fail to start.

Before yesterday, the same testcases look fine.

People should be able to reproduce it in local sandbox: Run UpdateSandboxMDM job with db flush enabled.

Quoted from /opt/mdm/logs/mdm-initscript.log
```
[22 Nov 2016;12:41:37.808]-[WARN ][MdmApplication - run:424 - main - T_mdm] - Exception while Flushing and Reloading Database:  ! com.totvslabs.fr
amework.core.common.exceptions.ApplicationException: {"errorCode":500,"responsibleField":""}
! at com.totvslabs.framework.core.common.exceptions.ServiceExceptionHelper.get(ServiceExceptionHelper.java:66)
! at com.totvslabs.framework.core.common.exceptions.ServiceExceptionHelper.get(ServiceExceptionHelper.java:47)
! at com.totvslabs.mdm.app.server.MdmApplication.flushDatabasesAndLoadSampleData(MdmApplication.java:257)
! at com.totvslabs.mdm.app.server.MdmApplication.run(MdmApplication.java:422)
! at com.totvslabs.mdm.app.server.MdmApplication.run(MdmApplication.java:1)
! at io.dropwizard.cli.EnvironmentCommand.run(EnvironmentCommand.java:40)
! at io.dropwizard.cli.ConfiguredCommand.run(ConfiguredCommand.java:77)
! at io.dropwizard.cli.Cli.run(Cli.java:70)
! at io.dropwizard.Application.run(Application.java:80)
! at com.totvslabs.mdm.app.server.MdmApplication.main(MdmApplication.java:939)
! Caused by: java.lang.NullPointerException: null
! at com.totvslabs.mdm.app.server.MdmApplication.flushDatabasesAndLoadSampleData(MdmApplication.java:255)
! ... 7 common frames omitted
```
** 2016-11-24: PR for dns change
  CLOSED: [2016-11-24 Thu 08:45]
cd /Users/mac/baidu/百度云同步盘/private_data/work/totvs_2016/totvs_code/mdm
git clone git@github.com:TOTVS/mdm.git

45.33.104.7
explorejenkins.fluigdata.com


Denny-mac:mdm mac$ git diff
diff --git a/app/mdm.yml b/app/mdm.yml
index c6d586d..14426f0 100755
--- a/app/mdm.yml
+++ b/app/mdm.yml
@@ -384,6 +384,7 @@ awsConfiguration:
             {recordKey: "mv", recordValue: "12.145.25.178", recordType: "A"},
             {recordKey: "admin", recordValue: "fluigdata.com.", recordType: "CNAME"},
             {recordKey: "www", recordValue: "fluigdata.com.", recordType: "CNAME"},
+            {recordKey: "explorejenkins", recordValue: "45.33.104.7", recordType: "A"},
             {recordKey: "injenkins", recordValue: "45.33.87.74", recordType: "A"},
             {recordKey: "inkibana", recordValue: "45.79.184.42", recordType: "A"}
         ]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-11-24: Get Slack Notifications For Config Files Changes
  CLOSED: [2016-11-24 Thu 13:03]
** 2016-11-24: enable es node check again: http://injenkins.fluigdata.com:48080/job/DockerDeployFeatureCookbooks/94/console
  CLOSED: [2016-11-24 Thu 14:14]
** 2016-11-24: update parameter RunCommandOnServers for explore jenkins
  CLOSED: [2016-11-24 Thu 16:04]
http://explorejenkins.fluigdata.com:18080/job/RunCommandOnServers/build?delay=0sec
** 2016-11-24: shutdown mdm instances: http://explorejenkins.fluigdata.com:18080/job/RunCommandOnServers/6/console
  CLOSED: [2016-11-24 Thu 16:04]
** 2016-11-24: low disk issue for explore env
  CLOSED: [2016-11-24 Thu 15:58]
** 2016-11-24: explore env need to update nagios
  CLOSED: [2016-11-24 Thu 16:08]
** 2016-11-24: switch uptimerobot emails alerts to slack alerts
  CLOSED: [2016-11-24 Thu 17:58]
** 2016-11-24: handle Linode disk replacement issue: back and restart inkibana service
  CLOSED: [2016-11-24 Thu 21:16]
** 2016-11-24: add explorees6 to explore env
  CLOSED: [2016-11-24 Thu 21:22]
explorees6

45.33.43.184
** 2016-11-27: prod env: update jenkins; disk capacity
  CLOSED: [2016-11-27 Sun 19:20]
 1) Command "[ $(ssh -i /var/lib/jenkins/.ssh/ci_id_rsa root@138.68.4.184 df -h /mnt/cb-backup-01 | tail -n1 |awk -F' ' '{print $4}' | awk -F'G' '{print $1}' | awk -F'.' '{print $1}') -gt 60 ]" exit_status should eq 0
2016-11-27 05:27:05      On host `localhost'
2016-11-27 05:27:05      Failure/Error: its(:exit_status) { should eq 0 }
2016-11-27 05:27:05
2016-11-27 05:27:05        expected: 0
2016-11-27 05:27:05             got: 1
2016-11-27 05:27:05
2016-11-27 05:27:05        (compared using ==)
** #  --8<-------------------------- separator ------------------------>8--
** 2016-11-28: add es07 to explore env
  CLOSED: [2016-11-28 Mon 12:05]
** 2016-11-29: Add es07, es08, and cb05 to prod env
  CLOSED: [2016-11-29 Tue 09:58]
** 2016-11-28: cherry-pick for mdm private key security improvement
  CLOSED: [2016-11-28 Mon 17:21]
** 2016-11-29: branch out 1.52 and run all CI tests
  CLOSED: [2016-11-29 Tue 08:10]
** 2016-11-29: [[https://trello.com/c/N5nlPQms][#1538]] Get slack notification, when es cluster is yellow or red: http://injenkins.fluigdata.com:48080/job/DockerDeployFeatureCookbooks/12/console
  CLOSED: [2016-11-29 Tue 08:11]
export branch_name=1.51-deploy
export KEEP_FAILED_INSTANCE=false
export KEEP_INSTANCE=true

export TEST_KITCHEN_YAML=".kitchen.yml"
# export DOCKER_PORT_FORWARD_PREFIX=31
export PACKAGE_URL='http://172.17.0.1:18000'
export APP_BRANCH_NAME=1.51
export FRAMEWORK_BRANCH_NAME=1.51
export CLUSTER_ID=kitchen-mdm-feature

cd /var/lib/jenkins/code/DockerDeployFeatureCookbooks/1.51-deploy/mdmdevops/cookbooks/mdm-cluster

export INSTANCE_NAME=mdm-cluster-DockerDeployFeatureCookbooks-11

export KITCHEN_YAML=.kitchen.yml
** 2016-11-29: Decommission old nexus server safely
  CLOSED: [2016-11-29 Tue 11:04]
** 2016-11-29: Fix code build failure for nexus setting
  CLOSED: [2016-11-29 Tue 11:17]
Bruno Volpato [9:35 AM]
@denny.zhang yes I ran

[9:35]
but I can't go to the console and see why it failed.

[9:35]
Access Denied

mdmadmin is missing the Overall/Read permission

[9:37]
@denny.zhang it seems in that error that the app was going to the old nexus

[9:38]
but when I deploy I do to the new one

[9:38]
 ```2016-11-29 00:15:18 [INFO] Scanning for projects...
2016-11-29 00:15:19 Downloading: http://nexus.fluigdata.com:8081/nexus/content/groups/public/com/totvslabs/mdm/mdm/1.52.0-SNAPSHOT/maven-metadata.xml
2016-11-29 00:15:19
Downloading: http://nexus.fluigdata.com:8081/nexus/content/groups/public/com/totvslabs/mdm/mdm/1.52.0-SNAPSHOT/mdm-1.52.0-SNAPSHOT.pom
2016-11-29 00:15:19
[ERROR] The build could not read 2 projects -> [Help 1]
2016-11-29 00:15:19 [ERROR]
2016-11-29 00:15:19 [ERROR]   The project com.totvslabs.mdm:mdm-backupserver-plugin:1.52.0-SNAPSHOT (/var/lib/jenkins/jobs/BuildMDMRepoCodeActiveSprint/workspace/1.52/mdm/plugins/mdm-backupserver-plugin/pom.xml) has 1 error
2016-11-29 00:15:19 [ERROR]     Non-resolvable parent POM: Could not find artifact com.totvslabs.mdm:mdm:pom:1.52.0-SNAPSHOT in fluigdata-repository (http://nexus.fluigdata.com:8081/nexus/content/groups/public/) and 'parent.relativePath' points at wrong local POM @ line 22, column 10 -> [Help 2]
```

[9:38]
am I missing something?

denny zhang [9:39 AM]
Bruno, I'm adding one more ES node to prod env.

When I'm done, let me go back to this. OK?
** 2016-11-30: Forced reboot by Linode: handle http://inkibana.fluigdata.com outage
  CLOSED: [2016-11-30 Wed 09:31]
** 2016-12-01: Change all automation code to use port 2702 for ssh connections
  CLOSED: [2016-12-01 Thu 11:17]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-12-01: [[https://trello.com/c/oOys2bmN][#1515]] For prod/explore env, configure sshd to listen on port 2702, instead of 22
  CLOSED: [2016-12-01 Thu 11:51]
https://trello.com/c/oOys2bmN

I'd like to change sshd daemon for prod env and explore envs on this Saturday 23:00 am.
Please raise your concern in advance.

The risk is low. Mainly for security improvement.
*** Reconfigure sshd in each server and restart the sshd daemon
fname="/etc/ssh/sshd_config"
cp "$fname" "${fname}_bak_$(date +'%Y-%m-%d_%H%M%S')"
ls -lths $fname*

sed -i 's/Port 22/Port 2702/g' $fname
grep "^Port " $fname

service ssh restart
http://explorejenkins.fluigdata.com:18080/job/RunCommandOnServers/18/console
*** Update firewall: allow incoming traffic of 2702, and forbid port 22
ufw allow 2702/tcp
http://explorejenkins.fluigdata.com:18080/job/RunCommandOnServers/15/console

http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/117/console
*** Change Jenkins jobs of DeploySystem and others.
** 2016-12-01: move TCPScanReport to a shared Jenkins: http://repo.fluigdata.com:18080/job/TCPScanReport/5/console
  CLOSED: [2016-12-01 Thu 13:05]
remove jenkins job http://prodjenkins.fluigdata.com:18080/view/Security/job/TCPScanReport/
** 2016-12-01: In prod and explore env, clean up garbage files for /tmp/run_command_on_servers.sh_*
  CLOSED: [2016-12-01 Thu 17:03]
rm -rf /tmp/run_command_on_servers.sh_*
ls -lth /tmp/
** 2016-12-02: [[https://trello.com/c/3p1zkszz][1516]] Automate docker images build verification: http://injenkins.fluigdata.com:48080/job/BuidAllDockerImages/21/console
  CLOSED: [2016-12-02 Fri 08:27]
Currently we leverage  a couple of self-created docker images for daily CI.

https://github.com/TOTVS/mdmdevops/wiki/customized-docker-images

A successful docker image build depends on multiple moving parts. It's better we define a weekly jenkins job and automate the image build verification.

docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test totvslabs/docker:v1.0 /bin/bash
docker exec -it my-test bash

docker stop my-test; docker rm my-test
docker run -t -d -h mytest --name my-test totvslabs/docker:v1.0 /bin/bash
docker exec -it my-test bash

/bin/sh -c '(service docker start || true) &&   sleep 5 && docker pull ubuntu:14.04'

docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test ubuntu:14.04 /bin/bash
docker exec -it my-test bash

docker rmi totvslabs/docker:v1.0

cd /root/
git clone https://github.com/TOTVS/mdmpublic.git
cd /root/mdmpublic/docker/Dockerfile
make test
*** add resource file
mkdir -p /root/docker_in_docker/resources

cd  /root/docker_in_docker/resources
vim jenkins_credential
vim github_id_rsa
*** save docker golden image
mkdir -p /root/docker_in_docker
docker save ubuntu:14.04 > /root/docker_in_docker/ubuntu_14.04.tar.gz
ls -lth /root/docker_in_docker/ubuntu_14.04.tar.gz
*** start docker container for dind
docker stop docker-images; docker rm docker-images
docker run -t -d --privileged -h mytest --name docker-images  -v /root/docker_in_docker:/root/docker_in_docker totvslabs/docker:v1.0 /usr/bin/dockerd
docker exec -it docker-images bash
*** load docker image
docker load -i /root/docker_in_docker/ubuntu_14.04.tar.gz
*** run docker image build test
cd /root/
git clone https://github.com/TOTVS/mdmpublic.git
cd /root/mdmpublic/docker/Dockerfile
*** copy resource
cp -r /root/docker_in_docker/resources/* /root/mdmpublic/docker/Dockerfile/resources/
**** 2016-12-02: dockerfile build with name convention
    CLOSED: [2016-11-25 Fri 18:06]
**** 2016-12-02: build images
   CLOSED: [2016-11-25 Fri 21:17]
cd /root/mdmpublic/docker/Dockerfile

git pull

make test
** 2016-12-02: add commonservercheck for explore jenkins: http://explorejenkins.fluigdata.com:18080/job/CommonServerCheck/1/console
  CLOSED: [2016-12-02 Fri 10:18]
** 2016-12-03: Redirect all major nagios notifications from #explore_env_alert  to #fd-explore
  CLOSED: [2016-12-03 Sat 08:58]
** 2016-12-03: Bug fix: UX sandbox test case for ssh key issue
  CLOSED: [2016-12-03 Sat 22:03]
** 2016-12-04: add cb04 and es08 to explore env
  CLOSED: [2016-12-04 Sun 21:22]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-12-06: [[https://trello.com/c/BCS6CpDN][#1428]] Get slack notification for es slow query or slow index actions
  CLOSED: [2016-11-29 Tue 12:15]
** 2016-12-06: Nagios process memory check improvement: only check residential memory
  CLOSED: [2016-12-06 Tue 21:49]
Bruno Volpato [4:24 PM]
@denny.zhang this virtual memory check is very noisy... and I don't see ES memory really causing issues

[4:24]
it allocates memory because of the heap size defined to Java... not that it is really blowing up memory

[4:25]
any suggestions to improve?


Nagios BOT [4:54 PM]
192.241.203.166/check_elasticsearch_mem is WARNING:
Memory: WARNING VIRT: 146176 MB - RES: 28654 MB used!

[4:59]
192.241.203.166/check_elasticsearch_mem is CRITICAL:
Memory: CRITICAL VIRT: 146176 MB - RES: 28680 MB used!

denny zhang [5:30 PM]
Bruno, I agree. Let me skip the check of virtual memory in nagios

denny zhang [5:41 PM]
Should be able to finish with other improvement late this week
** 2016-12-06: Update wiki for [[https://github.com/TOTVS/mdmdevops/wiki/Nagios-Monitoring-System][Nagios]] and [[https://github.com/TOTVS/mdmdevops/wiki/load-balancer][load balancer]] FAQ
  CLOSED: [2016-12-06 Tue 21:55]
** 2016-12-06: [[https://trello.com/c/hEwtaHfR][#1591]] Nagios raise alerts using hostname, instead of ip address: http://injenkins.fluigdata.com:48080/job/DockerDeployFeatureCookbooks/29/console
  CLOSED: [2016-12-06 Tue 23:57]
** 2016-12-07: ssh port not changed to 2702
  CLOSED: [2016-12-07 Wed 09:51]
** 2016-12-07: add es09 to prod env
  CLOSED: [2016-12-07 Wed 09:52]
198.199.95.111
** 2016-12-07: [[https://trello.com/c/Y3qdpqNp][#1588]] Deployment with flag for whether to skip the update of mdm.yml
  CLOSED: [2016-12-07 Wed 22:08]
update doc

if node['app_mdm']['skip_mdm_yml_update'] == 'false'
** 2016-12-08: branch out 1.53
  CLOSED: [2016-12-08 Thu 07:41]
** 2016-12-08: [[https://trello.com/c/HHCjYuBU][#154]] UpdateSandboxMDM fail at the first time: http://injenkins.fluigdata.com:48080/job/DockerDeploySandboxActiveSprint/38/console
  CLOSED: [2016-12-08 Thu 16:52]
Kung Wang [9:36 AM]
Denny, when Vicente clean up his sandbox and redo everything again, we found(it happens to me as well) on UpdateSandboxMDM it will fail on first time, but 2nd time will be ok

[9:36]
we need to fix this ASAP

[9:36]
it gives people sandbox always not stable

denny zhang [9:44 AM]
OK. Will look into it.
** 2016-12-08: verify nagios output: /usr/lib/nagios/plugins/check_nrpe -H 127.0.0.1 -c check_mdm_mem
  CLOSED: [2016-12-08 Thu 17:17]
** 2016-12-08: Support prod env issue with app01, app02 and app03
  CLOSED: [2016-12-08 Thu 20:38]
** 2016-12-08: [[https://trello.com/c/BC6kAKFI][#153]] Sandbox build fails when user add an extra space to the UpdateJenkinsItself job
  CLOSED: [2016-12-08 Thu 22:42]
** 2016-12-09: prod env add 2 es nodes, 1 cb node, and 2 workers
  CLOSED: [2016-12-09 Fri 08:57]
| prod-cb-6                |   104.236.129.147 | 12 Core/ 32 GB Memory / 320 GB Disk / SFO1   | Couchbase                           |
| prod-es-10               |   104.236.187.173 | 12 Core/ 32 GB Memory / 320 GB Disk / SFO1   | Elasticsearch                       |
| prod-es-11               |   107.170.252.123 | 12 Core/ 32 GB Memory / 320 GB Disk / SFO1   | Elasticsearch                       |
| prod-app-5               |     45.55.1.180 | 8 Core/ 16 GB Memory / 160 GB Disk / SFO1    | APP(Not recognized by LoadBalancer) |
| prod-app-6               |     45.55.9.157 | 8 Core/ 16 GB Memory / 160 GB Disk / SFO1    | APP(Not recognized by LoadBalancer) |
** 2016-12-09: verify data rebalance for es10 and es11 of prod env
  CLOSED: [2016-12-09 Fri 13:14]
| prod-es-10               |   104.236.187.173 | 12 Core/ 32 GB Memory / 320 GB Disk / SFO1   | Elasticsearch                       |
| prod-es-11               |   107.170.252.123 | 12 Core/ 32 GB Memory / 320 GB Disk / SFO1   | Elasticsearch                       |
** 2016-12-09: [[https://github.com/TOTVS/mdmdevops/wiki/Procedure-Of-Merging-ActiveSprint-To-Master][Add wiki]]: Procedure Of Merging ActiveSprint To Master
  CLOSED: [2016-12-09 Fri 13:21]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-12-11: add es11 and es12 to explore env: http://explorejenkins.fluigdata.com:18080/job/DeploySystem/51/console
  CLOSED: [2016-12-11 Sun 09:00]
104.237.158.48:2702
173.255.248.200:2702
** 2016-12-11: Explore env/Prod env: Updating nagios to use hostname, instead of ip address: http://prodjenkins.fluigdata.com:18080/job/DeploySystem/114/console
  CLOSED: [2016-12-11 Sun 09:00]
** 2016-12-12: [[https://github.com/TOTVS/mdmdevops/wiki/MDM-Server-List][Update wiki]]: Roboson kibana support
  CLOSED: [2016-12-12 Mon 09:13]
** 2016-12-12: add cb07 and es13 to explore env
  CLOSED: [2016-12-12 Mon 16:58]
5Kb8tJbF

linode2556192 45.79.70.179
linode2556207 45.33.44.216

explorees13
** 2016-12-12: add prod-cb-7 and prod-es-12 to prod env: http://prodjenkins.fluigdata.com:18080/job/DeploySystem/115/console
  CLOSED: [2016-12-12 Mon 19:03]
192.241.225.40:2702
192.241.228.149:2702
** 2016-12-12: add explorecb8 to explore cluster
  CLOSED: [2016-12-12 Mon 19:19]
linode2557431 74.207.244.89

74.207.244.89:2702

5Kb8tJbF
explorecb7

ssh -p 2702 root@45.79.70.179 cat /root/.ssh/authorized_keys

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCneF004Ajy1HZSNhwo2TDU88DCOOOTqdONL9orwvOFdymdplRdLFTmZQhKRr32PsyyVMMpggzIyojkOdUCGjgBSDKDGiFONU1V+30SULTU3vVqYDPwMjyk2e46VN4JkUIjjCEtUFmcIJ0Mx4z/87/M69Fd8r64QoFvLQO5ZZSW/X9zWKeeO471Xab7os7WZjEiz7cW9YTVPnwY4JOZdyDnaOrmSeQ3vAmO0h7TvSYIvPUrl9Tiot/iPKMmp+s77VQI8lf6KHC3ec3GHuutqKov8Gwbxq/B7Px3UUulY86zgTFs+QCpHROwRc+z5hMuLLFLf2C0vwFG9ea2MXE/jIt7 kung.wang@totvs.com
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDLBVrkveHzgHkTY3jeE3MfvO0TWQhcYoWNDYD/gFqiQPs5TlunvKvW9aXsHa05Hl2qjZ/JICPXSs9OQNpsYat169gwE5n2P7k2Yg07XnRnryAwTrTSyNPkwBcEwYpL3QaJxVt4zERKLcBvYbNw/i6onnxVoK/CSS2Nyq7ok3q9AWv5armxWvMgg11eeaC9lL0YQnQylpk1BF1HHsDizX+p8EPmseGEs/gOUTB5mn6us/yTtQHT/gFuu5pFkLire37MtCiWga8qZYSRuaZ7i1+Of4xlhapK0tn6ii402dXuv15AG35o0PBCQL7BT//V3oc6NBXNgyw5pQrbKjT+OD7p auto.devops@totvs.com
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCVG3i3ZsLnv/3SJDX0a/RADDDOr4kc6gQkhPYhIGQGURs69V/tmW6zaGGmcDRDEh9hXQ6xvF8FZhpHClgfHFbPnEENm2H1khxpwywXXPMWKeq8hPXW0FzF5b3jP6Y945t4ciJPq9kYOz3xjzBNVXy1Bui9Vl1DXuqYFW8SF6SfIc8vIHH/r7PaKzp7ziKubfjBXVoFfv+oflMEFo/mMhMt53VEGhTbq1ZYxPpmtTGOgmmo/c6iEn+IU3/LoAAwR9yaYsW8z0qCU1ecX+4NKM0ejGO5FnWnFaAxsmg5oz19rHonIz6MwIKh8HGN+6DDgI45R0qUvw9LRxjTWNFF9L7l denny@dennyzhang.com
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDgIRsZ+r4Ycj73pzcITmaLbBF3hRCpNnMYeY/j0UPT8xDwEltKEL1TIoqIZWv+SkEv4PqGo1Cw+6VQX21RfLD/gx10yA1S/fqOEMSp6U6L+m/T+PkZ/uNaple+oUrKfjKAPB+V+sxeH38xWktdkZsTPr5Gqb3ngkrCt46me/UPxwtA0/1DuPY1hgZClmp/EbgLScT8FpNBEwdJO+CEx3H1BFqzG2UgbxRgvVnGjrJmTIwKppOC3cuU8rhbTP+jeIE+kJaD47lFqV+K44uvCQQeQ6g3Vjp3Q4HZ0FO7SxEp9Gor25yHOQrlfdlmWjEZFsliwNRRTAp1Yj+uT98EEKWr brunocvcunha@gmail.com
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDkl9FWkhFgXJj4MqqOcI69pKl5/G8MXI/YiH0qpDA36Alpmw7GzbFBjdseSQgb1uBrrXCXMICCHjgpsO+nkfqZRH82WkYJTumKVK/oBh1h8gCOqxlCUOKioJzFcHSimR7SEcdw4tuba6Lriv7Nyc6u/1WOZMuOxxV2sRYgpFbBFVl81fisuaabJQIwtcTOkpiYIRLXSPQN97THSXImD+dgFd+bu3JTwPzARlK3uODGXj5Kr7MskJm/WKbQFfhg5gVyky04pwHXFkbcCv+C7yMjTzmS27dMPmWPZP+b/XBXRnCU2QELbAp3KJc5g4gDuDqKqW+wp9XMqAjqvDPa2kXv mitusingh27@yahoo.com
** 2016-12-12: add cb06 back and join db cluster: http://explorejenkins.fluigdata.com:18080/job/DeploySystemRehearsal/31/console
  CLOSED: [2016-12-12 Mon 23:20]
66.175.223.247:2702

ssh -p 2702 root@66.175.223.247

/etc/init.d/couchbase-server status

explorecb6
** 2016-12-13: branch out 1.54 and run all CI tests
  CLOSED: [2016-12-13 Tue 07:35]
** 2016-12-13: add explorees14 to explore env
  CLOSED: [2016-12-13 Tue 08:50]
explorees14
linode2559259 45.33.37.12
** 2016-12-13: [#A] disable swap for explore env: http://explorejenkins.fluigdata.com:18080/job/RunCommandOnServers/49/console
  CLOSED: [2016-12-13 Tue 08:50]
** 2016-12-13: Linode Dashboard update: 2 Linode VMs are missing from Dashboard
  CLOSED: [2016-12-13 Tue 09:38]
linode2539005	Chinwei	45.33.61.162	6 Cores / 12G RAM / 192G Disk / 8TB XFER	explorecb5
linode2539017	Chinwei	66.175.223.247	6 Cores / 12G RAM / 192G Disk / 8TB XFER	explorecb6
** 2016-12-13: add update_ubuntu_system.sh
  CLOSED: [2016-12-13 Tue 10:25]
** 2016-12-13: Add prod-es-13 to prod env
  CLOSED: [2016-12-13 Tue 10:28]
192.241.206.113:2702
** 2016-12-13: [[https://trello.com/c/mhhRUDea][#1464]] Linode check_swap_usage nagios warning in explore env
  CLOSED: [2016-12-13 Tue 11:46]
why ram available but swap all taken

cd /etc/nagios/nrpe.d
grep check_swap_usage ./ -r

/usr/lib/nagios/plugins/check_swap -w 20% -c 10%

/proc/swaps

linux check swap
#+BEGIN_EXAMPLE
root@explorees1:/etc/nagios/nrpe.d# /usr/lib/nagios/plugins/check_swap -w 20% -c 10%
/usr/lib/nagios/plugins/check_swap -w 20% -c 10%
SWAP CRITICAL - 1% free (0 MB out of 255 MB) |swap=0MB;51;25;0;255
#+END_EXAMPLE

#+BEGIN_EXAMPLE
root@explorees1:/etc/nagios/nrpe.d# df -h
df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/root       189G  141G   39G  79% /
devtmpfs        5.9G  4.0K  5.9G   1% /dev
none            4.0K     0  4.0K   0% /sys/fs/cgroup
none            1.2G  348K  1.2G   1% /run
none            5.0M     0  5.0M   0% /run/lock
none            5.9G     0  5.9G   0% /run/shm
none            100M     0  100M   0% /run/user
#+END_EXAMPLE
** 2016-12-13: Only use swap, when all RAM is occupied
  CLOSED: [2016-12-13 Tue 12:40]
** 2016-12-13: update default package url for docker image: docker push; upload: http://injenkins.fluigdata.com:48080/job/DockerDeployAllInOne/52/console
  CLOSED: [2016-12-13 Tue 19:10]
docker run -t -P -d --name my-test -p 18080:18080 totvslabs/mdm:v1.0 /bin/bash
docker exec -it my-test bash

export working_dir=/root/
export devops_git_repo="git@github.com:TOTVS/mdmdevops.git"
export devops_repo_name="mdmdevops"
export chef_version="12.16.42"
export devops_branch="master"
docker pull totvslabs/mdm:v1.1

docker tag  totvslabs/mdm:v1.1  totvslabs/mdm:latest

docker push totvslabs/mdm:latest
** 2016-12-13: add prod-es-14 to prod env: http://prodjenkins.fluigdata.com:18080/job/DeploySystem/119/console
  CLOSED: [2016-12-13 Tue 20:05]
prod-es-14 107.170.216.152
ssh -p 2702 root@159.203.216.25 "curl 159.203.216.25:9200/_cluster/health?pretty"

ssh -p 2702 root@107.170.216.152 df -h /
** 2016-12-13: update nagios client only in prod env and explore env: http://explorejenkins.fluigdata.com:18080/job/DeploySystem/63/console
  CLOSED: [2016-12-13 Tue 22:03]
** 2016-12-13: Why Bruno's key is not in cb05 and cb06 of explore env: http://explorejenkins.fluigdata.com:18080/job/RunCommandOnServers/57/console
  CLOSED: [2016-12-13 Tue 22:26]
echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDgIRsZ+r4Ycj73pzcITmaLbBF3hRCpNnMYeY/j0UPT8xDwEltKEL1TIoqIZWv+SkEv4PqGo1Cw+6VQX21RfLD/gx10yA1S/fqOEMSp6U6L+m/T+PkZ/uNaple+oUrKfjKAPB+V+sxeH38xWktdkZsTPr5Gqb3ngkrCt46me/UPxwtA0/1DuPY1hgZClmp/EbgLScT8FpNBEwdJO+CEx3H1BFqzG2UgbxRgvVnGjrJmTIwKppOC3cuU8rhbTP+jeIE+kJaD47lFqV+K44uvCQQeQ6g3Vjp3Q4HZ0FO7SxEp9Gor25yHOQrlfdlmWjEZFsliwNRRTAp1Yj+uT98EEKWr brunocvcunha@gmail.com" >> /root/.ssh/authorized_keys

45.33.48.11 (explorees09)

=============== Run Command on 45.33.48.11:2702
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCneF004Ajy1HZSNhwo2TDU88DCOOOTqdONL9orwvOFdymdplRdLFTmZQhKRr32PsyyVMMpggzIyojkOdUCGjgBSDKDGiFONU1V+30SULTU3vVqYDPwMjyk2e46VN4JkUIjjCEtUFmcIJ0Mx4z/87/M69Fd8r64QoFvLQO5ZZSW/X9zWKeeO471Xab7os7WZjEiz7cW9YTVPnwY4JOZdyDnaOrmSeQ3vAmO0h7TvSYIvPUrl9Tiot/iPKMmp+s77VQI8lf6KHC3ec3GHuutqKov8Gwbxq/B7Px3UUulY86zgTFs+QCpHROwRc+z5hMuLLFLf2C0vwFG9ea2MXE/jIt7 kung.wang@totvs.com
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDLBVrkveHzgHkTY3jeE3MfvO0TWQhcYoWNDYD/gFqiQPs5TlunvKvW9aXsHa05Hl2qjZ/JICPXSs9OQNpsYat169gwE5n2P7k2Yg07XnRnryAwTrTSyNPkwBcEwYpL3QaJxVt4zERKLcBvYbNw/i6onnxVoK/CSS2Nyq7ok3q9AWv5armxWvMgg11eeaC9lL0YQnQylpk1BF1HHsDizX+p8EPmseGEs/gOUTB5mn6us/yTtQHT/gFuu5pFkLire37MtCiWga8qZYSRuaZ7i1+Of4xlhapK0tn6ii402dXuv15AG35o0PBCQL7BT//V3oc6NBXNgyw5pQrbKjT+OD7p auto.devops@totvs.com
+ cat /root/.ssh/authorized_keys

=============== Run Command on 104.237.159.204:2702
+ cat /root/.ssh/authorized_keys
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCneF004Ajy1HZSNhwo2TDU88DCOOOTqdONL9orwvOFdymdplRdLFTmZQhKRr32PsyyVMMpggzIyojkOdUCGjgBSDKDGiFONU1V+30SULTU3vVqYDPwMjyk2e46VN4JkUIjjCEtUFmcIJ0Mx4z/87/M69Fd8r64QoFvLQO5ZZSW/X9zWKeeO471Xab7os7WZjEiz7cW9YTVPnwY4JOZdyDnaOrmSeQ3vAmO0h7TvSYIvPUrl9Tiot/iPKMmp+s77VQI8lf6KHC3ec3GHuutqKov8Gwbxq/B7Px3UUulY86zgTFs+QCpHROwRc+z5hMuLLFLf2C0vwFG9ea2MXE/jIt7 kung.wang@totvs.com
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDLBVrkveHzgHkTY3jeE3MfvO0TWQhcYoWNDYD/gFqiQPs5TlunvKvW9aXsHa05Hl2qjZ/JICPXSs9OQNpsYat169gwE5n2P7k2Yg07XnRnryAwTrTSyNPkwBcEwYpL3QaJxVt4zERKLcBvYbNw/i6onnxVoK/CSS2Nyq7ok3q9AWv5armxWvMgg11eeaC9lL0YQnQylpk1BF1HHsDizX+p8EPmseGEs/gOUTB5mn6us/yTtQHT/gFuu5pFkLire37MtCiWga8qZYSRuaZ7i1+Of4xlhapK0tn6ii402dXuv15AG35o0PBCQL7BT//V3oc6NBXNgyw5pQrbKjT+OD7p auto.devops@totvs.com
** 2016-12-14: keep config file in sync: googleKey to googleKeys
  CLOSED: [2016-12-14 Wed 21:22]
http://injenkins.fluigdata.com:48080/job/MonitorGitFileChangesMDMActiveSprint/157/console
2016-12-13 21:53:01  thirdPartyConfig:
2016-12-13 21:53:01      loqateKey: udh7Of0oudITYk3N8XCZMCRGg0v3qB2oQareeE6r
2016-12-13 21:53:01 -    googleKey: AIzaSyD2I0aT3ctHLtStOk2aSpChWuWsWxONOSU
2016-12-13 21:53:01 +    googleKeys: ["AIzaSyD2I0aT3ctHLtStOk2aSpChWuWsWxONOSU", "AIzaSyAbxOlZXJC7roovHY_RExuA69DknHZqYqE", "AIzaSyAFHyv2YxT9PTkRJAim0AmA55AgHFDm92U"]

default['app_mdm']['google_map_key'] = 'AIzaSyD2I0aT3ctHLtStOk2aSpChWuWsWxONOSU'
** 2016-12-14: Nagios warning improvement: mdm raise alert for No space left
  CLOSED: [2016-12-14 Wed 21:22]
** 2016-12-14: update /etc/hosts for explore env: http://explorejenkins.fluigdata.com:18080/job/RunCommandOnServers/59/console
  CLOSED: [2016-12-14 Wed 22:14]
http://explorejenkins.fluigdata.com:18080/job/DeploySystem/67/console
** 2016-12-16: redirect nagios alerts of prod and explore env to seperate channels.
  CLOSED: [2016-12-16 Fri 11:19]
Redirect all nagios alerts to #XXX.

Will redirect critical alerts back, once we're ready.
#  --8<-------------------------- separator ------------------------>8--
#fd-alerts-explore
#fd-alerts-prod
** 2016-12-16: Created one Scheduled Jenkins job to monitor network latency within explore servers in Linode Env
  CLOSED: [2016-12-16 Fri 14:09]
http://explorejenkins.fluigdata.com:18080/job/NetworkLatencyReport/build?delay=0sec
** 2016-12-16: Keep config in sync
  CLOSED: [2016-12-16 Fri 17:10]
** 2016-12-17: push jenkins images: http://injenkins.fluigdata.com:48080/job/BuidAllDockerImages/54/console
  CLOSED: [2016-12-17 Sat 21:09]
docker stop my-test; docker rm my-test
docker run -t -d --privileged -h mytest --name my-test totvslabs/jenkins:v1.0 /bin/bash

docker exec -it my-test bash
** 2016-12-18: When stop mdm/mdmbackup, kill SIGINT first, then kill SIGTERM
  CLOSED: [2016-12-18 Sun 09:03]
** #  --8<-------------------------- separator ------------------------>8--
** 2016-12-19: Preserve old running history of java gc log: http://injenkins.fluigdata.com:48080/job/DockerDeployAllInOne/71/console
  CLOSED: [2016-12-18 Sun 22:18]
So I'd propose to do this by scripts

- When service start, cp mdm-gc.log mdm-gc.$(date).log
- Keep latest 5 backup of gc log (edited)

mdm_java_opts="-Xms1024m -Xmx1024m -agentlib:jdwp=server=y,transport=dt_socket,address=8086,suspend=n -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/opt/mdm/ -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Xloggc:/opt/mdm/logs/mdm-gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=1K"

/usr/bin/java -Xms1024m -Xmx1024m -agentlib:jdwp=server=y,transport=dt_socket,address=8086,suspend=n -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/opt/mdm/ -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Xloggc:/opt/mdm/logs/mdm-gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=1K -XX:HeapDumpPath=/coredump -jar /opt/mdm/app-1.54.0-SNAPSHOT.jar server /opt/mdm/config/mdm.yml

#+BEGIN_EXAMPLE
Bruno Volpato [2:11 PM]
yes, that's correct

[2:11]
also, is it possible for we to store the mdm-gc.log?

[2:11]
it is recreated every time the process starts

[2:12]
thinking that it can be useful to track the memory usage of all the executions

denny zhang [2:12 PM]
Agree, we should preserves old running history.

[2:13]
Will do today. And merge change to master/1.53/1.54 afterwards.
#+END_EXAMPLE

java opts Xloggc don't truncate old log

default['app_mdm']['mdm_extra_java_opts'] = \
  '-agentlib:jdwp=server=y,transport=dt_socket,address=8086,suspend=n -XX:+UseParNewGC ' \
  '-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 ' \
  '-XX:+UseCMSInitiatingOccupancyOnly ' \
  '-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/opt/mdm/ ' \
  '-XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Xloggc:/opt/mdm/logs/mdm-gc.log'
Built-in support for GC log rotation has been added to the HotSpot JVM. It is described in the RFE 6941923 and is available in:

Java 6 Update 34
Java 7 Update 2 (but there is no reference to it in these release notes)
There are three new JVM flags that can be used to enable and configure it:

-XX:+UseGCLogFileRotation
must be used with -Xloggc:<filename>;
-XX:NumberOfGCLogFiles=<number of files>
must be >=1, default is one;
-XX:GCLogFileSize=<number>M (or K)
default will be set to 512K.
** 2016-12-20: [[https://trello.com/c/F4bjiD5Z][#1690]] nagios send critical errors to #devops/#fd-explore channel: http://injenkins.fluigdata.com:48080/job/DockerDeployAllInOne/81/console
  CLOSED: [2016-12-20 Tue 21:08]
node.default['nagios3']['slack_cri_channel'] = node['nagios_mdm']['slack_cri_channel']
node.default['nagios3']['slack_all_channel'] = node['nagios_mdm']['slack_all_channel']
** 2016-12-21: [[https://trello.com/c/WTjjklby][#1589]] DeploySystem: add passphrase protect for whether_flushdatabaseonconnect parameter
  CLOSED: [2016-12-21 Wed 16:05]
Running DeploySystem with whether_flushdatabaseonconnect enabled, we might easily flush all critical data.

Propose to avoid this occasional mistake.

1. Add a new parameter to this jenkins job(flush_passphrase)
2. When people run the job with whether_flushdatabaseonconnect enabled, they have to manually input a passphrase correctly. e.g, FLUSHCONFIRM

Update 2 DeploySystem job in explore env and prod env

Update wiki

Update:
1. flush_passphrase(password parameter). Default value: empty. (Near whether_flushdatabaseonconnect)

FLUSH_DB

When whether_flushdatabaseonconnect is enabled, we need to input passphrase correctly. This would avoid people occasionally flush DB by mistake.

The default value is FLUSH_DB

        <hudson.model.PasswordParameterDefinition>
          <name>flush_passphrase</name>
          <description>Please input flush_passphrase correctly, after enabled whether_flushdatabaseonconnect.

This would avoid people occasionally flush DB by mistake.

The default value is FLUSH_DB</description>
          <defaultValue>Kabdy2I2unZQ1RlaIbj6nA==</defaultValue>
        </hudson.model.PasswordParameterDefinition>

2. Update script

export FLUSH_PASSPHRASE=FLUSH_DB

if [ &quot;$whether_flushdatabaseonconnect&quot; = &quot;true&quot; ]; then
   if [ &quot;$flush_passphrase&quot; != &quot;$FLUSH_PASSPHRASE&quot; ]; then
      echo &quot;ERROR: whether_flushdatabaseonconnect is enabled, however flush_passphrase is input correctly.&quot;
      exit 1
   fi
fi
** 2016-12-21: Branch out 1.55 and run CI tests
  CLOSED: [2016-12-21 Wed 18:14]
** 2016-12-21: [[https://github.com/TOTVS/mdmdevops/wiki/How-To-Deploy-a-High-Availability-MDM-Cluster-Env][Update wiki]]: How To Deploy a High Availability MDM Cluster Env
  CLOSED: [2016-12-21 Wed 18:14]
** 2016-12-21: df -h /: http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/148/console
  CLOSED: [2016-12-21 Wed 18:46]
** 2016-12-21: In prod env, app generate log message super fast
  CLOSED: [2016-12-21 Wed 19:43]
** 2016-12-21: prod env: migrate nagios server to jenkins VM, and remove nagios VM from DigitalOcean
  CLOSED: [2016-12-21 Wed 20:19]
nagios (159.203.204.145) --> jenkins (45.55.6.34)

- ufw remove: http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/147/console
  ufw delete allow from 159.203.204.145

- update /etc/hosts: http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/145/console

- repo.fluigdata.com
- wiki

- shutdown old nagios, and use new nagios

- deploy new nagios server: http://prodjenkins.fluigdata.com:18080/job/DeploySystem/128/console

#+BEGIN_EXAMPLE
Currently nagios and jenkins are running in different VMs.

We can host them in one VM. This would save some $ for us.

I will make this improvement tomorrow, if no strong objections.

For explore env, we're doing the same. Should be fine.
#+END_EXAMPLE
*** 2016-12-21: update /etc/hosts to remove nagis server: http://prodjenkins.fluigdata.com:18080/job/RunCommandOnServers/149/console
  CLOSED: [2016-12-21 Wed 19:20]
sed -i.bak '/159.203.204.145/d' /etc/hosts
** 2016-12-21: stop DigitalOcean VM of nexus.fluigdata.com and remove DNS
  CLOSED: [2016-12-21 Wed 21:24]
104.236.180.184
nexus.fluigdata.com
https://cloud.digitalocean.com/droplets/5524580

cd /usr/local/nexus-2.13.0-01
su nexus
./nexus stop

./nexus status

#+BEGIN_EXAMPLE
nexus@mdm-nexus:/usr/local/nexus-2.13.0-01/bin$ ps -ef | grep nexus
nexus     1082     1  0 Nov29 ?        00:23:14 /usr/local/nexus/bin/jsw/linux-x86-64/wrapper /usr/local/nexus/bin/jsw/conf/wrapper.conf wrapper.syslog.ident=nexus wrapper.pidfile=/usr/local/nexus/nexus.pid wrapper.daemonize=TRUE
nexus     1088  1082  0 Nov29 ?        02:31:42 java -XX:MaxPermSize=192m -Djava.io.tmpdir=./tmp -Djava.net.preferIPv4Stack=true -Dcom.sun.jndi.ldap.connect.pool.protocol=plain ssl -Xms256m -Xmx768m -Djava.library.path=bin/jsw/lib -classpath bin/jsw/lib/wrapper-3.2.3.jar:./lib/jetty-http-8.1.16.v20140903.jar:./lib/jetty-xml-8.1.16.v20140903.jar:./lib/jetty-server-8.1.16.v20140903.jar:./lib/jetty-webapp-8.1.16.v20140903.jar:./lib/nexus-bootstrap-2.13.0-01.jar:./lib/javax.servlet-3.0.0.v201112011016.jar:./lib/jetty-util-8.1.16.v20140903.jar:./lib/jetty-continuation-8.1.16.v20140903.jar:./lib/jetty-client-8.1.16.v20140903.jar:./lib/jetty-jmx-8.1.16.v20140903.jar:./lib/jetty-servlet-8.1.16.v20140903.jar:./lib/jetty-deploy-8.1.16.v20140903.jar:./lib/metrics-core-2.2.0.jar:./lib/jetty-rewrite-8.1.16.v20140903.jar:./lib/jetty-io-8.1.16.v20140903.jar:./lib/plexus-interpolation-1.16.jar:./lib/jetty-security-8.1.16.v20140903.jar:./lib/logback-access-1.1.2.jar:./lib/metrics-jetty-2.2.0.jar:./lib/logback-core-1.1.2.jar:./lib/metrics-logback-2.2.0.jar:./lib/slf4j-api-1.7.6.jar:./lib/logback-classic-1.1.2.jar:./lib/jul-to-slf4j-1.7.6.jar:./conf/ -Dwrapper.key=UYbNuZQGU6rDUkkH -Dwrapper.port=32000 -Dwrapper.jvm.port.min=31000 -Dwrapper.jvm.port.max=31999 -Dwrapper.pid=1082 -Dwrapper.version=3.2.3 -Dwrapper.native_library=wrapper -Dwrapper.service=TRUE -Dwrapper.cpu.timeout=10 -Dwrapper.jvmid=1 org.sonatype.nexus.bootstrap.jsw.JswLauncher ./conf/jetty.xml ./conf/jetty-requestlog.xml
#+END_EXAMPLE
** 2016-12-22: keep config file in sync
  CLOSED: [2016-12-22 Thu 08:59]
** 2016-12-22: Merge 1.54 to master
  CLOSED: [2016-12-22 Thu 08:59]
** 2016-12-22: add explorees15 to explore env
  CLOSED: [2016-12-22 Thu 09:02]
23.239.20.4:2702

echo "explorees15" > /etc/hostname
hostname -F /etc/hostname
exit

➜  ~ ssh -p 2702 root@173.255.243.91 "curl 173.255.243.91:9200/_cluster/health?pretty"
Warning: Permanently added '[173.255.243.91]:2702' (RSA) to the list of known hosts.
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   475  100   475    0     0   484k      0 --:--:-- --:--:-- --:--:--  4{ 0
  "cluster_name" : "mdm",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 18,
  "number_of_data_nodes" : 13,
  "active_primary_shards" : 161,
  "active_shards" : 318,
  "relocating_shards" : 0,
  "initializing_shards" : 4,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 98.75776397515527
}
63k
** 2016-12-22: [[https://trello.com/c/6UsdnByX][#1645]] Disk clean up for long run code build Jenkins: http://injenkins.fluigdata.com:48080/job/DockerDeploySandboxActiveSprint/87/console
  CLOSED: [2016-12-22 Thu 10:29]
http://159.203.196.76:8080

Update jenkins: branch_name -> app_branch_name, old_build_retention_days

Code build for every different branch will use ~1GB. We enforce data retention to reclaim disk capacity.

This would be useful for long-run sandbox env

- Verify in Wilson's QA env
- Verify repo.fluigdata.com: reconfgure the parameter

find $working_dir -type d -mtime +$old_build_retention_days -exec rm -rf {} +

# different folder
/var/lib/jenkins/jobs/BuildMDMRepoActiveSprint/workspace

# fail to remove
find: `./1.49/mdm/mdm-tools': No such file or directory
find: `./1.49/mdm/plugins': No such file or directory
find: `./1.49/mdm/build_be.sh': No such file or directory

# keep scripts directory
Use a different code directory

working_dir=/var/lib/jenkins/code/
old_build_retention_days=7

find /var/www/repo -maxdepth 1 -type d -ctime +$old_build_retention_days -exec rm -rf {} +
find $working_dir -mindepth 2 -maxdepth 2 -type d -ctime +$old_build_retention_days -exec rm -rf {} +
*** update jenkins jobs
branch_name: app_branch_name

old_build_retention_days

7

Code build for every different branch will use ~1GB. We enforce data retention to reclaim disk capacity.

This would be useful for long-run env
*** update scripts
#!/bin/bash -ex
function current_git_sha() {
    set -e
    local src_dir=${1?}
    cd $src_dir
    sha=$(git log -n 1 | head -n 1 | grep commit | awk -F' ' '{print $2}')
    echo $sha
}

function git_log() {
    echo "check log at dir: $code_dir"
    local code_dir=${1?}
    local tail_count=${2:-"10"}
    cd $code_dir
    command="git log -n $tail_count --pretty=format:\"%h - %an, %ar : %s\""
    echo -e "\nShow latest git commits: $command"
    eval $command
    echo -e "\n"
}

function git_update_code() {
    set -e
    local branch_name=${1?}
    local working_dir=${2?}
    local git_repo_url=${3?}

    local git_repo
    git_repo=$(echo "${git_repo_url%.git}" | awk -F '/' '{print $2}')

    local code_dir="$working_dir/$branch_name/$git_repo"
    echo "Git update code for $git_repo_url to $code_dir"
    # checkout code, if absent
    if [ ! -d "$working_dir/$branch_name/$git_repo" ]; then
        mkdir -p "$working_dir/$branch_name"
        cd "$working_dir/$branch_name"
        git clone --depth 1 "$git_repo_url" --branch "$branch_name" --single-branch
        cd "$code_dir"
        git tag -l
        git config user.email "jenkins@devops.com"
        git config user.name "Jenkins Auto"
    else
        cd "$code_dir"
        git tag -l
        git config remote.origin.url "$git_repo_url"
        git config user.email "jenkins@devops.com"
        git config user.name "Jenkins Auto"
        # add retry for network turbulence
        git pull origin "$branch_name" || (sleep 2 && git pull origin "$branch_name")
    fi

    cd "$code_dir"
    git checkout "$branch_name"
    # git reset --hard
}

function generate_version() {
    local git_repo=${1?}
    local branch_name=${2?}
    local revision_sha=${3?}

    local my_ip=$(/sbin/ifconfig eth0 | grep 'inet addr:' | cut -d: -f2 | awk '{ print $1}')
    echo "Build From $branch_name branch of $git_repo repo."
    echo "Revision: $revision_sha"
    echo "Build Time: `date +'%Y-%m-%d %H:%M:%S'`"
    echo "Jenkins Job: ${JOB_NAME}:${BUILD_DISPLAY_NAME} on $my_ip"
}

function copy_to_reposerver() {
    # Upload Packages to local apache vhost
    local git_repo=${1?}
    shift
    local branch_name=${1?}
    shift
    local code_dir=${1?}
    shift

    local files_to_copy=($*)
    cd $code_dir

    local repo_dir="/var/www/repo"
    local repo_link="$repo_dir/$branch_name"

    [ -d $repo_link ] || mkdir -p $repo_link
    for f in ${files_to_copy[*]};do
        if [[ "$f" == "$git_repo/"* ]]; then
            cp $f $repo_link/
        fi
    done
}

flag_file="/var/lib/jenkins/$JOB_NAME.flag"

function shell_exit() {
    errorcode=$?

    echo -e "\n\nShow latest commits for TOTVS/mdm"
    git_log $code_dir

    echo -e "\n\nShow latest commits for TOTVS/totvslabs-framework"
    git_log $framework_code_dir
    if [ $errorcode -eq 0 ]; then
        echo "OK"> $flag_file
    else
        echo "ERROR"> $flag_file
        exit 1
    fi
}
###############################################################################
. /etc/profile
# Build Repo
github_repo=$(echo ${git_repo_url%.git} | awk -F '/' '{print $2}')
code_dir=$working_dir/$app_branch_name/$github_repo

framework_github_repo=$(echo ${framework_git_repo_url%.git} | awk -F '/' '{print $2}')
framework_code_dir=$working_dir/$framework_branch_name/$framework_github_repo

trap shell_exit SIGHUP SIGINT SIGTERM 0

if $clean_start; then
    export clean_fe_build=true
    [ ! -d $framework_code_dir ] || rm -rf $framework_code_dir
    [ ! -d $code_dir ] || rm -rf $code_dir
    rm -rf ~/.m2/repository
fi

if [ ! -d $working_dir ]; then
    mkdir -p "$working_dir"
    chown -R jenkins:jenkins "$working_dir"
fi

if [ -d $code_dir ]; then
    old_sha=$(current_git_sha $code_dir)
else
    old_sha=""
fi

# Update code
git_update_code $framework_branch_name $working_dir $framework_git_repo_url
git_update_code $app_branch_name $working_dir $git_repo_url

new_sha=$(current_git_sha $code_dir)
echo "old_sha: $old_sha, new_sha: $new_sha"
if ! $force_build; then
    if [ "$old_sha" = "$new_sha" ]; then
        echo "No new commit, since previous build"
        if [ -f $flag_file ] && [[ `cat $flag_file` = "ERROR" ]]; then
            echo "Previous build has failed"
            exit 1
        else
            exit 0
        fi
    fi
fi

echo "================= Build Environment ================="
env
echo -e "\n\n\n"

cd $framework_code_dir
export framework_code_dir="$framework_code_dir"
make

cd $code_dir
export code_dir="$code_dir"
make

if [ -n "files_to_copy" ]; then
    echo "================= Copy built Packages ================="
    # copy mdm
    copy_to_reposerver $github_repo $app_branch_name $working_dir/$app_branch_name $files_to_copy
    # copy framework
    copy_to_reposerver $framework_github_repo $framework_branch_name $working_dir/$framework_branch_name $files_to_copy
fi

[ -n "$old_build_retention_days" ] || old_build_retention_days=7
echo "================= Remove old backkup older than $old_build_retention_days ================="
find /var/www/repo -maxdepth 1 -type d -ctime +$old_build_retention_days -exec rm -rf {} +
find $working_dir -mindepth 2 -maxdepth 2 -type d -ctime +$old_build_retention_days -exec rm -rf {} +
*** diff
jenkins@e9bde93b0287:~/jobs/BuildMDMRepoCodeActiveSprint$ diff /tmp/config.xml config.xml
41c41
<           <name>branch_name</name>
---
>           <name>app_branch_name</name>
68a69,75
>         <hudson.model.StringParameterDefinition>
>           <name>old_build_retention_days</name>
>           <description>Code build for every different branch will use ~1GB. We enforce data retention to reclaim disk capacity.
>
> This would be useful for long-run env</description>
>           <defaultValue>7</defaultValue>
>         </hudson.model.StringParameterDefinition>
90c97
<       <command>#!/bin/bash -e
---
>       <command>#!/bin/bash -ex
99a107
>     echo &quot;check log at dir: $code_dir&quot;
127,128c135,136
<         git config --global user.email &quot;jenkins@devops.com&quot;
<         git config --global user.name &quot;Jenkins Auto&quot;
---
>         git config user.email &quot;jenkins@devops.com&quot;
>         git config user.name &quot;Jenkins Auto&quot;
133,134c141,142
<         git config --global user.email &quot;jenkins@devops.com&quot;
<         git config --global user.name &quot;Jenkins Auto&quot;
---
>         git config user.email &quot;jenkins@devops.com&quot;
>         git config user.name &quot;Jenkins Auto&quot;
200c208
< code_dir=$working_dir/$branch_name/$github_repo
---
> code_dir=$working_dir/$app_branch_name/$github_repo
207a216,217
>     export clean_fe_build=true
>     [ ! -d $framework_code_dir ] || rm -rf $framework_code_dir
225c235
< git_update_code $branch_name $working_dir $git_repo_url
---
> git_update_code $app_branch_name $working_dir $git_repo_url
245d254
<
257c266
<     copy_to_reposerver $github_repo $branch_name $working_dir/$branch_name $files_to_copy
---
>     copy_to_reposerver $github_repo $app_branch_name $working_dir/$app_branch_name $files_to_copy
260a270,274
>
> [ -n &quot;$old_build_retention_days&quot; ] || old_build_retention_days=7
> echo &quot;================= Remove old backkup older than $old_build_retention_days =================&quot;
> find /var/www/repo -maxdepth 1 -type d -ctime +$old_build_retention_days -exec rm -rf {} +
> find $working_dir -mindepth 2 -maxdepth 2 -type d -ctime +$old_build_retention_days -exec rm -rf {} +
*** TODO update dev jenkins for data cleanup
http://159.203.196.76:8080/job/Sandbox-arizona-02-Build/
http://159.203.196.76:8080/job/Sandbox-utah-02-Build/
http://159.203.196.76:8080/job/MDM-PR-FE/
http://159.203.196.76:8080/job/MDM-PR/
** 2016-12-23: shutdown 2 digitalOcean VMs
  CLOSED: [2016-12-23 Fri 10:00]
Took snapshot and shutdown 2 DigitalOcean VMs now. Will remove the snapshot this weekend, if no issue is reported
- old nagios server(159.203.204.145) and nexus.fluigdata.com
** 2016-12-23: add explorees16 to explore env
  CLOSED: [2016-12-23 Fri 10:00]
5Kb8tJbF

linode2591362 45.79.95.206

echo "explorees16" > /etc/hostname
hostname -F /etc/hostname
** 2016-12-23: explore10 run out of disk in explore env
  CLOSED: [2016-12-23 Fri 10:26]
http://explorejenkins.fluigdata.com:18080/job/DeploySystemRehearsal/40/console
104.237.159.204
** 2016-12-24: [[https://trello.com/c/MjoXEZCm][#171]] mdm initscript: after deployment, mdm service is not started
  CLOSED: [2016-12-24 Sat 10:38]
#+BEGIN_EXAMPLE

denny zhang [12:54 AM]
This line is suspicious. The find command might fail in certain scenarios. As a result, it will skip start-stop-daemon. This means, mdm won't be started.

https://github.com/TOTVS/mdmdevops/blob/master/cookbooks/app-mdm/templates/default/mdm_initscript.erb#L60

```    if [ -f "/opt/mdm/logs/mdm-gc.log" ]; then
        cp /opt/mdm/logs/mdm-gc.log /opt/mdm/logs/mdm-gc-$(date +'%Y%m%d_%H%M%S').log
    fi
    # remove backup older than 15 days
    find /opt/mdm/logs -type f -mtime +15 -name "mdm-gc-*.log" -exec rm -rf {} \;

    if start-stop-daemon --start --quiet --make-pidfile --pidfile $PIDFILE --background -d /opt/mdm --startas /bin/bash -- -c "exec /usr/bin/java $mdm_java_opts -jar /opt/mdm/<%= @app_jar_filename %> server /opt/mdm/config/mdm.yml >>/opt/mdm/logs/mdm-initscript.log 2>&1"; then
        log_end_msg 0
```

[12:54]
Will check and test more tomorrow morning.
#+END_EXAMPLE
** 2016-12-26: [[https://github.com/TOTVS/mdmdevops/wiki/Add-a-DB-node-to-existing-cluster][Add wiki]]: Add a DB node to existing cluster
  CLOSED: [2016-12-18 Sun 23:12]
https://github.com/TOTVS/mdmdevops/wiki/Add-a-DB-node-to-existing-cluster
** 2016-12-24: #fd-metric: list cost of digitalocean
  CLOSED: [2016-12-24 Sat 17:33]
** 2016-12-25: verify nagios in explore env:  http://explorejenkins.fluigdata.com:18080/job/DeploySystemRehearsal/47/console
  CLOSED: [2016-12-25 Sun 14:09]
** 2016-12-25: nagios monitor alert: http://explorejenkins.fluigdata.com:18080/job/DeploySystem/90/console
  CLOSED: [2016-12-25 Sun 14:15]
** 2016-12-27: add prod-es-15 to prod env: http://prodjenkins.fluigdata.com:18080/job/DeploySystem/134/console
  CLOSED: [2016-12-27 Tue 09:40]
prod-es-15

107.170.253.222

ssh -p 2702 root@159.203.216.25 "curl 159.203.216.25:9200/_cluster/health?pretty"
** 2016-12-27: add explorees17 to explore env
  CLOSED: [2016-12-27 Tue 14:47]
96.126.103.191

echo "explorees17" > /etc/hostname
hostname -F /etc/hostname
exit
** 2016-12-27: explorees15(23.239.20.4) is not reachable in explore env: Linode issue
  CLOSED: [2016-12-27 Tue 14:24]

Nagios BOT [2:10 PM]
Host 'explorees15' is DOWN:
PING CRITICAL - Packet loss = 100%

denny zhang [2:17 PM]
Can't ping explorees15(23.239.20.4) indeed.

[2:19]
Confirmed it's Linode event: linode2588330(explorees15)


Nagios BOT [2:20 PM]
Host 'explorees15' is UP:
PING OK - Packet loss = 0%, RTA = 0.45 ms

denny zhang [2:21 PM]
uploaded an image: Pasted image at 2016-12-27, 2:20 PM
Add Comment

denny zhang [2:22 PM]
We got Linode notification of hardware issue quite often.

I haven't measured it closely. The frequency is about once per week, from my draft estimation.

Usually we don't need to do anything. Just blindly wait.

As you may nagios of nagios alerts, our service especially es cluster would have some turbulence.

Might need to evaluate whether we should move back from Linode to DigitalOcean.
** 2016-12-28: Branch out 1.56 and run all ci tests
  CLOSED: [2016-12-28 Wed 07:06]
** 2016-12-28: [#A] check prod es cluster: unassigned shard 1
  CLOSED: [2016-12-28 Wed 07:06]
ssh -p 2702 root@159.203.216.25 "curl 159.203.216.25:9200/_cluster/health?pretty"

#+BEGIN_EXAMPLE
➜  ~ ssh -p 2702 root@159.203.216.25 "curl 159.203.216.25:9200/_cluster/health?pretty"
Warning: Permanently added '[159.203.216.25]:2702' (RSA) to the list of known hosts.
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0{
  "cluster_name" : "mdm",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 21,
  "number_of_data_nodes" : 15,
  "active_primary_shards" : 146,
  "active_shards" : 421,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 1,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0,
  "task_max_waiting_in_queue_millis" : 0,
  "active_shards_percent_as_number" : 99.76303317535546
}
#+END_EXAMPLE
** 2016-12-29: Merge 1.55 To Master and run all CI tests
  CLOSED: [2016-12-29 Thu 19:46]
** 2016-12-29: Change default behavior: force logrotate in sandbox env only
  CLOSED: [2016-12-29 Thu 21:41]
** 2016-12-29: Created a crontab to enforce log rotate in all mdm nodes: http://prodjenkins.fluigdata.com:18080/job/TmpEnforceLogrotate/
  CLOSED: [2016-12-29 Thu 22:09]
It will run every 4 hours. It means we will only keep mdm log files within 1 day.

This is a temporary workaround for huge application logging events.

Please info me to cancel the job, once we have root fix the issue in application layer.
** 2016-12-31: [[https://trello.com/c/vsGw0UrW][#1739]] When config file template has been changed in dev repo, send slack notification
  CLOSED: [2016-12-31 Sat 04:17]
** 2016-12-31: [[https://trello.com/c/HCboIpmR][#1738]] When active sprint merged into master branch, send slack notification
  CLOSED: [2016-12-31 Sat 12:28]
git diff mdm-cluster/recipes/default.rb  | grep -iE "^- |^\+ "

export git_repo_url="https://github.com/DennyZhang/devops_public.git"
export branch_name="master"
filelist_to_monitor="devops_common_library.cksum"
monitor_pattern_list="echo "

bash -ex /var/lib/jenkins/code/devops_public/monitor/monitor_git_contentchanges.sh
** #  --8<-------------------------- separator ------------------------>8--
* 2015                                                             :noexport:
:PROPERTIES:
:type:     Life
:END:

[[https://www.dennyzhang.com/wp-content/uploads/2014/04/blog_mediation.jpg]]
- Tracking what I have done for every working day

https://totvslab.atlassian.net/browse/CLOUDPASS-5966
https://totvslab.atlassian.net/browse/TECH-36

|       Date | Summary                                                                               |
|------------+---------------------------------------------------------------------------------------|
| 2015-03-25 | app01 of prod env is down with suspicious client request in RMI                       |
| 2015-03-24 | Setup squid for kitchen                                                               |
| 2015-03-23 | MDM sandbox test in digital ocean                                                     |
| 2015-03-23 | Identity HA explanation                                                               |
| 2015-03-18 | report BR bug CP-7020 fixed; confirm for new screen of master branch                  |
| 2015-03-18 | CP-7020: Applications ThickSSOProfifeGenerator.exe, DesktopLauncher.exe               |
| 2015-03-17 | Frontend Jenkins build fail                                                           |
| 2015-03-17 | Resource env suspicious chef update issue                                             |
| 2015-03-14 | sandbox-test: login to docker container and perform chef update                       |
| 2015-03-14 | vagrant No enough memory                                                              |
| 2015-03-14 | chef: cache oracle java download link                                                 |
| 2015-03-13 | Wiki: Trouble shooting guide for Identity                                             |
| 2015-03-12 | 2-nodes cluster env to restore customerfi                                             |
| 2015-03-12 | all-in-one: couchbase, elasticsearch, simple configuration                            |
| 2015-03-11 | jenkins plugin doesn't support 18080                                                  |
| 2015-03-10 | Use berk to download all dependened cookbooks                                         |
| 2015-03-06 | improvement for nagios disk history and nagios memory history                         |
| 2015-03-06 | check_proc_memory don't get resident memory for nagiosgraph                           |
| 2015-03-06 | Talk with Lucas to better DevOps knowledge transfer                                   |
| 2015-03-04 | CP-7006: Problem on restarting Rest, due to rest.yml changed                          |
| 2015-03-04 | CP-7007: email sending doesn't work in customerfi                                     |
| 2015-03-04 | CP-7005: CPU load of rest service in app02 is high                                    |
| 2015-03-02 | vagrant and kitchen add a hostonly adapter; support Lucas for Resource enterprise env |
| 2015-03-01 | Identity cluster deployment                                                           |
** [Monthly Journal] Feb, 2015
|       Date | Summary                                                                      |
|------------+------------------------------------------------------------------------------|
| 2015-02-27 | kitchen + digitalocean to perform all-in-one deployment                      |
| 2015-02-27 | fix nagios customlogin no rrd issue; customerfi data backup and remote copy  |
| 2015-02-26 | create docker image                                                          |
| 2015-02-25 | migrate customerfi from aio to cluster                                       |
| 2015-02-25 | DevOps Skeleton wiki for MDM                                                 |
| 2015-02-24 | CP-6702: nagios monitor racagent instances                                   |
| 2015-02-24 | support: customerfi custom images missing issue                              |
| 2015-02-23 | CP-6792 neo4j logging: remove logback.xml and logrotate                      |
| 2015-02-23 | update support login page for $?                                             |
| 2015-02-23 | nagios3 cookbooks integration test for ubuntu12-04, ubuntu14-04, centos-65   |
| 2015-02-20 | nagios check process cpu: ps -p $pid -o %cpu                                 |
| 2015-02-20 | submit app to appstore                                                       |
| 2015-02-17 | CP-6904: www.fliugidentity.com                                               |
| 2015-02-17 | TECH-75: nagios log check                                                    |
| 2015-02-14 | enforce kitchen+docker for all my cookbooks                                  |
| 2015-02-14 | Fix Mac build nokogiri(libiconv/libxml2) issue                               |
| 2015-02-13 | Kitchen + Docker; Push customerfi                                            |
| 2015-02-12 | Try testkitchen in my mac air;                                               |
| 2015-02-12 | learn more about vagrant; support of enterprise test/BR data team for backup |
| 2015-02-11 | fix appstore issues of uploading FSET app                                    |
| 2015-02-11 | CP-6922: nagios check flipping: selenium login of prod env                   |
| 2015-02-11 | Deploy all-in-one hadoop                                                     |
| 2015-02-11 | CP-6917: Fix Heart Bleed bug in customerfi                                   |
| 2015-02-11 | CP-6918: Password Protect events log                                         |
| 2015-02-10 | Shivan'g Share session about elasticsearch+couchbase                         |
| 2015-02-09 | Update wiki for upgrade and contact list                                     |
| 2015-02-09 | Update all prod env to new 1.4.4 again                                       |
| 2015-02-05 | Update all prod env to new 1.4.4 again, and issue handling                   |
| 2015-02-05 | Blanver upgrade; customerfi data migration; Prod env enum type problem       |
| 2015-02-04 | Update all prod env to new 1.4.4 again and issue handling                    |
| 2015-02-04 | customerfi seems to loss RAC data, after upgrade to 1.4.4                    |
| 2015-02-04 | CP-6859 nagios selenium custom login; Update hornetq conf in Chef            |
| 2015-02-04 | Make sure latestSSVersion are always updated automatically (CP6700)          |
| 2015-02-03 | hornetq conf change for 1.4.4; Trouble shooting for prod upgrade             |
| 2015-02-03 | Update all prod env to 1.4.4                                                 |
| 2015-02-03 | Testkitchen with vagrant and docker                                          |
| 2015-02-02 | Update doc of backup, nagios and Apache certificate                          |
** [Monthly Journal] Jan, 2015
|       Date | Summary                                                                                            |
|------------+----------------------------------------------------------------------------------------------------|
| 2015-01-30 | Test log rotate(CP-6763); Discuss backup plan; Document wiki(CP-6755)                              |
| 2015-01-29 | deploy for customerfi upgrade(CP-6807); Documentation for prod env upgrade;                        |
| 2015-01-28 | coorindate for customerfi upgrade; Guide Lucas/Support enigneer for enterprise upgrade             |
| 2015-01-27 | DevOps CV;                                                                                         |
| 2015-01-27 | CP-6731: avoid stdout to logfiles;                                                                 |
| 2015-01-26 | Chef enforce unit test                                                                             |
| 2015-01-26 | Update wiki of envs per Lucas' usage;                                                              |
|------------+----------------------------------------------------------------------------------------------------|
| 2015-01-26 | Track new nagios log errors(TECH75); Doc for how to update certificate                             |
| 2015-01-25 | nagios monitor customerfi(CP6791);                                                                 |
| 2015-01-23 | discussion and sharing with Kung for chef supermarket                                              |
| 2015-01-23 | wiki for env support, to avoid TOI style questions                                                 |
| 2015-01-23 | FSETNet change app name of testflight;                                                             |
| 2015-01-22 | Refine chef cookbook in the supermarket; Doc for FAQ of env support; wiki cleanup                  |
| 2015-01-21 | support Leandro to fix ACL issue; document for on-premise upgrade                                  |
| 2015-01-20 | bug triag and prioritize                                                                           |
| 2015-01-20 | Provide DevOps/OpenStack consultant slogan                                                         |
| 2015-01-16 | training Lucas for on-premise                                                                      |
| 2015-01-15 | wiki for TOTVS env list                                                                            |
| 2015-01-14 | CP-6732: Rest.log generates NullException from time to time                                        |
| 2015-01-14 | CP-6551: Huge temporary logs files of adsync.log*tmp                                               |
| 2015-01-14 | prod env issue: messaging machine is inaccessible for 40 min;                                      |
| 2015-01-13 | Upgrade prod env/customerfi/psfluigidentity to 1.4.3; TECH-75: nagios avoid false alarm            |
| 2015-01-12 | 3 prod envs issues support; CP-6682 auto assign company racagent                                   |
| 2015-01-12 | Upgrade prod env/customerfi/psfluigidentity to patched 1.4.3; CP-6682 auto assign company racagent |
| 2015-01-10 | Prod env: network issue, search high CPU, rmi fd                                                   |
| 2015-01-09 | Provide and maintain on-premise doc                                                                |
| 2015-01-08 | Upgrade prod env/customerfi/psfluigidentity to 1.4.3                                               |
| 2015-01-08 | on-premise deploy_version env incorrect; Support Leandro of on-premise deploy                      |
| 2015-01-07 | email sending problem of qafluigidentity; vmapp deploy wrong version of identity                   |
| 2015-01-07 | prod env: rmi fd in app02; On-premise doc; Blanver On-premise env;                                 |
| 2015-01-06 | on-premise bug fixing: env inheritance                                                             |
| 2015-01-06 | Brazil Insurance test env: activate company; send out email;all logfiles goes to /var/log/chef/    |
| 2015-01-05 | Brazil Insurance: Install test env                                                                 |
| 2015-01-03 | chef_update_sh: update itself                                                                      |
| 2015-01-03 | collect all noticable exceptions in all critical logfiles; JIRA 6664: mix.js                       |
| 2015-01-01 | nagios understand false alarm of log files(TECH-75); Too many fd for rmi in app02(JIRA 6676)       |
** #  --8<-------------------------- separator ------------------------>8--
** 2015-04-16: aws developer certification exam fail
准备了两周，竟然没考过。还只是一个associate level! 要好好总结一下里面的问题和原因了
** 2015-04-22: Setup chef 12 and do the chef server migration from spchef.fluigidentity.com
** 2015-06-05: authright: finish all chef cookbook tests in docker
** 2015-06-06: Setup openvpn
** 2015-06-07: build up elk docker image
** 2015-07-28: portal healtcheck; audit improvement; jenkins improvement; merge new opendj cookbook to new
** 2015-07-28: update docker image to cache more
** 2015-07-29: osc build latest docker image
** 2015-07-31: Apache vhost with BinLiang
** 2015-08-02: couchbase autoscale feature
** 2015-08-03: squid initscript issue for pidfile; replace opendj with 389 directory
** 2015-08-03: chef automate mongo and 389 directory initialization
** 2015-08-04: totvs couchbase autoscale demo
** 2015-08-05: code merge issue; reduce jenkins jobs duplication, by hosting logic in sh
** #  --8<-------------------------- separator ------------------------>8--
** 2015-08-06: serverspec check free memory and disk
** 2015-08-06: Update monitor files
** 2015-08-06: conditionally update for berkshelf
** 2015-08-06: chef update: wait for tomcat to be up
** 2015-08-06: change /opt/authright to /var/iam/devops
** 2015-08-06: osc Implement random reboot test
** 2015-08-07: speed up chef deployment
** 2015-08-07: VM reboot test: http://50.198.76.249:443/view/DeployCookbook/job/LongRunAllInOne/65/console
** 2015-08-08: gem sources: conditional choose
** 2015-08-09: Upgrade version of elk: elasticsearch, logstash and kibana
** 2015-08-10: logstash fail to start, due to configuration fail but no message output
** 2015-08-10: kibana initscript has wrong pattern to match process. It will result in mulitple running kibana running instance
** 2015-08-10: kibana start issue: re-org elk cookbook
** 2015-08-10: aliyun deployment speed up, due to elk cookbook change for kibana download
** 2015-08-11: Apache ip-based proxy
** 2015-08-11: Apache proxy
** 2015-08-11: nagios check use hostname, instead of ip. Restart docker container will result in ip changed
** 2015-08-12: Structurize analysis for DevOps work
** 2015-08-13: logstash 1.7 start issue
** 2015-08-14: after machine reboot, logstash_server fail to start
** 2015-08-15: relocate git repo for devops code
** 2015-08-16: smoke test for osc project
** 2015-08-17: Enable new all-in-one
** 2015-08-18: doc: sync file in between two repo

